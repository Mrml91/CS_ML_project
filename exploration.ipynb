{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "\n",
    "PICTURES_FOLDER = \"pictures\"\n",
    "os.makedirs(PICTURES_FOLDER, exist_ok=True)\n",
    "\n",
    "SLEEP_STAGES_COLORS = {\n",
    "    0: \"blue\",\n",
    "    1: \"green\",\n",
    "    2: \"red\",\n",
    "    3: \"black\",\n",
    "    4: \"orange\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"kaggle_data/X_train.h5/X_train.h5\"\n",
    "test_file = \"kaggle_data/X_test.h5/X_test.h5\"\n",
    "\n",
    "h5_train = h5py.File(train_file, mode='a')\n",
    "h5_test = h5py.File(test_file, mode='a')\n",
    "\n",
    "y_train = pd.read_csv(\"kaggle_data/y_train.csv\", index_col=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES = ['accel_norm', 'eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse', 'speed_norm', 'speed_x', 'speed_y', 'speed_z', 'x', 'y', 'z']\n",
      "FREQUENCIES = {'accel_norm': 10.0, 'eeg_1': 50.0, 'eeg_2': 50.0, 'eeg_3': 50.0, 'eeg_4': 50.0, 'eeg_5': 50.0, 'eeg_6': 50.0, 'eeg_7': 50.0, 'pulse': 10.0, 'speed_norm': 10.0, 'speed_x': 10.0, 'speed_y': 10.0, 'speed_z': 10.0, 'x': 10.0, 'y': 10.0, 'z': 10.0}\n"
     ]
    }
   ],
   "source": [
    "def update_globals():\n",
    "    features = [feat for feat in h5_train.keys() if feat not in ('index', 'index_absolute', 'index_window')]\n",
    "    frequencies = {feat: h5_train[feat][0].size / 30 for feat in features}\n",
    "    return features, frequencies\n",
    "    \n",
    "FEATURES, FREQUENCIES = update_globals()\n",
    "print(\"FEATURES =\", FEATURES)\n",
    "print(\"FREQUENCIES =\", FREQUENCIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECT #0\n",
      "SUBJECT #1\n",
      "SUBJECT #2\n",
      "SUBJECT #3\n",
      "SUBJECT #4\n",
      "SUBJECT #5\n",
      "SUBJECT #6\n",
      "SUBJECT #7\n",
      "SUBJECT #8\n",
      "SUBJECT #9\n",
      "SUBJECT #10\n",
      "SUBJECT #11\n",
      "SUBJECT #12\n",
      "SUBJECT #13\n",
      "SUBJECT #14\n",
      "SUBJECT #15\n",
      "SUBJECT #16\n",
      "SUBJECT #17\n",
      "SUBJECT #18\n",
      "SUBJECT #19\n",
      "SUBJECT #20\n",
      "SUBJECT #21\n",
      "SUBJECT #22\n",
      "SUBJECT #23\n",
      "SUBJECT #24\n",
      "SUBJECT #25\n",
      "SUBJECT #26\n",
      "SUBJECT #27\n",
      "SUBJECT #28\n",
      "SUBJECT #29\n",
      "SUBJECT #30\n",
      "SUBJECT #31\n",
      "SUBJECT #32\n",
      "SUBJECT #33\n",
      "SUBJECT #34\n",
      "SUBJECT #35\n",
      "SUBJECT #36\n",
      "SUBJECT #37\n",
      "SUBJECT #38\n",
      "SUBJECT #39\n",
      "SUBJECT #40\n",
      "SUBJECT #41\n",
      "SUBJECT #42\n",
      "SUBJECT #43\n",
      "SUBJECT #44\n",
      "SUBJECT #45\n",
      "SUBJECT #46\n",
      "SUBJECT #47\n",
      "SUBJECT #48\n",
      "SUBJECT #49\n",
      "SUBJECT #50\n",
      "SUBJECT #51\n",
      "SUBJECT #52\n",
      "SUBJECT #53\n",
      "SUBJECT #54\n",
      "SUBJECT #55\n",
      "SUBJECT #56\n",
      "SUBJECT #57\n",
      "SUBJECT #58\n",
      "SUBJECT #59\n",
      "SUBJECT #60\n",
      "['accel_norm', 'eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'index', 'index_absolute', 'index_window', 'pulse', 'speed_norm', 'speed_x', 'speed_y', 'speed_z', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# HELPERS\n",
    "\n",
    "def make_timeline(freq):\n",
    "    \"\"\"\n",
    "    ARGS:\n",
    "        freq (int): frequency in Hertz\n",
    "    \n",
    "    RETURNS:\n",
    "        (pd.timedelta_range) : timestamps for a signal sampled at <freq> Hz for 30 seconds\n",
    "    \"\"\"\n",
    "    return pd.timedelta_range(start='0s', end='30s', periods=freq*30)\n",
    "\n",
    "\n",
    "def make_full_timeline(windows, freq):\n",
    "    # test there is no missing data\n",
    "    deltas = np.unique(np.diff(windows))\n",
    "    assert (len(deltas) == 1) and (int(deltas[0]) == 1)\n",
    "    return pd.timedelta_range(start='0s',\n",
    "                              end=pd.to_timedelta('30s') * (windows[-1] + 1),\n",
    "                              periods=freq * 30 * (windows[-1] + 1))\n",
    "\n",
    "def get_subject_ids(h5_file):\n",
    "    return np.unique(h5_file[\"index\"][:])\n",
    "\n",
    "    \n",
    "def get_subject_boundaries(h5_file, subject_id, ready_to_use=True):\n",
    "    \"\"\"\n",
    "    Helper function to select data relating to a given subject (on numpy arrays)\n",
    "    \n",
    "    ARGS:\n",
    "        h5_file (h5py.File)\n",
    "        subject_id (int)\n",
    "        ready_to_use (bool, default=True): return a slice or a tuple\n",
    "        \n",
    "    RETURNS:\n",
    "        subject_boundaries : (slice) (index_start, index_end+1) if <ready_to_use>\n",
    "                             (tuple) (index_start, index_end) if not <ready_to_use>\n",
    "                        \n",
    "    \"\"\"\n",
    "    sids = h5_file['index'][:]\n",
    "    start = np.argmax(sids == subject_id)\n",
    "    end = len(sids) - 1 - np.argmax(sids[::-1] == subject_id)\n",
    "    \n",
    "    indexers = h5_file['index_absolute'][:]\n",
    "    start = indexers[start]\n",
    "    end = indexers[end]\n",
    "    if ready_to_use:\n",
    "        return slice(start, end + 1) # for numpy arrays\n",
    "    return (start, end)\n",
    "\n",
    "\n",
    "def get_subject_feature_signals(h5_file, subject_id, feature, as_timeseries=False):\n",
    "    \"\"\"\n",
    "    Get the full timeseries for a given (subject_id, feature) pair.\n",
    "    \n",
    "    ARGS:\n",
    "        h5_file (h5py.File)\n",
    "        subject_id (int)\n",
    "        feature (str)\n",
    "        \n",
    "    RETURNS:\n",
    "        timeseries : (pd.Series if <as_timeseries>) represents the <feature> timeseries of the subject \n",
    "                     (list[np.array[?]] if not <as_timeseries>) list of <feature> signals from the subject\n",
    "    \"\"\"\n",
    "    # Fetch subject boundaries\n",
    "    boundaries = get_subject_boundaries(h5_file, subject_id)\n",
    "    # Retrieve samples\n",
    "    feature_timeseries = h5_file[feature][boundaries]\n",
    "    if not as_timeseries:\n",
    "        return feature_timeseries\n",
    "    feature_timeseries = np.concatenate(feature_timeseries, axis=0)\n",
    "    # Build timeline\n",
    "    feature_frequency = FREQUENCIES[feature]\n",
    "    windows = h5_file['index_window'][boundaries]\n",
    "    timeline = make_full_timeline(windows, feature_frequency)\n",
    "    return pd.Series(data=feature_timeseries, index=timeline)\n",
    "\n",
    "\n",
    "def get_subject_sleep_stage(subject_id):\n",
    "    start, end = get_subject_boundaries(h5_train, subject_id, ready_to_use=False)\n",
    "    return y_train.loc[start:end] # because loc includes <end> (different behaviour than numpy arrays)\n",
    "    \n",
    "\n",
    "def _create_speed_and_acceleration(h5_file, overwrite=False, verbose=True):\n",
    "    \"\"\"\n",
    "    a[t] = (v[t] - v[t-1]) / dt \n",
    "    ===> v[t] = sum_{s=0}^{t} a[s] (+ v[-1] = 0)\n",
    "    \"\"\"\n",
    "    freq = 10\n",
    "    dt = 1 / freq\n",
    "    \n",
    "    # Create datasets if required\n",
    "    if \"accel_norm\" in h5_file.keys() and not overwrite:\n",
    "        return None\n",
    "    shape, dtype = h5_file[\"x\"].shape, h5_file[\"x\"].dtype\n",
    "    for name in [\"accel_norm\", \"speed_x\", \"speed_y\", \"speed_z\", \"speed_norm\"]:\n",
    "        try:\n",
    "            h5_file.create_dataset(name, shape=shape, dtype=dtype)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Initiate subject id\n",
    "    sid = -1\n",
    "    for ix in range(shape[0]):\n",
    "        if sid != h5_file[\"index\"][ix]:\n",
    "            sid = h5_file[\"index\"][ix]\n",
    "            speed = np.array([[0, 0, 0]])\n",
    "            if verbose:\n",
    "                print(f\"SUBJECT #{sid}\")\n",
    "        # acceleration\n",
    "        accel = np.stack([h5_file[feat][ix] for feat in (\"x\", \"y\", \"z\")], axis=-1)\n",
    "        h5_file[\"accel_norm\"][ix] = np.linalg.norm(accel, ord=2, axis=1)\n",
    "        # speed\n",
    "        speed = speed + np.cumsum(accel, axis=0) * dt\n",
    "        h5_file[\"speed_x\"][ix] = speed[:, 0]\n",
    "        h5_file[\"speed_y\"][ix] = speed[:, 1]\n",
    "        h5_file[\"speed_z\"][ix] = speed[:, 2]\n",
    "        h5_file[\"speed_norm\"][ix] = np.linalg.norm(speed, ord=2, axis=1)\n",
    "        # speed for next iteration\n",
    "        speed = speed[[-1], :]\n",
    "    return None\n",
    "        \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "# FOURIER\n",
    "\n",
    "from scipy.fft import fft\n",
    "\n",
    "def get_spectrum(seq, fs):\n",
    "    ft_modulus = np.abs(fft(seq))\n",
    "    # The signal is real so the spectrum is symmetric \n",
    "    if len(seq) % 2 == 0:\n",
    "        ft_modulus = ft_modulus[:len(seq) // 2]\n",
    "    else:\n",
    "        ft_modulus = ft_modulus[:len(seq) // 2 + 1]\n",
    "    freqs = np.arange(0, len(ft_modulus)) * fs / len(seq) # frequencies of the spectrum\n",
    "    return pd.Series(data=ft_modulus, index=freqs)\n",
    "    \n",
    "\n",
    "def get_spectrum_maxima(seq, fs, thresh=0.1):\n",
    "    spectrum = get_spectrum(seq, fs)\n",
    "    delta_left = np.diff(spectrum, prepend=spectrum[0] - 1) > 0 # ascending\n",
    "    delta_right = np.diff(spectrum[::-1], prepend=spectrum[-1] - 1)[::-1] > 0 # descending\n",
    "    ix_keep = np.logical_and(delta_left, delta_right) # local maximum\n",
    "    spectrum_util = spectrum.loc[ix_keep]\n",
    "    spectrum_util = spectrum_util.loc[spectrum_util > spectrum_util.max() * thresh]\n",
    "    return spectrum_util\n",
    "\n",
    "# ACCELEROMETER\n",
    "\n",
    "def acceleration_to_speed(accel_arr):\n",
    "    if accel_arr.ndim == 2:\n",
    "        return np.cumsum(accel_arr, axis=1)\n",
    "    return np.cumsum(accel_arr)\n",
    "\n",
    "def vec_to_norm(arr):\n",
    "    return np.linalg.norm(arr, axis=1)\n",
    "\"\"\"\n",
    "\n",
    "# Create speed and acceleration\n",
    "_create_speed_and_acceleration(h5_train, overwrite=False, verbose=True)\n",
    "_create_speed_and_acceleration(h5_test, overwrite=False, verbose=True)\n",
    "FEATURES, FREQUENCIES = update_globals()\n",
    "print(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "get_subject_feature_signals(h5_train, 1, \"eeg_1\", as_timeseries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "get_subject_feature_signals(h5_train, 1, \"eeg_1\", as_timeseries=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef plot_subject_quantiles(subject_id, q_inf=0.025, q_sup=0.975, n_quantiles=20):\\n    sleep_states = get_subject_sleep_state(subject_id)\\n    qts = np.linspace(q_inf, q_sup, n_quantiles).round(3)\\n    for feature in FEATURES:\\n        signal = get_subject_feature_signals(h5_train, subject_id, feature, as_timeseries=False)\\n        size = signal[0].size\\n        signal_by_state = pd.Series(data=np.concatenate(signal, axis=0),\\n                                    index=np.repeat(sleep_states.values, size))\\n        qt_df = signal_by_state.groupby(signal_by_state.index).quantile(qts)\\n        qt_df.unstack(0).plot()\\n        # sns.heatmap(qt_df.unstack(0))\\n        plt.title(feature)\\n        plt.show()\\n    return None\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def plot_subject_quantiles(subject_id, q_inf=0.025, q_sup=0.975, n_quantiles=20):\n",
    "    sleep_states = get_subject_sleep_state(subject_id)\n",
    "    qts = np.linspace(q_inf, q_sup, n_quantiles).round(3)\n",
    "    for feature in FEATURES:\n",
    "        signal = get_subject_feature_signals(h5_train, subject_id, feature, as_timeseries=False)\n",
    "        size = signal[0].size\n",
    "        signal_by_state = pd.Series(data=np.concatenate(signal, axis=0),\n",
    "                                    index=np.repeat(sleep_states.values, size))\n",
    "        qt_df = signal_by_state.groupby(signal_by_state.index).quantile(qts)\n",
    "        qt_df.unstack(0).plot()\n",
    "        # sns.heatmap(qt_df.unstack(0))\n",
    "        plt.title(feature)\n",
    "        plt.show()\n",
    "    return None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= FEATURE 1/16 =========\n",
      "========= FEATURE 2/16 =========\n",
      "========= FEATURE 3/16 =========\n",
      "========= FEATURE 4/16 =========\n",
      "========= FEATURE 5/16 =========\n",
      "========= FEATURE 6/16 =========\n",
      "========= FEATURE 7/16 =========\n",
      "========= FEATURE 8/16 =========\n",
      "========= FEATURE 9/16 =========\n",
      "========= FEATURE 10/16 =========\n",
      "========= FEATURE 11/16 =========\n",
      "========= FEATURE 12/16 =========\n",
      "========= FEATURE 13/16 =========\n",
      "========= FEATURE 14/16 =========\n",
      "========= FEATURE 15/16 =========\n",
      "========= FEATURE 16/16 =========\n"
     ]
    }
   ],
   "source": [
    "def robust_rescale(df):\n",
    "    \"\"\"\n",
    "    X_rescaled = (X - MED(X)) / MED(|X - MED(X)|)\n",
    "    \"\"\"\n",
    "    med = df.median()\n",
    "    med_spread = (df - df.median()).abs().median()\n",
    "    # df_rescaled = (df - med) / med_spread\n",
    "    return (df - med) / med_spread\n",
    "\n",
    "def min_max_rescale(df):\n",
    "    min_ = df.min()\n",
    "    max_ = df.max()\n",
    "    return (df - min_) / (max_ - min_)\n",
    "    \n",
    "def z_rescale(df): \n",
    "    mean = df.mean()\n",
    "    std = df.std()\n",
    "    return (df - mean) / std\n",
    "\n",
    "def save_feature_quantiles(feature,\n",
    "                           inf_qt=0.025,\n",
    "                           sup_qt=0.975,\n",
    "                           n_quantiles=21,\n",
    "                           robust_rescaling=False,\n",
    "                           overwrite=False,\n",
    "                           verbose=True):\n",
    "    \"\"\"\n",
    "    See pictures/quantile_plots\n",
    "    \n",
    "    Can be improved (make robust and not robust qplots simultaneously)\n",
    "    \"\"\"\n",
    "    # Make directory if it does not exist\n",
    "    qplot_dir = os.path.join(PICTURES_FOLDER, f\"quantile_plots\")\n",
    "    os.makedirs(qplot_dir, exist_ok=True)\n",
    "    # Escape if not overwrite and already done\n",
    "    qplot_fname = os.path.join(qplot_dir, f'{feature}{\"--rescaled\" if robust_rescaling else \"\"}.png')\n",
    "    if (not overwrite) and os.path.exists(qplot_fname):\n",
    "        return None\n",
    "    # Otherwise,\n",
    "    subject_ids = get_subject_ids(h5_train)\n",
    "    quantiles = np.linspace(inf_qt, sup_qt, n_quantiles).round(3)\n",
    "    subjects_quantiles = dict()\n",
    "    for cnt, sid in enumerate(subject_ids):\n",
    "        if verbose:\n",
    "            print(f\"--> SUBJECT {cnt+1}/{len(subject_ids)} (RESCALE = {str(robust_rescale)})\")\n",
    "        # Robust representation of the signal\n",
    "        signal = get_subject_feature_signals(h5_train, sid, feature, as_timeseries=False)\n",
    "        size = signal[0].size\n",
    "        signal = pd.Series(np.concatenate(signal))\n",
    "        if robust_rescaling:\n",
    "            signal = robust_rescale(signal)\n",
    "        # Behaviour by sleep stage\n",
    "        sleep_stages = get_subject_sleep_stage(sid).values\n",
    "        signal_by_stage = signal.groupby(np.repeat(sleep_stages, size))\n",
    "        subjects_quantiles[sid] = signal_by_stage.quantile(quantiles).unstack(0)\n",
    "        \n",
    "    fig, axes = plt.subplots(10, 3, figsize=(10, 40))\n",
    "    for ax, sid in zip(np.ravel(axes), subject_ids):\n",
    "        subjects_quantiles[sid].plot(ax=ax, title=f\"Subject {sid}\", color=SLEEP_STAGES_COLORS)\n",
    "    plt.savefig(qplot_fname)\n",
    "    plt.clf()\n",
    "    return subjects_quantiles\n",
    "\n",
    "\n",
    "# TO WRITE QUANTILE PLOTS IN pictures/quantile_plots\n",
    "for i, feat in enumerate(FEATURES):\n",
    "    print(f\"========= FEATURE {i+1}/{len(FEATURES)} =========\")\n",
    "    save_feature_quantiles(feat, robust_rescaling=False, overwrite=False, verbose=True)\n",
    "    save_feature_quantiles(feat, robust_rescaling=True, overwrite=False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef do_nothing(x):\\n    return x\\n\\ndef aggregate_stat(stat_func, char, dataset, y_vals, chunksize=1000):\\n    \"\"\"stat_func must have axis kwarg and take 2d arrays as arg\"\"\"\\n    chunks_ix = np.array_split(y_vals.index, len(y_vals) / chunksize)\\n    final = pd.Series([list() for _ in np.unique(y_vals)], index=np.unique(y_vals))\\n    for cnt, ix in enumerate(chunks_ix): \\n        # print(cnt, \\'/\\', len(chunks_ix))\\n        tmp = pd.Series(stat_func(dataset[char][ix.tolist()], axis=1), index=y_vals.loc[ix])\\n        tmp = tmp.groupby(tmp.index).agg(list)\\n        final = final + tmp.reindex(final.index, fill_value=list())\\n    return final\\n\\ndef custom_group_mean(char, dataset, y_vals, chunksize=1000):\\n    chunks_ix = np.array_split(y_vals.index, len(y_vals) / chunksize)\\n    for cnt, ix in enumerate(chunks_ix): \\n        # print(cnt, \\'/\\', len(chunks_ix))\\n        mean_tmp = pd.Series(np.mean(dataset[char][ix.tolist()], axis=1), index=y_vals.loc[ix])\\n        mean_tmp = mean_tmp.groupby(mean_tmp.index).agg([\\'mean\\', \\'size\\'])\\n        if cnt == 0:\\n            mean_df = mean_tmp\\n            continue\\n        mean_df.loc[:, \\'mean\\'] = (mean_df.prod(axis=1) + mean_tmp.prod(axis=1)) / (mean_df[\\'size\\'] + mean_tmp[\\'size\\'])\\n        mean_df.loc[:, \\'size\\'] = mean_df[\\'size\\'] + mean_df[\\'size\\']\\n    return mean_df[\\'mean\\']\\n\\n# custom_group_mean(\\'eeg_1\\', h5_train, y_train)\\naverages = pd.concat(map(lambda x: custom_group_mean(x, h5_train, y_train, 1000), FEATURES), axis=1, keys=FEATURES)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def do_nothing(x):\n",
    "    return x\n",
    "\n",
    "def aggregate_stat(stat_func, char, dataset, y_vals, chunksize=1000):\n",
    "    \"\"\"stat_func must have axis kwarg and take 2d arrays as arg\"\"\"\n",
    "    chunks_ix = np.array_split(y_vals.index, len(y_vals) / chunksize)\n",
    "    final = pd.Series([list() for _ in np.unique(y_vals)], index=np.unique(y_vals))\n",
    "    for cnt, ix in enumerate(chunks_ix): \n",
    "        # print(cnt, '/', len(chunks_ix))\n",
    "        tmp = pd.Series(stat_func(dataset[char][ix.tolist()], axis=1), index=y_vals.loc[ix])\n",
    "        tmp = tmp.groupby(tmp.index).agg(list)\n",
    "        final = final + tmp.reindex(final.index, fill_value=list())\n",
    "    return final\n",
    "\n",
    "def custom_group_mean(char, dataset, y_vals, chunksize=1000):\n",
    "    chunks_ix = np.array_split(y_vals.index, len(y_vals) / chunksize)\n",
    "    for cnt, ix in enumerate(chunks_ix): \n",
    "        # print(cnt, '/', len(chunks_ix))\n",
    "        mean_tmp = pd.Series(np.mean(dataset[char][ix.tolist()], axis=1), index=y_vals.loc[ix])\n",
    "        mean_tmp = mean_tmp.groupby(mean_tmp.index).agg(['mean', 'size'])\n",
    "        if cnt == 0:\n",
    "            mean_df = mean_tmp\n",
    "            continue\n",
    "        mean_df.loc[:, 'mean'] = (mean_df.prod(axis=1) + mean_tmp.prod(axis=1)) / (mean_df['size'] + mean_tmp['size'])\n",
    "        mean_df.loc[:, 'size'] = mean_df['size'] + mean_df['size']\n",
    "    return mean_df['mean']\n",
    "\n",
    "# custom_group_mean('eeg_1', h5_train, y_train)\n",
    "averages = pd.concat(map(lambda x: custom_group_mean(x, h5_train, y_train, 1000), FEATURES), axis=1, keys=FEATURES)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_train.close()\n",
    "h5_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python37164bitdc6ddf9b5234459bacda0ad4bcef452b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
