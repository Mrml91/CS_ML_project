{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "from kaggle_submit import submit_to_kaggle\n",
    "from helpers import *\n",
    "from utils.globals import *\n",
    "from utils.distribution_statistics import *\n",
    "\n",
    "train_file = \"kaggle_data/X_train.h5/X_train.h5\"\n",
    "test_file = \"kaggle_data/X_test.h5/X_test.h5\"\n",
    "\n",
    "h5_train = h5py.File(train_file, mode='a')\n",
    "h5_test = h5py.File(test_file, mode='a')\n",
    "\n",
    "y_train = pd.read_csv(\"kaggle_data/y_train.csv\", index_col=0, squeeze=True)\n",
    "y_train_arr = y_train.to_numpy()\n",
    "\n",
    "# MAKE CUSTOM FEATURES\n",
    "from additional_features.make_features import make_all_features\n",
    "make_all_features(h5_train, h5_test, n_chunks=10, verbose=True, overwrite=False)\n",
    "\n",
    "\n",
    "from objects import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(list(set(h5_train.keys()) - set(IRRELEVANT_FEATURES)), columns=[\"Feature\"])\n",
    "features_df.loc[:, \"Dim\"] = features_df['Feature'].apply(lambda x: h5_train[x][0].shape[0])\n",
    "features_df.sort_values(by=[\"Dim\", \"Feature\"])\n",
    "\n",
    "import re\n",
    "\n",
    "BAND_LOG_ENERGY_FEATURES = [feat for feat in FEATURES if re.search(\"(?:(?:alpha)|(?:beta)|(?:delta)|(?:theta)){1}.*_logE\", feat)]\n",
    "\n",
    "SLEEP_FEATURES = [feat for feat in FEATURES if re.search('sleep.*[^(?:logmod)]', feat)]\n",
    "\n",
    "LOGMOD_FEATURES = [feat for feat in FEATURES if re.search('.*_logmod', feat)]\n",
    "\n",
    "TIME_FEATURES\n",
    "\n",
    "OTHER_FEATURES = [\"pulse_max_freq\", \"pulse_max_logE\"]\n",
    "\n",
    "_features = sum([BAND_LOG_ENERGY_FEATURES, SLEEP_FEATURES, LOGMOD_FEATURES, TIME_FEATURES, OTHER_FEATURES], [])\n",
    "assert sorted(_features) == sorted(FEATURES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def make_input_default(h5_file):\n",
    "    df_bandlog = make_input_new(\n",
    "        h5_file,\n",
    "        features=BAND_LOG_ENERGY_FEATURES,\n",
    "        rescale_by_subject=False,\n",
    "        moments=[1],\n",
    "    )\n",
    "    #pre_op=np.exp,\n",
    "    #    pre_op_name=\"energy\"\n",
    "    #)\n",
    "    \n",
    "    df_sleep = make_input_new(\n",
    "        h5_file,\n",
    "        features=SLEEP_FEATURES,\n",
    "        rescale_by_subject=False,\n",
    "        moments=[1]\n",
    "    )\n",
    "    \n",
    "    ## LOGMOD RENAME COLUMNS\n",
    "    \n",
    "    df_logmod_no_rescale = make_input_new(\n",
    "        h5_file,\n",
    "        features=LOGMOD_FEATURES,\n",
    "        rescale_by_subject=False,\n",
    "        moments=[1],\n",
    "        quantiles_inv=[0.25, 0.5, 0.75],\n",
    "        diff_orders=[0],\n",
    "        pre_op=lambda x: np.exp(2 * x),\n",
    "        pre_op_name=\"energy\"\n",
    "    )\n",
    "    \n",
    "   # cols_no_rescale = [(col[0] + \"_no_rescale\", *col[1:]) for col in df_logmod_no_rescale.columns]\n",
    "   # df_logmod_no_rescale.columns = pd.MultiIndex.from_tuples(cols_no_rescale)\n",
    "    \n",
    "   # df_logmod_with_rescale = make_input_bis(\n",
    "   #     h5_file,\n",
    "   #     features=LOGMOD_FEATURES,\n",
    "   #     rescale=True,\n",
    "   #     moments=[1, 2],\n",
    "   #     quantiles=[0.05, 0.95],\n",
    "   #     quantiles_inv=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "   #     diff_orders=[0, 1],\n",
    "   #     pre_op=lambda x: np.exp(2 * x),\n",
    "   #     pre_op_name=\"energy\"\n",
    "   # )\n",
    "    \n",
    "   # cols_with_rescale = [(col[0] + \"_with_rescale\", *col[1:]) for col in df_logmod_with_rescale.columns]\n",
    "   # df_logmod_with_rescale.columns = pd.MultiIndex.from_tuples(cols_with_rescale)\n",
    "    \n",
    "    ## END LOGMOD RENAME COLUMNS\n",
    "    \n",
    "    \n",
    "    df_time = make_input_new(\n",
    "        h5_file,\n",
    "        features=TIME_FEATURES,\n",
    "        rescale_by_subject=False,\n",
    "        # moments=[1, 2, 3, 4],\n",
    "        quantiles=[0.1, 0.5, 0.9],\n",
    "        diff_orders=[0]\n",
    "    )\n",
    "    \n",
    "    df_pulse_max_freq = make_input_new(\n",
    "        h5_file,\n",
    "        features=[\"pulse_max_freq\"],\n",
    "        rescale_by_subject=False,\n",
    "        moments=[1],\n",
    "    )\n",
    "    \n",
    "    df_pulse_max_logE = make_input_new(\n",
    "        h5_file,\n",
    "        features=[\"pulse_max_logE\"],\n",
    "        rescale_by_subject=False,\n",
    "        moments=[1, 2],\n",
    "        pre_op=np.exp,\n",
    "        pre_op_name=\"energy\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return pd.concat([\n",
    "        df_bandlog,\n",
    "        #df_sleep,\n",
    "        #df_logmod_no_rescale, \n",
    "        #df_logmod_with_rescale, \n",
    "        df_time,\n",
    "        #df_pulse_max_freq,\n",
    "        #df_pulse_max_logE,\n",
    "        ],\n",
    "        axis=1)\n",
    "\n",
    "def shift_and_fill(df, shift):\n",
    "    shifted_df = df.shift(shift)\n",
    "    if shift > 0:\n",
    "        shifted_df.bfill(inplace=True)\n",
    "    elif shift < 0:\n",
    "        shifted_df.ffill(inplace=True)\n",
    "    return shifted_df\n",
    "\n",
    "\n",
    "def roll_and_concat(df, shifts_range):\n",
    "    return pd.concat(map(lambda shift: shift_and_fill(df, shift), shifts_range), \n",
    "                     axis=1, keys=shifts_range)    \n",
    "    \n",
    "def concat_windows(h5_file, df, shifts):\n",
    "    df = df.groupby(h5_file[\"index\"][:], as_index=False).apply(roll_and_concat, shifts_range=shifts)\n",
    "    return df\n",
    "    \n",
    "def make_input_default_test(h5_file):\n",
    "    return make_input_new(h5_file, [\"eeg_1\", \"eeg_2\"], moments=[1])\n",
    "\n",
    "def make_input_default_rolling(h5_file, shifts):\n",
    "    \"\"\"\n",
    "    !!! not suited for pca because columns have 3 levels\n",
    "    \"\"\"\n",
    "    df = make_input_default(h5_file)\n",
    "    df_with_window = concat_windows(h5_file, df, shifts)\n",
    "    return df_with_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature #1/1\u001b[1K1K\r"
     ]
    }
   ],
   "source": [
    "X_train_raw, X_test_raw = make_input_default(h5_train), make_input_default(h5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = get_subject_ids(h5_train)\n",
    "train_train_ids, train_val_ids = train_ids[:28], train_ids[28:]\n",
    "test_ids = get_subject_ids(h5_test)\n",
    "\n",
    "X_train_train = X_train_raw.loc[subjects_ids_to_indexers(h5_train, train_train_ids, as_indices=True), :]\n",
    "y_train_train = y_train_arr[subjects_ids_to_indexers(h5_train, train_train_ids, as_indices=True)]\n",
    "\n",
    "X_train_val = X_train_raw.loc[subjects_ids_to_indexers(h5_train, train_val_ids, as_indices=True), :]\n",
    "y_train_val = y_train_arr[subjects_ids_to_indexers(h5_train, train_val_ids, as_indices=True)]\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_ = MinMaxScaler()\n",
    "pca_ = PCA(0.99)\n",
    "\n",
    "X_train_train = pca_.fit_transform(scaler_.fit_transform(X_train_train))\n",
    "X_train_val = pca_.transform(scaler_.transform(X_train_val))\n",
    "X_test = pca_.transform(scaler_.transform(X_test_raw))\n",
    "\n",
    "def subjects_ids_col(h5_file):\n",
    "    return h5_file[\"index\"][:]\n",
    "\n",
    "def concat_windows(arr, subjects_ids, h5_file, shifts): # subjects_ids must be sorted\n",
    "    sid_col = subjects_ids_col(h5_file)\n",
    "    sid_col = sid_col[np.isin(sid_col, subjects_ids)]\n",
    "    df = pd.DataFrame(arr)\n",
    "    \n",
    "    return df.groupby(sid_col).apply(roll_and_concat, shifts_range=shifts)\n",
    "    \n",
    "shifts = [-1, 0, 1]\n",
    "X_train_train_rolled = concat_windows(X_train_train, train_train_ids, h5_train, shifts)\n",
    "X_train_val_rolled = concat_windows(X_train_val, train_val_ids, h5_train, shifts)\n",
    "X_test_rolled = concat_windows(X_test, test_ids, h5_test, shifts)\n",
    "\"\"\"\n",
    "_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator = RandomForestClassifier(verbose=1, min_samples_leaf=10)\n",
    "estimator.fit(X_train_train, y_train_train)\n",
    "\n",
    "train_score = custom_score(estimator.predict(X_train_train), y_train_train)\n",
    "val_score = custom_score(estimator.predict(X_train_val), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_score)\n",
    "print(val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "estimator_svc = SVC(verbose=1, kernel='rbf', C=1)\n",
    "estimator_svc.fit(X_train_train, y_train_train)\n",
    "\n",
    "train_score_svc = custom_score(estimator_svc.predict(X_train_train), y_train_train)\n",
    "val_score_svc = custom_score(estimator_svc.predict(X_train_val), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2892077938342528\n",
      "0.24134018697794937\n"
     ]
    }
   ],
   "source": [
    "print(train_score_svc)\n",
    "print(val_score_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python37164bitdc6ddf9b5234459bacda0ad4bcef452b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
