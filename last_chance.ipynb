{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_create_eeg_mean\n",
      "_create_log_energy\n",
      "_create_log_modulus\n",
      "_create_pulse_max_log_energy_and_freq\n",
      "_create_speed_and_acceleration\n",
      "_create_time_features\n",
      "_create_eeg_mean\n",
      "_create_log_energy\n",
      "_create_log_modulus\n",
      "_create_eeg_mean\n",
      "_create_log_energy\n",
      "_create_log_modulus\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "from kaggle_submit import submit_to_kaggle\n",
    "from helpers import *\n",
    "from utils.globals import *\n",
    "from utils.distribution_statistics import *\n",
    "\n",
    "train_file = \"kaggle_data/X_train.h5/X_train.h5\"\n",
    "test_file = \"kaggle_data/X_test.h5/X_test.h5\"\n",
    "\n",
    "h5_train = h5py.File(train_file, mode='a')\n",
    "h5_test = h5py.File(test_file, mode='a')\n",
    "\n",
    "y_train = pd.read_csv(\"kaggle_data/y_train.csv\", index_col=0, squeeze=True)\n",
    "y_train_arr = y_train.to_numpy()\n",
    "\n",
    "# MAKE CUSTOM FEATURES\n",
    "from additional_features.make_features import make_all_features\n",
    "make_all_features(h5_train, h5_test, n_chunks=10, verbose=True, overwrite=False)\n",
    "\n",
    "from additional_features.make_features import _create_eeg_mean, _create_log_energy, _create_log_modulus\n",
    "for h5_file in (h5_train, h5_test):\n",
    "    for creation_function in [_create_eeg_mean, _create_log_energy, _create_log_modulus]:\n",
    "        print(creation_function.__name__)\n",
    "        creation_function(h5_file, n_chunks=100, verbose=True, overwrite=False)\n",
    "\n",
    "from objects import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beta_eeg_1_logE', 'beta_eeg_2_logE', 'beta_eeg_7_logE', 'beta_eeg_5_logE', 'theta_eeg_1_logE', 'theta_eeg_3_logE', 'alpha_eeg_2_logE', 'alpha_eeg_4_logE', 'theta_eeg_5_logE', 'alpha_eeg_7_logE', 'beta_eeg_3_logE', 'theta_eeg_4_logE', 'theta_eeg_6_logE', 'delta_eeg_4_logE', 'delta_eeg_6_logE', 'alpha_eeg_6_logE', 'delta_eeg_2_logE', 'alpha_eeg_1_logE', 'theta_eeg_2_logE', 'beta_eeg_6_logE', 'alpha_eeg_3_logE', 'delta_eeg_7_logE', 'theta_eeg_7_logE', 'delta_eeg_3_logE', 'delta_eeg_5_logE', 'delta_eeg_1_logE', 'beta_eeg_4_logE', 'alpha_eeg_5_logE']\n"
     ]
    }
   ],
   "source": [
    "features_df = pd.DataFrame(list(set(h5_train.keys()) - set(IRRELEVANT_FEATURES)), columns=[\"Feature\"])\n",
    "features_df.loc[:, \"Dim\"] = features_df['Feature'].apply(lambda x: h5_train[x][0].shape[0])\n",
    "features_df.sort_values(by=[\"Dim\", \"Feature\"])\n",
    "\n",
    "import re\n",
    "\n",
    "BAND_LOG_ENERGY_FEATURES = [feat for feat in FEATURES if re.search(\"(?:(?:alpha)|(?:beta)|(?:delta)|(?:theta)){1}.*_logE\", feat)]\n",
    "\n",
    "SLEEP_FEATURES = [feat for feat in FEATURES if re.search('sleep.*[^(?:logmod)]', feat)]\n",
    "\n",
    "LOGMOD_FEATURES = [feat for feat in FEATURES if re.search('.*_logmod', feat)]\n",
    "\n",
    "TIME_FEATURES\n",
    "\n",
    "OTHER_FEATURES = [\"pulse_max_freq\", \"pulse_max_logE\"]\n",
    "\n",
    "_features = sum([BAND_LOG_ENERGY_FEATURES, SLEEP_FEATURES, LOGMOD_FEATURES, TIME_FEATURES, OTHER_FEATURES], [])\n",
    "# print(set(_features) - set(FEATURES), set(FEATURES) - set(_features))\n",
    "assert sorted(_features) == sorted(FEATURES)\n",
    "\n",
    "def eeg_mean_filter(*args):\n",
    "    eeg_elem_filter_ = lambda x: not re.search(\"eeg_mean\", x)\n",
    "    return [[x for x in arg if eeg_elem_filter_(x)] for arg in args]\n",
    "    \n",
    "\n",
    "BAND_LOG_ENERGY_FEATURES_OLD, SLEEP_FEATURES_OLD, LOGMOD_FEATURES_OLD, TIME_FEATURES_OLD, OTHER_FEATURES_OLD = \\\n",
    "    eeg_mean_filter(BAND_LOG_ENERGY_FEATURES, SLEEP_FEATURES, LOGMOD_FEATURES, TIME_FEATURES, OTHER_FEATURES)\n",
    "\n",
    "print(BAND_LOG_ENERGY_FEATURES_OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_default(h5_file):\n",
    "    df_bandlog = make_input_new(\n",
    "        h5_file,\n",
    "        features=BAND_LOG_ENERGY_FEATURES_OLD,\n",
    "        rescale_by_subject=False,\n",
    "        moments=[1],\n",
    "        #pre_op=np.exp,\n",
    "        #pre_op_name=\"energy\"\n",
    "    )\n",
    "    \n",
    "    df_sleep = make_input_new(\n",
    "        h5_file,\n",
    "        features=SLEEP_FEATURES_OLD,\n",
    "        rescale_by_subject=False,\n",
    "        moments=[1]\n",
    "    )\n",
    "    \n",
    "    ## LOGMOD RENAME COLUMNS\n",
    "    \n",
    "    df_logmod = make_input_new(\n",
    "        h5_file,\n",
    "        features=LOGMOD_FEATURES_OLD,\n",
    "        rescale_by_subject=False,\n",
    "        # moments=[1],\n",
    "        quantiles_inv=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "        #quantiles=[0.1, 0.5, 0.9],\n",
    "        diff_orders=[0],\n",
    "        #interquantiles_inv=[(0.1, 0.9), (0.01, 0.99)],\n",
    "        #pre_op=lambda x: np.exp(2 * x),\n",
    "        #pre_op_name=\"energy\"\n",
    "    )\n",
    "    \n",
    "   # cols_no_rescale = [(col[0] + \"_no_rescale\", *col[1:]) for col in df_logmod_no_rescale.columns]\n",
    "   # df_logmod_no_rescale.columns = pd.MultiIndex.from_tuples(cols_no_rescale)\n",
    "    \n",
    "   # df_logmod_with_rescale = make_input_bis(\n",
    "   #     h5_file,\n",
    "   #     features=LOGMOD_FEATURES,\n",
    "   #     rescale=True,\n",
    "   #     moments=[1, 2],\n",
    "   #     quantiles=[0.05, 0.95],\n",
    "   #     quantiles_inv=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "   #     diff_orders=[0, 1],\n",
    "   #     pre_op=lambda x: np.exp(2 * x),\n",
    "   #     pre_op_name=\"energy\"\n",
    "   # )\n",
    "    \n",
    "   # cols_with_rescale = [(col[0] + \"_with_rescale\", *col[1:]) for col in df_logmod_with_rescale.columns]\n",
    "   # df_logmod_with_rescale.columns = pd.MultiIndex.from_tuples(cols_with_rescale)\n",
    "    \n",
    "    ## END LOGMOD RENAME COLUMNS\n",
    "    \n",
    "    \n",
    "    df_time_diff_0 = make_input_new(\n",
    "        h5_file,\n",
    "        features=TIME_FEATURES_OLD,\n",
    "        rescale_by_subject=False,\n",
    "        # moments=[1, 2],\n",
    "        quantiles=[1e-4, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 1-1e-4],\n",
    "        #interquantiles=[(0.1, 0.9), (0.01, 0.99)],\n",
    "        diff_orders=[0]\n",
    "    )\n",
    "    \n",
    "    #df_time_diff_1 = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=TIME_FEATURES,\n",
    "    #    rescale_by_subject=False,\n",
    "    #    # moments=[1, 2],\n",
    "    #    quantiles=[1e-4, 1-1e-4],\n",
    "    #    diff_orders=[1]\n",
    "    #)\n",
    "    \n",
    "    #df_pulse_max_freq = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=[\"pulse_max_freq\"],\n",
    "    #    rescale_by_subject=True,\n",
    "    #    moments=[1],\n",
    "    #)\n",
    "    \n",
    "    #df_pulse_max_logE = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=[\"pulse_max_logE\"],\n",
    "    #    rescale_by_subject=False,\n",
    "    #    moments=[1],\n",
    "        #pre_op=np.exp,\n",
    "        #pre_op_name=\"energy\"\n",
    "    #)\n",
    "    \n",
    "    \n",
    "    return pd.concat([\n",
    "        df_bandlog,\n",
    "        df_sleep,\n",
    "        df_logmod, \n",
    "        #df_logmod_with_rescale, \n",
    "        df_time_diff_0,\n",
    "        #df_time_diff_1,\n",
    "        #df_pulse_max_freq,\n",
    "        #df_pulse_max_logE,\n",
    "        ],\n",
    "        axis=1)\n",
    "\n",
    "def shift_and_fill(df, shift):\n",
    "    shifted_df = df.shift(shift)\n",
    "    if shift > 0:\n",
    "        shifted_df.bfill(inplace=True)\n",
    "    elif shift < 0:\n",
    "        shifted_df.ffill(inplace=True)\n",
    "    return shifted_df\n",
    "\n",
    "\n",
    "def roll_and_concat(df, shifts_range):\n",
    "    return pd.concat(map(lambda shift: shift_and_fill(df, shift), shifts_range), \n",
    "                     axis=1, keys=shifts_range)    \n",
    "    \n",
    "def concat_windows(h5_file, df, shifts):\n",
    "    df = df.groupby(h5_file[\"index\"][:], as_index=False).apply(roll_and_concat, shifts_range=shifts)\n",
    "    return df\n",
    "    \n",
    "def make_input_default_test(h5_file):\n",
    "    return make_input_new(h5_file, [\"eeg_1\", \"eeg_2\"], moments=[1])\n",
    "\n",
    "def make_input_default_rolling(h5_file, shifts):\n",
    "    \"\"\"\n",
    "    !!! not suited for pca because columns have 3 levels\n",
    "    \"\"\"\n",
    "    df = make_input_default(h5_file)\n",
    "    df_with_window = concat_windows(h5_file, df, shifts)\n",
    "    return df_with_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature #2/10\u001b[1KK\u001b[1K\u001b[1K\r"
     ]
    }
   ],
   "source": [
    "X_train_raw, X_test_raw = make_input_default(h5_train), make_input_default(h5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = get_subject_ids(h5_train)\n",
    "train_train_ids, train_val_ids = train_ids[:28], train_ids[28:]\n",
    "\n",
    "X_train_train = X_train_raw.loc[subjects_ids_to_indexers(h5_train, train_train_ids, as_indices=True), :]\n",
    "y_train_train = y_train_arr[subjects_ids_to_indexers(h5_train, train_train_ids, as_indices=True)]\n",
    "\n",
    "X_train_val = X_train_raw.loc[subjects_ids_to_indexers(h5_train, train_val_ids, as_indices=True), :]\n",
    "y_train_val = y_train_arr[subjects_ids_to_indexers(h5_train, train_val_ids, as_indices=True)]\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_ = MinMaxScaler()\n",
    "pca_ = PCA(0.99)\n",
    "\n",
    "X_train_train = pca_.fit_transform(scaler_.fit_transform(X_train_train))\n",
    "X_train_val = pca_.transform(scaler_.transform(X_train_val))\n",
    "X_test = pca_.transform(scaler_.transform(X_test_raw))\n",
    "\"\"\"\n",
    "def subjects_ids_col(h5_file):\n",
    "    return h5_file[\"index\"][:]\n",
    "\n",
    "def concat_windows(arr, subjects_ids, h5_file, shifts): # subjects_ids must be sorted\n",
    "    sid_col = subjects_ids_col(h5_file)\n",
    "    sid_col = sid_col[np.isin(sid_col, subjects_ids)]\n",
    "    df = pd.DataFrame(arr)\n",
    "    \n",
    "    return df.groupby(sid_col).apply(roll_and_concat, shifts_range=shifts)\n",
    "\n",
    "shifts = [-1, 0, 1]\n",
    "X_train_train_rolled = concat_windows(X_train_train, train_train_ids, h5_train, shifts)\n",
    "X_train_val_rolled = concat_windows(X_train_val, train_val_ids, h5_train, shifts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator_rf = RandomForestClassifier(\n",
    "    # verbose=1,\n",
    "    random_state=1,\n",
    "    # max_depth=10,\n",
    "    n_estimators=100, # default=100\n",
    "    # min_samples_leaf=10,\n",
    ")\n",
    "\n",
    "estimator_rf.fit(X_train_train_rolled, y_train_train)\n",
    "\n",
    "train_score_rf = custom_score(estimator_rf.predict(X_train_train_rolled), y_train_train)\n",
    "val_score_rf = custom_score(estimator_rf.predict(X_train_val_rolled), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_score_rf)\n",
    "print(val_score_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Random Forest Params | Time Features Quantiles | Time Features Moments | Sleep Features | Pulse Freq (f_max, A_max) | Shifts | Comments | Training Score | Validation Score |\n",
    "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "| - | 0.1, 0.5, 0.9 | - | No | - | 0 | - | 1| 0.67|\n",
    "| - | 0.1, 0.5, 0.9 | - | Yes | - | 0 | - | 1 | 0.69|\n",
    "| - | 0.1, 0.5, 0.9 | - | Yes | - | -1, 0, 1 | - | 1 | 0.7|\n",
    "| - | 0.01, 0.1, 0.5, 0.9, 0.99 | - | Yes | - | -1, 0, 1 | - | 1| 0.7  |\n",
    "| `min_samples_leaf=10` | 0.01, 0.1, 0.5, 0.9, 0.99 | - | Yes | - | -1, 0, 1 | - | 0.89 | 0.69  |\n",
    "| `min_samples_leaf=10` | 0.01, 0.1, 0.5, 0.9, 0.99 | 1, 2 | Yes | - | -1, 0, 1 |  - | 0.89 | 0.69  |\n",
    "| - | 0.01, 0.1, 0.5, 0.9, 0.99 | 1, 2 | Yes | - | -1, 0, 1 |  - | 1 | 0.697  |\n",
    "| - | 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99 | - | Yes | - | -1, 0, 1 |  - | 1 | 0.708  |\n",
    "| - | 0.01, DECILES, 0.99 | - | Yes | - | -1, 0, 1 |  - | 1 | 0.709  |\n",
    "| `min_samples_leaf=10` | 0.01, DECILES, 0.99 | - | Yes | - | -1, 0, 1 |  - | 0.89 | 0.697 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 |  - | 0.89 | 0.699 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 |  - | 1 | 0.713929 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Yes | -1, 0, 1 |  - | 1 | 0.7 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Yes | -1, 0, 1 | - | 0.89 | 0.697 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Pulse Only | -1, 0, 1 | - | 0.89 | 0.7 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Pulse Only | -1, 0, 1 |  - | 1 | 0.7055 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | - |  1 | 0.7055 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee 0.5 | - | Yes | - | -1, 0, 1 |  - | 1 | 0.708 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee 0.5 | - | Yes | - | -1, 0, 1 |  - | 0.89 | 0.700|\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee MIN, MAX | - | Yes | - | -1, 0, 1 | - | 0.89| 0.7 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee MIN, MAX | - | Yes | - | -1, 0, 1 | - | 1| 0.703|\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `bandlog rescaled`| 1| 0.665 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = 10%, 90% for logmod`| 1| 0.7165 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = 10%, 90% for logmod`| ? | 0.70 < x < 0.71 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES for logmod`| 1 | 0.7371780518172594 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES for logmod`| 0.898 | 0.721 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES for logmod; eeg_mean only`| 1 | 0.645 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New submission file at submissions/rf_best_alternate_2021-01-01.csv\n"
     ]
    }
   ],
   "source": [
    "test_ids = get_subject_ids(h5_test)\n",
    "X_test_rolled = concat_windows(X_test_raw, test_ids, h5_test, shifts)\n",
    "y_pred = estimator_rf.predict(X_test_rolled)\n",
    "#submit_to_kaggle(y_pred, h5_test, fname='rf_best_alternate_2021-01-01.csv', msg=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_ = MinMaxScaler()\n",
    "\n",
    "X_train_train_rolled_svc = scaler_.fit_transform(X_train_train_rolled) \n",
    "X_train_val_rolled_svc = scaler_.transform(X_train_val_rolled)\n",
    "X_test_rolled_svc = scaler_.transform(X_test_rolled)\n",
    "\n",
    "\n",
    "estimator_svc = SVC(verbose=1, kernel='rbf', C=1, max_iter=1000, random_state=1)\n",
    "estimator_svc.fit(X_train_train_rolled_svc, y_train_train)\n",
    "\n",
    "train_score_svc = custom_score(estimator_svc.predict(X_train_train_rolled_svc), y_train_train)\n",
    "val_score_svc = custom_score(estimator_svc.predict(X_train_val_rolled_svc), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24980, 63)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rolled.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python37164bitdc6ddf9b5234459bacda0ad4bcef452b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
