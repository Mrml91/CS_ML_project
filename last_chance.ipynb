{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_create_eeg_mean\n",
      "_create_log_energy\n",
      "1/10\u001b[1K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theophile/Desktop/ECP/3A/ML/project/additional_features/eeg_band_log_energies.py:70: RuntimeWarning: overflow encountered in log\n",
      "  log_energy = np.log(1 + energy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_create_log_modulus\n",
      "9/10\u001b[1KK\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theophile/Desktop/ECP/3A/ML/project/additional_features/features_to_frequential.py:62: RuntimeWarning: overflow encountered in log\n",
      "  1 + get_spectrum_modulus(h5_file[orig_feat][chunk_start : chunk_end], sampling_freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_create_pulse_max_log_energy_and_freq\n",
      "4/10\u001b[1K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theophile/Desktop/ECP/3A/ML/project/additional_features/pulse_to_freq.py:47: RuntimeWarning: overflow encountered in log\n",
      "  h5_file[\"pulse_max_logE\"][chunk_start:chunk_end] = np.log(1 + max_energies)[:, None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_create_speed_and_acceleration\n",
      "SUBJECT #29\u001b[1K\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "from kaggle_submit import submit_to_kaggle\n",
    "from helpers import *\n",
    "from utils.globals import *\n",
    "from utils.distribution_statistics import *\n",
    "\n",
    "train_file = \"kaggle_data/X_train.h5/X_train.h5\"\n",
    "test_file = \"kaggle_data/X_test.h5/X_test.h5\"\n",
    "\n",
    "h5_train = h5py.File(train_file, mode='a')\n",
    "h5_test = h5py.File(test_file, mode='a')\n",
    "\n",
    "y_train = pd.read_csv(\"kaggle_data/y_train.csv\", index_col=0, squeeze=True)\n",
    "y_train_arr = y_train.to_numpy()\n",
    "\n",
    "# MAKE CUSTOM FEATURES\n",
    "from additional_features.make_features import make_all_features\n",
    "make_all_features(h5_train, h5_test, n_chunks=10, verbose=True, overwrite=True)\n",
    "\n",
    "\n",
    "from objects import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame(list(set(h5_train.keys()) - set(IRRELEVANT_FEATURES)), columns=[\"Feature\"])\n",
    "features_df.loc[:, \"Dim\"] = features_df['Feature'].apply(lambda x: h5_train[x][0].shape[0])\n",
    "features_df.sort_values(by=[\"Dim\", \"Feature\"])\n",
    "\n",
    "import re\n",
    "\n",
    "BAND_LOG_ENERGY_FEATURES = [feat for feat in FEATURES if re.search(\"(?:(?:alpha)|(?:beta)|(?:delta)|(?:theta)){1}.*_logE\", feat)]\n",
    "\n",
    "SLEEP_FEATURES = [feat for feat in FEATURES if re.search('sleep.*[^(?:logmod)]', feat)]\n",
    "\n",
    "LOGMOD_FEATURES = [feat for feat in FEATURES if re.search('.*_logmod', feat)]\n",
    "\n",
    "TIME_FEATURES\n",
    "\n",
    "OTHER_FEATURES = [\"pulse_max_freq\", \"pulse_max_logE\"]\n",
    "\n",
    "_features = sum([BAND_LOG_ENERGY_FEATURES, SLEEP_FEATURES, LOGMOD_FEATURES, TIME_FEATURES, OTHER_FEATURES], [])\n",
    "assert sorted(_features) == sorted(FEATURES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_default(h5_file):\n",
    "    df_bandlog = make_input_new(\n",
    "        h5_file,\n",
    "        features=BAND_LOG_ENERGY_FEATURES,\n",
    "        rescale_by_subject=False,\n",
    "        moments=[1],\n",
    "        #pre_op=np.exp,\n",
    "        #pre_op_name=\"energy\"\n",
    "    )\n",
    "    \n",
    "    df_sleep = make_input_new(\n",
    "        h5_file,\n",
    "        features=SLEEP_FEATURES,\n",
    "        rescale_by_subject=False,\n",
    "        moments=[1]\n",
    "    )\n",
    "    \n",
    "    ## LOGMOD RENAME COLUMNS\n",
    "    \n",
    "    df_logmod = make_input_new(\n",
    "        h5_file,\n",
    "        features=LOGMOD_FEATURES,\n",
    "        rescale_by_subject=False,\n",
    "        # moments=[1],\n",
    "        quantiles_inv=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "        diff_orders=[0],\n",
    "        #pre_op=lambda x: np.exp(2 * x),\n",
    "        #pre_op_name=\"energy\"\n",
    "    )\n",
    "    \n",
    "   # cols_no_rescale = [(col[0] + \"_no_rescale\", *col[1:]) for col in df_logmod_no_rescale.columns]\n",
    "   # df_logmod_no_rescale.columns = pd.MultiIndex.from_tuples(cols_no_rescale)\n",
    "    \n",
    "   # df_logmod_with_rescale = make_input_bis(\n",
    "   #     h5_file,\n",
    "   #     features=LOGMOD_FEATURES,\n",
    "   #     rescale=True,\n",
    "   #     moments=[1, 2],\n",
    "   #     quantiles=[0.05, 0.95],\n",
    "   #     quantiles_inv=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "   #     diff_orders=[0, 1],\n",
    "   #     pre_op=lambda x: np.exp(2 * x),\n",
    "   #     pre_op_name=\"energy\"\n",
    "   # )\n",
    "    \n",
    "   # cols_with_rescale = [(col[0] + \"_with_rescale\", *col[1:]) for col in df_logmod_with_rescale.columns]\n",
    "   # df_logmod_with_rescale.columns = pd.MultiIndex.from_tuples(cols_with_rescale)\n",
    "    \n",
    "    ## END LOGMOD RENAME COLUMNS\n",
    "    \n",
    "    \n",
    "    df_time_diff_0 = make_input_new(\n",
    "        h5_file,\n",
    "        features=TIME_FEATURES,\n",
    "        rescale_by_subject=False,\n",
    "        # moments=[1, 2],\n",
    "        quantiles=[1e-4, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 1-1e-4],\n",
    "        diff_orders=[0]\n",
    "    )\n",
    "    \n",
    "    #df_time_diff_1 = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=TIME_FEATURES,\n",
    "    #    rescale_by_subject=False,\n",
    "    #    # moments=[1, 2],\n",
    "    #    quantiles=[1e-4, 1-1e-4],\n",
    "    #    diff_orders=[1]\n",
    "    #)\n",
    "    \n",
    "    #df_pulse_max_freq = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=[\"pulse_max_freq\"],\n",
    "    #    rescale_by_subject=True,\n",
    "    #    moments=[1],\n",
    "    #)\n",
    "    \n",
    "    #df_pulse_max_logE = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=[\"pulse_max_logE\"],\n",
    "    #    rescale_by_subject=False,\n",
    "    #    moments=[1],\n",
    "        #pre_op=np.exp,\n",
    "        #pre_op_name=\"energy\"\n",
    "    #)\n",
    "    \n",
    "    \n",
    "    return pd.concat([\n",
    "        df_bandlog,\n",
    "        df_sleep,\n",
    "        df_logmod, \n",
    "        #df_logmod_with_rescale, \n",
    "        df_time_diff_0,\n",
    "        #df_time_diff_1,\n",
    "        #df_pulse_max_freq,\n",
    "        #df_pulse_max_logE,\n",
    "        ],\n",
    "        axis=1)\n",
    "\n",
    "def shift_and_fill(df, shift):\n",
    "    shifted_df = df.shift(shift)\n",
    "    if shift > 0:\n",
    "        shifted_df.bfill(inplace=True)\n",
    "    elif shift < 0:\n",
    "        shifted_df.ffill(inplace=True)\n",
    "    return shifted_df\n",
    "\n",
    "\n",
    "def roll_and_concat(df, shifts_range):\n",
    "    return pd.concat(map(lambda shift: shift_and_fill(df, shift), shifts_range), \n",
    "                     axis=1, keys=shifts_range)    \n",
    "    \n",
    "def concat_windows(h5_file, df, shifts):\n",
    "    df = df.groupby(h5_file[\"index\"][:], as_index=False).apply(roll_and_concat, shifts_range=shifts)\n",
    "    return df\n",
    "    \n",
    "def make_input_default_test(h5_file):\n",
    "    return make_input_new(h5_file, [\"eeg_1\", \"eeg_2\"], moments=[1])\n",
    "\n",
    "def make_input_default_rolling(h5_file, shifts):\n",
    "    \"\"\"\n",
    "    !!! not suited for pca because columns have 3 levels\n",
    "    \"\"\"\n",
    "    df = make_input_default(h5_file)\n",
    "    df_with_window = concat_windows(h5_file, df, shifts)\n",
    "    return df_with_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature #10/10\u001b[1K\r"
     ]
    }
   ],
   "source": [
    "X_train_raw, X_test_raw = make_input_default(h5_train), make_input_default(h5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = get_subject_ids(h5_train)\n",
    "train_train_ids, train_val_ids = train_ids[:28], train_ids[28:]\n",
    "\n",
    "X_train_train = X_train_raw.loc[subjects_ids_to_indexers(h5_train, train_train_ids, as_indices=True), :]\n",
    "y_train_train = y_train_arr[subjects_ids_to_indexers(h5_train, train_train_ids, as_indices=True)]\n",
    "\n",
    "X_train_val = X_train_raw.loc[subjects_ids_to_indexers(h5_train, train_val_ids, as_indices=True), :]\n",
    "y_train_val = y_train_arr[subjects_ids_to_indexers(h5_train, train_val_ids, as_indices=True)]\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_ = MinMaxScaler()\n",
    "pca_ = PCA(0.99)\n",
    "\n",
    "X_train_train = pca_.fit_transform(scaler_.fit_transform(X_train_train))\n",
    "X_train_val = pca_.transform(scaler_.transform(X_train_val))\n",
    "X_test = pca_.transform(scaler_.transform(X_test_raw))\n",
    "\"\"\"\n",
    "def subjects_ids_col(h5_file):\n",
    "    return h5_file[\"index\"][:]\n",
    "\n",
    "def concat_windows(arr, subjects_ids, h5_file, shifts): # subjects_ids must be sorted\n",
    "    sid_col = subjects_ids_col(h5_file)\n",
    "    sid_col = sid_col[np.isin(sid_col, subjects_ids)]\n",
    "    df = pd.DataFrame(arr)\n",
    "    \n",
    "    return df.groupby(sid_col).apply(roll_and_concat, shifts_range=shifts)\n",
    "\n",
    "shifts = [-1, 0, 1]\n",
    "X_train_train_rolled = concat_windows(X_train_train, train_train_ids, h5_train, shifts)\n",
    "X_train_val_rolled = concat_windows(X_train_val, train_val_ids, h5_train, shifts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator_rf = RandomForestClassifier(\n",
    "    # verbose=1,\n",
    "    random_state=1,\n",
    "    # max_depth=10,\n",
    "    n_estimators=200, # default=100\n",
    "    # min_samples_leaf=10,\n",
    ")\n",
    "\n",
    "estimator_rf.fit(X_train_train_rolled, y_train_train)\n",
    "\n",
    "train_score_rf = custom_score(estimator_rf.predict(X_train_train_rolled), y_train_train)\n",
    "val_score_rf = custom_score(estimator_rf.predict(X_train_val_rolled), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655940496519377\n",
      "0.6360466510558397\n"
     ]
    }
   ],
   "source": [
    "print(train_score_rf)\n",
    "print(val_score_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Random Forest Params | Time Features Quantiles | Time Features Moments | Sleep Features | Pulse Freq (f_max, A_max) | Shifts | Comments | Training Score | Validation Score |\n",
    "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "| None | 0.1, 0.5, 0.9 | None | No | None | 0 | - | 1| 0.67|\n",
    "| None | 0.1, 0.5, 0.9 | None | Yes | None | 0 | - | 1 | 0.69|\n",
    "| None | 0.1, 0.5, 0.9 | None | Yes | None | -1, 0, 1 | - | 1 | 0.7|\n",
    "| None | 0.01, 0.1, 0.5, 0.9, 0.99 | None | Yes | None | -1, 0, 1 | - | 1| 0.7  |\n",
    "| `min_samples_leaf=10` | 0.01, 0.1, 0.5, 0.9, 0.99 | None | Yes | None | -1, 0, 1 | - | 0.89 | 0.69  |\n",
    "| `min_samples_leaf=10` | 0.01, 0.1, 0.5, 0.9, 0.99 | 1, 2 | Yes | None | -1, 0, 1 |  - | 0.89 | 0.69  |\n",
    "| None | 0.01, 0.1, 0.5, 0.9, 0.99 | 1, 2 | Yes | None | -1, 0, 1 |  - | 1 | 0.697  |\n",
    "| None | 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99 | None | Yes | None | -1, 0, 1 |  - | 1 | 0.708  |\n",
    "| None | 0.01, DECILES, 0.99 | None | Yes | None | -1, 0, 1 |  - | 1 | 0.709  |\n",
    "| `min_samples_leaf=10` | 0.01, DECILES, 0.99 | None | Yes | None | -1, 0, 1 |  - | 0.89 | 0.697 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | None | -1, 0, 1 |  - | 0.89 | 0.699 |\n",
    "| None | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | None | -1, 0, 1 |  - | 1 | 0.713929 |\n",
    "| None | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | Yes | -1, 0, 1 |  - | 1 | 0.7 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | Yes | -1, 0, 1 | - | 0.89 | 0.697 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | Pulse Only | -1, 0, 1 | - | 0.89 | 0.7 |\n",
    "| None | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | Pulse Only | -1, 0, 1 |  - | 1 | 0.7055 |\n",
    "| None | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | None | -1, 0, 1 | - |  1 | 0.7055 |\n",
    "| None | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee 0.5 | None | Yes | None | -1, 0, 1 |  - | 1 | 0.708 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee 0.5 | None | Yes | None | -1, 0, 1 |  - | 0.89 | 0.700|\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee MIN, MAX | None | Yes | None | -1, 0, 1 | - | 0.89| 0.7 |\n",
    "| None | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee MIN, MAX | None | Yes | None | -1, 0, 1 | - | 1| 0.703|\n",
    "| None | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | None | -1, 0, 1 | `bandlog rescaled`| 1| 0.665 |\n",
    "| None | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | None | -1, 0, 1 | `quantiles_inv = 10%, 90% for logmod`| 1| 0.7165 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | None | -1, 0, 1 | `quantiles_inv = 10%, 90% for logmod`| ? | 0.70 < x < 0.71 |\n",
    "| None | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | None | -1, 0, 1 | `quantiles_inv = ODD_DECILES for logmod`| 1 | 0.7371780518172594 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | None | Yes | None | -1, 0, 1 | `quantiles_inv = ODD_DECILES for logmod`| 0.898 | 0.721 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New submission file at submissions/rf_best_alternate_2021-01-01.csv\n"
     ]
    }
   ],
   "source": [
    "test_ids = get_subject_ids(h5_test)\n",
    "X_test_rolled = concat_windows(X_test_raw, test_ids, h5_test, shifts)\n",
    "y_pred = estimator_rf.predict(X_test_rolled)\n",
    "#submit_to_kaggle(y_pred, h5_test, fname='rf_best_alternate_2021-01-01.csv', msg=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_ = MinMaxScaler()\n",
    "\n",
    "X_train_train_rolled_svc = scaler_.fit_transform(X_train_train_rolled) \n",
    "X_train_val_rolled_svc = scaler_.transform(X_train_val_rolled)\n",
    "X_test_rolled_svc = scaler_.transform(X_test_rolled)\n",
    "\n",
    "\n",
    "estimator_svc = SVC(verbose=1, kernel='rbf', C=1, max_iter=1000, random_state=1)\n",
    "estimator_svc.fit(X_train_train_rolled_svc, y_train_train)\n",
    "\n",
    "train_score_svc = custom_score(estimator_svc.predict(X_train_train_rolled_svc), y_train_train)\n",
    "val_score_svc = custom_score(estimator_svc.predict(X_train_val_rolled_svc), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24980, 63)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rolled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1988, 243)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val_rolled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1988, 513)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val_rolled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add((np.array([[1, 2], [3, 4]]), np.array([[2, 3], [3, 4]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eeg_5', 'eeg_6', 'eeg_3', 'eeg_1', 'eeg_2', 'eeg_7', 'eeg_4', 'eeg_mean']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "[feat for feat in FEATURES + [\"eeg_mean\"] if re.search(\"^eeg_(?:\\d|(?:mean)){1}$\", feat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Couldn't delete link (callback link pointer is NULL (specified link may be '.' or not exist))\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7ac0d7835012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mh5_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eeg_mean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mh5_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eeg_mean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__delitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;34m\"\"\" Delete (unlink) an item from this group. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwith_phil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.unlink\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Couldn't delete link (callback link pointer is NULL (specified link may be '.' or not exist))\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python37164bitdc6ddf9b5234459bacda0ad4bcef452b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
