{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "from kaggle_submit import submit_to_kaggle\n",
    "\n",
    "\n",
    "from objects import *\n",
    "from helpers import *\n",
    "# from utils.globals import *\n",
    "from utils.distribution_statistics import *\n",
    "\n",
    "train_file = \"kaggle_data/X_train.h5/X_train.h5\"\n",
    "test_file = \"kaggle_data/X_test.h5/X_test.h5\"\n",
    "\n",
    "def get_train_test_connections():\n",
    "    h5_train = h5py.File(train_file, mode='a')\n",
    "    h5_test = h5py.File(test_file, mode='a')\n",
    "    return h5_train, h5_test\n",
    "\n",
    "def close_train_test_connections(h5_train, h5_test):\n",
    "    h5_train.close()\n",
    "    h5_test.close()\n",
    "    \n",
    "#h5_train, h5_test = get_train_test_connections()\n",
    "\n",
    "y_train = pd.read_csv(\"kaggle_data/y_train.csv\", index_col=0, squeeze=True)\n",
    "y_train_arr = y_train.to_numpy()\n",
    "\n",
    "# MAKE CUSTOM FEATURES\n",
    "#from additional_features.make_features import make_all_features\n",
    "#make_all_features(h5_train, h5_test, n_chunks=10, verbose=True, overwrite=False)\n",
    "#from additional_features.features_to_frequential import _create_log_modulus, BAND_EEG\n",
    "#from additional_features.eeg_band_signals import _create_band_signals\n",
    "#for h5_file in (h5_train, h5_test):\n",
    "#    _create_band_signals(h5_file, n_chunks=10, overwrite=False, verbose=True)\n",
    "#    _create_log_modulus(h5_file, n_chunks=10, features_to_convert=BAND_EEG, overwrite=False, verbose=True)\n",
    "\n",
    "# Close connections because utils.globals needs them\n",
    "#close_train_test_connections(h5_train, h5_test)\n",
    "\n",
    "from utils.globals import *\n",
    "\n",
    "h5_train, h5_test = get_train_test_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "eeg_nums = list(range(1, 8))\n",
    "greek_letters = [\"alpha\", \"beta\", \"delta\", \"theta\"]\n",
    "\n",
    "EEG_FEATURES = [f\"eeg_{i}\" for i in eeg_nums]\n",
    "EEG_BAND_FEATURES = [f\"{greek}_eeg_{i}\" for greek, i in itertools.product(greek_letters, eeg_nums)]\n",
    "OTHER_TIME_FEATURES = [\"speed_norm\", \"accel_norm\", \"pulse\"]\n",
    "\n",
    "EEG_LOGMOD_FEATURES = [f\"{eeg}_ft_logmod\" for eeg in EEG_FEATURES]\n",
    "EEG_BAND_LOGMOD_FEATURES = [f\"{eeg_band}_ft_logmod\" for eeg_band in EEG_BAND_FEATURES]\n",
    "OTHER_LOGMOD_FEATURES = [f\"{time_feat}_ft_logmod\" for time_feat in OTHER_TIME_FEATURES]\n",
    "\n",
    "SLEEP_FEATURES = [feat for feat in h5_train.keys() if \"sleep\" in feat]\n",
    "\n",
    "# OLD NAMES\n",
    "BAND_LOG_ENERGY_FEATURES_OLD = [f\"{greek}_{eeg}_logE\" for greek, eeg in itertools.product(greek_letters, EEG_FEATURES)]\n",
    "LOGMOD_FEATURES_OLD = EEG_LOGMOD_FEATURES + OTHER_LOGMOD_FEATURES\n",
    "TIME_FEATURES_OLD = EEG_FEATURES + OTHER_TIME_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sleep_left', 'sleep_time', 'sleep_time_relative']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLEEP_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eeg_1_ft_logmod', 'eeg_2_ft_logmod', 'eeg_3_ft_logmod', 'eeg_4_ft_logmod', 'eeg_5_ft_logmod', 'eeg_6_ft_logmod', 'eeg_7_ft_logmod', 'speed_norm_ft_logmod', 'accel_norm_ft_logmod', 'pulse_ft_logmod']\n"
     ]
    }
   ],
   "source": [
    "print(LOGMOD_FEATURES_OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_default(h5_file):\n",
    "    \n",
    "    dfs = list()\n",
    "    \n",
    "    dfs.append( # df_bandlog\n",
    "        make_input_new(\n",
    "            h5_file,\n",
    "            features=BAND_LOG_ENERGY_FEATURES_OLD,\n",
    "            rescale_by_subject=False,\n",
    "            moments=[1],\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #dfs.append( # df_spectral_entropy = \n",
    "    #    make_input_new(\n",
    "    #        h5_file,\n",
    "    #        features=EEG_LOGMOD_FEATURES,\n",
    "    #        rescale_by_subject=False,\n",
    "    #        entropy=True,\n",
    "    #        pre_op=lambda x: np.exp(2 * x),\n",
    "    #        pre_op_name='energy'\n",
    "    #    )\n",
    "    #)\n",
    "    \n",
    "    #dfs.append( # df_time_hjorth = \n",
    "    #    make_input_new(\n",
    "    #        h5_file,\n",
    "    #        features=EEG_FEATURES,\n",
    "    #        rescale_by_subject=False,\n",
    "    #        hjorth=True,\n",
    "            #pre_op=lambda x: np.exp(2 * x),\n",
    "            #pre_op_name='energy'\n",
    "    #    )\n",
    "    #)\n",
    "        \n",
    "    dfs.append( # df_sleep = \n",
    "        make_input_new(\n",
    "            h5_file,\n",
    "            features=SLEEP_FEATURES[:2],\n",
    "            rescale_by_subject=False,\n",
    "            moments=[1]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ## LOGMOD RENAME COLUMNS\n",
    "    \n",
    "    dfs.append( # df_logmod = \n",
    "        make_input_new(\n",
    "            h5_file,\n",
    "            features=OTHER_LOGMOD_FEATURES,\n",
    "            rescale_by_subject=False,\n",
    "            #interquantiles=[(0.2, 0.8)],\n",
    "            quantiles_inv=[0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99],\n",
    "            diff_orders=[0],\n",
    "            interquantiles_inv=[(0.1, 0.9), (0.3, 0.7)],\n",
    "        )\n",
    "    )\n",
    "    \n",
    " \n",
    "    dfs.append( # df_time_diff_0 = \n",
    "        make_input_new(\n",
    "            h5_file,\n",
    "            features=sorted(set(TIME_FEATURES_OLD) - {\"speed_norm\"}),\n",
    "            rescale_by_subject=False,\n",
    "            hjorth=True, mmd=True,\n",
    "            # hjorth=True,\n",
    "            # moments=[1, 2],\n",
    "            quantiles=[1e-4, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 1-1e-4],\n",
    "            # interquantiles=[(0.1, 0.9), (0.3, 0.7)],\n",
    "            diff_orders=[0]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    dfs.append( # df_eeg_band_logmod = \n",
    "        make_input_new(\n",
    "            h5_file,\n",
    "            features=EEG_BAND_LOGMOD_FEATURES,\n",
    "            rescale_by_subject=False,\n",
    "            entropy=True, renyi_entropy=True,\n",
    "            # hjorth=True,\n",
    "            # moments=[1, 2],\n",
    "            #quantiles=[1e-4, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 1-1e-4],\n",
    "            #interquantiles=[(0.1, 0.9), (0.3, 0.7)],\n",
    "            diff_orders=[0],\n",
    "            pre_op=lambda x: np.exp(2*x),\n",
    "            pre_op_name='energy',\n",
    "            post_op=np.log,\n",
    "            post_op_name='log',\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    #dfs.append( # df_eeg_bandlog =\n",
    "    #    make_input_new(\n",
    "    #        h5_file,\n",
    "    #        features=EEG_LOGMOD_FEATURES,\n",
    "    #        renyi_entropy=True,\n",
    "    #        diff_orders=[0],\n",
    "    #        pre_op=lambda x: np.exp(2*x),\n",
    "    #        pre_op_name='energy',\n",
    "    #        post_op=np.log,\n",
    "    #        post_op_name='log'\n",
    "    #    )\n",
    "    #)\n",
    "    #dfs.append( # df_eeg_bands =\n",
    "    #    make_input_new(\n",
    "    #        h5_file,\n",
    "    #        features=EEG_BAND_FEATURES,\n",
    "    #       hjorth=True,\n",
    "    #       diff_orders=[0]\n",
    "    #    )\n",
    "    #)\n",
    "    \n",
    "    # EEG INTEGRATED\n",
    "    #eeg_features = [feat for feat in FEATURES if re.search('^eeg_\\d$', feat)]\n",
    "    # print(eeg_features)\n",
    "    #df_eeg_integrated = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=eeg_features,\n",
    "    #    rescale_by_subject=False,\n",
    "    #    # moments=[1, 2],\n",
    "    #    quantiles=[0.1, 0.5, 0.9],\n",
    "    #    #interquantiles=[(0.1, 0.9), (0.3, 0.7)],\n",
    "    #    diff_orders=[-1]\n",
    "    #)\n",
    "    \n",
    "    \n",
    "    #df_time_diff_1 = make_input_new(\n",
    "     #   h5_file,\n",
    "     #   features=TIME_FEATURES_OLD,\n",
    "     #   rescale_by_subject=False,\n",
    "     #   moments=[1, 2],\n",
    "        #quantiles=[1e-4, 1-1e-4],\n",
    "     #   diff_orders=[1]\n",
    "    #)\n",
    "    \n",
    "    #df_pulse_max_freq = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=[\"pulse_max_freq\"],\n",
    "    #    rescale_by_subject=True,\n",
    "    #    moments=[1],\n",
    "    #)\n",
    "    \n",
    "    #df_pulse_max_logE = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=[\"pulse_max_logE\"],\n",
    "    #    rescale_by_subject=False,\n",
    "    #    moments=[1],\n",
    "        #pre_op=np.exp,\n",
    "        #pre_op_name=\"energy\"\n",
    "    #)\n",
    "    \n",
    "    \n",
    "    res_df = pd.concat(dfs, axis=1, keys=[str(i) for i in range(len(dfs))])\n",
    "    \n",
    "    # Filling policy\n",
    "    missing_values = res_df.isna().sum(axis=0)\n",
    "    missing_values = missing_values.loc[missing_values > 0]\n",
    "    if len(missing_values) > 0:\n",
    "        print(\"Missing values :\")\n",
    "        print(missing_values)\n",
    "        print(\"Filling missing values with zero\")\n",
    "        res_df = res_df.fillna(0)\n",
    "        \n",
    "    return res_df\n",
    "\n",
    "def shift_and_fill(df, shift):\n",
    "    shifted_df = df.shift(shift)\n",
    "    if shift > 0:\n",
    "        shifted_df.bfill(inplace=True)\n",
    "    elif shift < 0:\n",
    "        shifted_df.ffill(inplace=True)\n",
    "    return shifted_df\n",
    "\n",
    "\n",
    "def roll_and_concat(df, shifts_range):\n",
    "    return pd.concat(map(lambda shift: shift_and_fill(df, shift), shifts_range), \n",
    "                     axis=1, keys=shifts_range)    \n",
    "    \n",
    "def concat_windows(df, ids, shifts):\n",
    "    df = df.groupby(ids, as_index=False).apply(roll_and_concat, shifts_range=shifts)\n",
    "    return df\n",
    "    \n",
    "def make_input_default_test(h5_file):\n",
    "    return make_input_new(h5_file, [\"eeg_1\", \"eeg_2\"], moments=[1])\n",
    "\n",
    "def make_input_default_rolling(h5_file, shifts):\n",
    "    \"\"\"\n",
    "    !!! not suited for pca because columns have 3 levels\n",
    "    \"\"\"\n",
    "    df = make_input_default(h5_file)\n",
    "    df_with_window = concat_windows(h5_file, df, shifts)\n",
    "    return df_with_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature #28/28\u001b[1K\r"
     ]
    }
   ],
   "source": [
    "X_train_raw = make_input_default(h5_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "concat_windows() missing 1 required positional argument: 'shifts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-15fa607a63c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mshifts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mX_train_train_rolled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_train_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshifts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mX_train_val_rolled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_val_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshifts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: concat_windows() missing 1 required positional argument: 'shifts'"
     ]
    }
   ],
   "source": [
    "train_ids = get_subject_ids(h5_train)\n",
    "np.random.seed(1)\n",
    "train_ids = np.random.permutation(train_ids)\n",
    "train_train_ids, train_val_ids = sorted(train_ids[:28]), sorted(train_ids[28:])\n",
    "#train_train_ids, train_val_ids = train_ids[:28], train_ids[28:]\n",
    "\n",
    "X_train_train = X_train_raw.loc[subjects_ids_to_indexers(h5_train, train_train_ids, as_indices=True), :]\n",
    "y_train_train = y_train_arr[subjects_ids_to_indexers(h5_train, train_train_ids, as_indices=True)]\n",
    "\n",
    "X_train_val = X_train_raw.loc[subjects_ids_to_indexers(h5_train, train_val_ids, as_indices=True), :]\n",
    "y_train_val = y_train_arr[subjects_ids_to_indexers(h5_train, train_val_ids, as_indices=True)]\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_ = MinMaxScaler()\n",
    "pca_ = PCA(0.99)\n",
    "\n",
    "X_train_train = pca_.fit_transform(scaler_.fit_transform(X_train_train))\n",
    "X_train_val = pca_.transform(scaler_.transform(X_train_val))\n",
    "X_test = pca_.transform(scaler_.transform(X_test_raw))\n",
    "\"\"\"\n",
    "def subjects_ids_col(h5_file):\n",
    "    return h5_file[\"index\"][:]\n",
    "\n",
    "def concat_windows(arr, subjects_ids, h5_file, shifts): # subjects_ids must be sorted\n",
    "    sid_col = subjects_ids_col(h5_file)\n",
    "    sid_col = sid_col[np.isin(sid_col, subjects_ids)]\n",
    "    df = pd.DataFrame(arr)\n",
    "    \n",
    "    return df.groupby(sid_col).apply(roll_and_concat, shifts_range=shifts)\n",
    "\n",
    "shifts = [-1, 0, 1]\n",
    "X_train_train_rolled = concat_windows(X_train_train, train_train_ids, h5_train, shifts)\n",
    "X_train_val_rolled = concat_windows(X_train_val, train_val_ids, h5_train, shifts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 0, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    " \n",
    "class VotingClassifier_:\n",
    "    \"\"\" Implements a voting classifier for pre-trained classifiers\"\"\"\n",
    "\n",
    "    def __init__(self, estimators):\n",
    "        self.estimators = estimators\n",
    "        # self.weights = weights\n",
    "\n",
    "    def predict_hard(self, X, weights=None):\n",
    "        # get values\n",
    "        Y = np.zeros([X.shape[0], len(self.estimators)], dtype=int)\n",
    "        for i, clf in enumerate(self.estimators):\n",
    "            Y[:, i] = clf.predict(X)\n",
    "        # apply voting \n",
    "        y = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y[i] = np.argmax(np.bincount(Y[i,:], weights=weights))\n",
    "        return y\n",
    "\n",
    "    def predict_soft(self, X, weights=None):\n",
    "        # get values\n",
    "        if weights is None:\n",
    "            weights = np.ones((len(self.estimators),), dtype=float)\n",
    "        Y = np.zeros([X.shape[0], len(self.estimators), 5], dtype=float)\n",
    "        for i, clf in enumerate(self.estimators):\n",
    "            Y[:, i, :] = clf.predict_proba(X)\n",
    "        Y_proba = np.sum(Y * np.array(weights).reshape((1, len(self.estimators), 1)), axis=1)\n",
    "        # apply voting \n",
    "        y = np.zeros(X.shape[0], dtype=float)\n",
    "        # print(weights.dtype, Y.dtype, Y_proba.dtype)\n",
    "        for i in range(X.shape[0]):\n",
    "            y[i] = np.argmax(Y_proba[i, :])\n",
    "        return y\n",
    "\n",
    "    \n",
    "# Random Forest\n",
    "rfc_ = RandomForestClassifier(random_state=1, n_estimators=300, verbose=1, n_jobs=-2)\n",
    "rfc_.fit(X_train_train_rolled, y_train_train)\n",
    "print(\"Random Forest validation score :\", custom_score(rfc_.predict(X_train_val_rolled), y_train_val))\n",
    "\n",
    "# Extra Trees\n",
    "#etc_ = ExtraTreesClassifier(verbose=1, random_state=1, n_estimators=1000, n_jobs=-2)\n",
    "#etc_.fit(X_train_train_rolled, y_train_train)\n",
    "#print(\"Extra Trees validation score :\", custom_score(etc_.predict(X_train_val_rolled), y_train_val))\n",
    "\n",
    "# Bagging\n",
    "#bc_ = BaggingClassifier(verbose=1, random_state=1, n_estimators=70, n_jobs=-2)\n",
    "#bc_.fit(X_train_train_rolled, y_train_train)\n",
    "#print('Bagging validation score :', custom_score(bc_.predict(X_train_val_rolled), y_train_val))\n",
    "\n",
    "# Gradient Boosting\n",
    "#hgb_ = HistGradientBoostingClassifier(verbose=1, random_state=1, max_iter=1000)\n",
    "#print(\"fit\")\n",
    "#hgb_.fit(X_train_train_rolled, y_train_train)\n",
    "#print('HistGradientBoosting validation score :', custom_score(hgb_.predict(X_train_val_rolled), y_train_val))\n",
    "     \n",
    "\n",
    "meta_estimator = VotingClassifier_([rfc_, etc_, bc_, hgb_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual predictions\n",
      "RandomForestClassifier(n_jobs=-2, random_state=1, verbose=1) -----> 0.7192152460425681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    1.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(n_estimators=1000, n_jobs=-2, random_state=1, verbose=1) -----> 0.6991284274439796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(n_estimators=70, n_jobs=-2, random_state=1, verbose=1) -----> 0.7254172221127266\n",
      "HistGradientBoostingClassifier(max_iter=1000, random_state=1, verbose=1) -----> 0.796072989711446\n",
      "Hard prediction weights=[1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7298699342541014\n",
      "Hard prediction weights=[0.2, 0.2, 0.2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796072989711446\n",
      "Hard prediction weights=[0.4, 0.4, 0.4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7755866640989398\n",
      "Hard prediction weights=[0.7, 0.7, 0.7, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7659853502503688\n",
      "Soft prediction weights=[1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7715646707697053\n",
      "Soft prediction weights=[0.2, 0.2, 0.2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7957171332063077\n",
      "Soft prediction weights=[0.4, 0.4, 0.4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7880804187412094\n",
      "Soft prediction weights=[0.7, 0.7, 0.7, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7787980580047907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Individual predictions\")\n",
    "for est in meta_estimator.estimators:\n",
    "    print(est, \"----->\", custom_score(est.predict(X_train_val_rolled), y_train_val))\n",
    "\n",
    "print(\"Hard prediction weights=[1, 1, 1, 1]\")\n",
    "print(custom_score(meta_estimator.predict_hard(X_train_val_rolled, weights=[1, 1, 1, 1]), y_train_val))\n",
    "    \n",
    "print(\"Hard prediction weights=[0.2, 0.2, 0.2, 1]\")\n",
    "print(custom_score(meta_estimator.predict_hard(X_train_val_rolled, weights=[0.2, 0.2, 0.2, 1]), y_train_val))\n",
    "\n",
    "print(\"Hard prediction weights=[0.4, 0.4, 0.4, 1]\")\n",
    "print(custom_score(meta_estimator.predict_hard(X_train_val_rolled, weights=[0.4, 0.4, 0.4, 1]), y_train_val))\n",
    "\n",
    "print(\"Hard prediction weights=[0.7, 0.7, 0.7, 1]\")\n",
    "print(custom_score(meta_estimator.predict_hard(X_train_val_rolled, weights=[0.7, 0.7, 0.7, 1]), y_train_val))\n",
    "\n",
    "print(\"Soft prediction weights=[1, 1, 1, 1]\")\n",
    "print(custom_score(meta_estimator.predict_soft(X_train_val_rolled, weights=[1, 1, 1, 1]), y_train_val))\n",
    "    \n",
    "print(\"Soft prediction weights=[0.2, 0.2, 0.2, 1]\")\n",
    "print(custom_score(meta_estimator.predict_soft(X_train_val_rolled, weights=[0.2, 0.2, 0.2, 1]), y_train_val))\n",
    "\n",
    "print(\"Soft prediction weights=[0.4, 0.4, 0.4, 1]\")\n",
    "print(custom_score(meta_estimator.predict_soft(X_train_val_rolled, weights=[0.4, 0.4, 0.4, 1]), y_train_val))\n",
    "\n",
    "print(\"Soft prediction weights=[0.7, 0.7, 0.7, 1]\")\n",
    "print(custom_score(meta_estimator.predict_soft(X_train_val_rolled, weights=[0.7, 0.7, 0.7, 1]), y_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft prediction weights=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7774488783662253\n",
      "Soft prediction weights=[0.2, 0.2, 0.2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7773833762779745\n",
      "Soft prediction weights=[0.4, 0.4, 0.4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7804111096740698\n",
      "Soft prediction weights=[0.7, 0.7, 0.7, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.781832453189057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_base = RandomForestClassifier(verbose=1, random_state=1, n_estimators=100, n_jobs=-2)\n",
    "rf_base.fit(X_train_train_rolled, y_train_train)\n",
    "custom_score(rf_base.predict(X_train_val_rolled), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "Binning 0.139 GB of training data: 2.250 s\n",
      "Binning 0.016 GB of validation data: 0.066 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 5 trees, 155 leaves (31 on avg), max depth = 9, train loss: 1.25911, val loss: 1.27682, in 1.398s\n",
      "[2/100] 5 trees, 155 leaves (31 on avg), max depth = 9, train loss: 1.11466, val loss: 1.14326, in 0.692s\n",
      "[3/100] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 1.00447, val loss: 1.04346, in 0.716s\n",
      "[4/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.91655, val loss: 0.96353, in 1.277s\n",
      "[5/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.84523, val loss: 0.89942, in 1.722s\n",
      "[6/100] 5 trees, 155 leaves (31 on avg), max depth = 9, train loss: 0.78476, val loss: 0.84468, in 0.727s\n",
      "[7/100] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 0.73287, val loss: 0.80091, in 1.042s\n",
      "[8/100] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 0.68810, val loss: 0.76328, in 1.611s\n",
      "[9/100] 5 trees, 155 leaves (31 on avg), max depth = 9, train loss: 0.64900, val loss: 0.72985, in 1.629s\n",
      "[10/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.61428, val loss: 0.70137, in 0.703s\n",
      "[11/100] 5 trees, 155 leaves (31 on avg), max depth = 9, train loss: 0.58262, val loss: 0.67602, in 0.750s\n",
      "[12/100] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 0.55474, val loss: 0.65282, in 0.943s\n",
      "[13/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.52969, val loss: 0.63345, in 1.829s\n",
      "[14/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.50627, val loss: 0.61547, in 0.776s\n",
      "[15/100] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 0.48479, val loss: 0.59876, in 1.054s\n",
      "[16/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.46566, val loss: 0.58435, in 0.946s\n",
      "[17/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.44767, val loss: 0.57014, in 2.153s\n",
      "[18/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.43090, val loss: 0.55800, in 0.958s\n",
      "[19/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.41536, val loss: 0.54701, in 0.875s\n",
      "[20/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.40061, val loss: 0.53640, in 0.795s\n",
      "[21/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.38644, val loss: 0.52623, in 1.767s\n",
      "[22/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.37321, val loss: 0.51754, in 0.853s\n",
      "[23/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.36068, val loss: 0.50886, in 0.874s\n",
      "[24/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.34903, val loss: 0.49994, in 0.932s\n",
      "[25/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.33840, val loss: 0.49270, in 1.801s\n",
      "[26/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.32724, val loss: 0.48568, in 0.740s\n",
      "[27/100] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 0.31772, val loss: 0.47996, in 0.849s\n",
      "[28/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.30844, val loss: 0.47460, in 1.624s\n",
      "[29/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.29940, val loss: 0.46977, in 1.730s\n",
      "[30/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.29081, val loss: 0.46369, in 0.946s\n",
      "[31/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.28241, val loss: 0.45890, in 0.814s\n",
      "[32/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.27415, val loss: 0.45339, in 1.309s\n",
      "[33/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.26597, val loss: 0.44926, in 1.996s\n",
      "[34/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.25854, val loss: 0.44539, in 1.049s\n",
      "[35/100] 5 trees, 155 leaves (31 on avg), max depth = 18, train loss: 0.25146, val loss: 0.44183, in 0.962s\n",
      "[36/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.24472, val loss: 0.43824, in 1.870s\n",
      "[37/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.23766, val loss: 0.43471, in 1.935s\n",
      "[38/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.23129, val loss: 0.43133, in 1.012s\n",
      "[39/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.22505, val loss: 0.42863, in 0.865s\n",
      "[40/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.21884, val loss: 0.42560, in 1.683s\n",
      "[41/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.21326, val loss: 0.42217, in 1.310s\n",
      "[42/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.20781, val loss: 0.41947, in 0.759s\n",
      "[43/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.20241, val loss: 0.41696, in 0.783s\n",
      "[44/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.19724, val loss: 0.41445, in 1.930s\n",
      "[45/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.19223, val loss: 0.41257, in 1.421s\n",
      "[46/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.18746, val loss: 0.40980, in 0.864s\n",
      "[47/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.18300, val loss: 0.40774, in 0.958s\n",
      "[48/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.17859, val loss: 0.40648, in 1.704s\n",
      "[49/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.17387, val loss: 0.40382, in 1.434s\n",
      "[50/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.16980, val loss: 0.40193, in 0.931s\n",
      "[51/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.16569, val loss: 0.39964, in 1.182s\n",
      "[52/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.16177, val loss: 0.39812, in 1.839s\n",
      "[53/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.15775, val loss: 0.39634, in 1.550s\n",
      "[54/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.15388, val loss: 0.39414, in 1.682s\n",
      "[55/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.15035, val loss: 0.39181, in 2.199s\n",
      "[56/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.14678, val loss: 0.39034, in 2.460s\n",
      "[57/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.14337, val loss: 0.38959, in 1.450s\n",
      "[58/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.13992, val loss: 0.38750, in 0.758s\n",
      "[59/100] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.13669, val loss: 0.38570, in 0.827s\n",
      "[60/100] 5 trees, 155 leaves (31 on avg), max depth = 18, train loss: 0.13349, val loss: 0.38502, in 1.934s\n",
      "[61/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.13052, val loss: 0.38417, in 1.888s\n",
      "[62/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.12754, val loss: 0.38257, in 1.010s\n",
      "[63/100] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.12459, val loss: 0.38214, in 1.101s\n",
      "[64/100] 5 trees, 155 leaves (31 on avg), max depth = 17, train loss: 0.12185, val loss: 0.38136, in 1.589s\n",
      "[65/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.11900, val loss: 0.38096, in 1.320s\n",
      "[66/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.11641, val loss: 0.38043, in 0.939s\n",
      "[67/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.11381, val loss: 0.38015, in 0.893s\n",
      "[68/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.11123, val loss: 0.37914, in 1.515s\n",
      "[69/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.10882, val loss: 0.37839, in 1.353s\n",
      "[70/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.10644, val loss: 0.37711, in 0.806s\n",
      "[71/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.10416, val loss: 0.37603, in 0.806s\n",
      "[72/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.10190, val loss: 0.37553, in 1.741s\n",
      "[73/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.09964, val loss: 0.37485, in 1.489s\n",
      "[74/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.09760, val loss: 0.37430, in 0.807s\n",
      "[75/100] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 0.09546, val loss: 0.37367, in 0.862s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.09353, val loss: 0.37282, in 1.771s\n",
      "[77/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.09138, val loss: 0.37172, in 1.580s\n",
      "[78/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.08944, val loss: 0.37184, in 0.952s\n",
      "[79/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.08765, val loss: 0.37121, in 1.051s\n",
      "[80/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.08584, val loss: 0.37071, in 2.200s\n",
      "[81/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.08404, val loss: 0.37056, in 2.828s\n",
      "[82/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.08228, val loss: 0.37020, in 0.904s\n",
      "[83/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.08053, val loss: 0.36921, in 0.794s\n",
      "[84/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.07879, val loss: 0.36839, in 0.840s\n",
      "[85/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.07698, val loss: 0.36739, in 1.041s\n",
      "[86/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.07533, val loss: 0.36743, in 0.734s\n",
      "[87/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.07375, val loss: 0.36673, in 0.758s\n",
      "[88/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.07223, val loss: 0.36642, in 0.760s\n",
      "[89/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.07063, val loss: 0.36621, in 1.056s\n",
      "[90/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.06913, val loss: 0.36585, in 0.745s\n",
      "[91/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.06772, val loss: 0.36586, in 1.060s\n",
      "[92/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.06636, val loss: 0.36520, in 0.941s\n",
      "[93/100] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.06501, val loss: 0.36523, in 1.192s\n",
      "[94/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.06369, val loss: 0.36505, in 0.712s\n",
      "[95/100] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.06241, val loss: 0.36441, in 0.982s\n",
      "[96/100] 5 trees, 155 leaves (31 on avg), max depth = 19, train loss: 0.06116, val loss: 0.36392, in 1.225s\n",
      "[97/100] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.05981, val loss: 0.36378, in 1.125s\n",
      "[98/100] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.05855, val loss: 0.36352, in 0.782s\n",
      "[99/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.05736, val loss: 0.36321, in 0.918s\n",
      "[100/100] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.05617, val loss: 0.36239, in 0.803s\n",
      "Fit 500 trees in 125.478 s, (15500 total leaves)\n",
      "Time spent computing histograms: 68.720s\n",
      "Time spent finding best splits:  27.258s\n",
      "Time spent applying splits:      14.915s\n",
      "Time spent predicting:           0.494s\n",
      "predict\n",
      "0.7857309121615127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "gg = HistGradientBoostingClassifier(verbose=1, random_state=1, max_iter=100)\n",
    "print(\"fit\")\n",
    "gg.fit(X_train_train_rolled, y_train_train)\n",
    "print(\"predict\")\n",
    "print(custom_score(gg.predict(X_train_val_rolled), y_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ens_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-91b63eef688f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mens_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_val_rolled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ens_model' is not defined"
     ]
    }
   ],
   "source": [
    "for est in ens_model.estimators_:\n",
    "    print(est)\n",
    "    print(custom_score(est.predict(X_train_val_rolled), y_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(random_state=1, verbose=1),\n",
       " BaggingClassifier(random_state=1, verbose=1),\n",
       " GradientBoostingClassifier(random_state=1, verbose=1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens_model.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   17.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest validation score : 0.7585675942465293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-2)]: Done 444 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-2)]: Done 794 tasks      | elapsed:   31.9s\n",
      "[Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed:   40.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees validation score : 0.7151015595476256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:  6.5min finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging validation score : 0.7614552994819377\n",
      "Gradient Boosting validation score : 0.7676057390387728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'refit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a8b52fd3ae8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmeta_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnsembleVoteClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrfc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbc_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'soft'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Meta validation score :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_val_rolled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'refit'"
     ]
    }
   ],
   "source": [
    "# ! pip3 install mlxtend\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "\n",
    "# Random Forest\n",
    "rfc_ = RandomForestClassifier(random_state=6, n_estimators=100, verbose=1, n_jobs=-2)\n",
    "rfc_.fit(X_train_train_rolled, y_train_train)\n",
    "print(\"Random Forest validation score :\", custom_score(rfc_.predict(X_train_val_rolled), y_train_val))\n",
    "\n",
    "# Extra Trees\n",
    "etc_ = ExtraTreesClassifier(verbose=1, random_state=1, n_estimators=1000, n_jobs=-2)\n",
    "etc_.fit(X_train_train_rolled, y_train_train)\n",
    "print(\"Extra Trees validation score :\", custom_score(etc_.predict(X_train_val_rolled), y_train_val))\n",
    "\n",
    "# Bagging\n",
    "bc_ = BaggingClassifier(verbose=1, random_state=1, n_estimators=70, n_jobs=-2)\n",
    "bc_.fit(X_train_train_rolled, y_train_train)\n",
    "print('Bagging validation score :', custom_score(bc_.predict(X_train_val_rolled), y_train_val))\n",
    "\n",
    "# Gradient Boosting\n",
    "\n",
    "meta_estimator = EnsembleVoteClassifier(clfs=[rfc_, etc_, bc_, gbc_], weights=[1, 1, 1, 1], refit=False, voting='soft')\n",
    "print('Meta validation score :', custom_score(meta_estimator.predict(X_train_val_rolled), y_train_val))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7665147580716193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from joblib import load\n",
    "\n",
    "#ens_model_orig = load(\"models_archives/\")\n",
    "#gbc_ = ens_model_orig.estimators_[-1]\n",
    "\n",
    "\n",
    "class VotingClassifier_(object):\n",
    "    \"\"\" Implements a voting classifier for pre-trained classifiers\"\"\"\n",
    "\n",
    "    def __init__(self, estimators, weights):\n",
    "        self.estimators = estimators\n",
    "        self.weights = weights\n",
    "\n",
    "    def predict(self, X):\n",
    "        # get values\n",
    "        Y = np.zeros([X.shape[0], len(self.estimators)], dtype=int)\n",
    "        for i, clf in enumerate(self.estimators):\n",
    "            Y[:, i] = clf.predict(X)\n",
    "        # apply voting \n",
    "        y = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            y[i] = np.argmax(np.bincount(Y[i,:], weights=self.weights))\n",
    "        return y\n",
    "    \n",
    "z = VotingClassifier_([rfc_, etc_, bc_, gg], weights=[0.6, 0, 0.6, 0.6])\n",
    "pred = z.predict(X_train_val_rolled)\n",
    "print(custom_score(pred, y_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7472935982285043"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_score(pred, y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "Binning 0.103 GB of training data: 2.037 s\n",
      "Binning 0.011 GB of validation data: 0.068 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/1000] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 1.27188, val loss: 1.29291, in 0.777s\n",
      "[2/1000] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 1.12838, val loss: 1.16196, in 0.707s\n",
      "[3/1000] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 1.01860, val loss: 1.06476, in 0.810s\n",
      "[4/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.93048, val loss: 0.98763, in 0.937s\n",
      "[5/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.85840, val loss: 0.92206, in 1.221s\n",
      "[6/1000] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.79825, val loss: 0.87061, in 1.028s\n",
      "[7/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.74767, val loss: 0.82639, in 0.703s\n",
      "[8/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.70255, val loss: 0.78754, in 0.557s\n",
      "[9/1000] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 0.66352, val loss: 0.75610, in 0.963s\n",
      "[10/1000] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.62929, val loss: 0.72573, in 0.599s\n",
      "[11/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.59747, val loss: 0.70022, in 0.628s\n",
      "[12/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.56963, val loss: 0.67729, in 0.607s\n",
      "[13/1000] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.54449, val loss: 0.65809, in 1.036s\n",
      "[14/1000] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.52074, val loss: 0.63849, in 0.597s\n",
      "[15/1000] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 0.49890, val loss: 0.62148, in 0.587s\n",
      "[16/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.47919, val loss: 0.60575, in 0.663s\n",
      "[17/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.46085, val loss: 0.59183, in 1.022s\n",
      "[18/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.44407, val loss: 0.57875, in 0.586s\n",
      "[19/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.42826, val loss: 0.56584, in 0.592s\n",
      "[20/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.41303, val loss: 0.55442, in 0.887s\n",
      "[21/1000] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.39905, val loss: 0.54443, in 1.315s\n",
      "[22/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.38608, val loss: 0.53553, in 0.540s\n",
      "[23/1000] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.37393, val loss: 0.52744, in 0.553s\n",
      "[24/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.36234, val loss: 0.51903, in 0.638s\n",
      "[25/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.35166, val loss: 0.51215, in 1.324s\n",
      "[26/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.34089, val loss: 0.50418, in 0.549s\n",
      "[27/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.33055, val loss: 0.49648, in 0.578s\n",
      "[28/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.32066, val loss: 0.49081, in 0.531s\n",
      "[29/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.31153, val loss: 0.48476, in 1.455s\n",
      "[30/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.30277, val loss: 0.47937, in 0.905s\n",
      "[31/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.29448, val loss: 0.47518, in 1.049s\n",
      "[32/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.28650, val loss: 0.47060, in 0.900s\n",
      "[33/1000] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.27910, val loss: 0.46576, in 1.626s\n",
      "[34/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.27143, val loss: 0.46154, in 0.840s\n",
      "[35/1000] 5 trees, 155 leaves (31 on avg), max depth = 18, train loss: 0.26430, val loss: 0.45741, in 0.620s\n",
      "[36/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.25713, val loss: 0.45325, in 0.698s\n",
      "[37/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.25050, val loss: 0.45031, in 1.359s\n",
      "[38/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.24430, val loss: 0.44644, in 0.560s\n",
      "[39/1000] 5 trees, 155 leaves (31 on avg), max depth = 18, train loss: 0.23801, val loss: 0.44337, in 0.644s\n",
      "[40/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.23218, val loss: 0.44014, in 0.784s\n",
      "[41/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.22658, val loss: 0.43686, in 1.302s\n",
      "[42/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.22100, val loss: 0.43426, in 0.608s\n",
      "[43/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.21563, val loss: 0.43067, in 0.569s\n",
      "[44/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.21046, val loss: 0.42830, in 0.776s\n",
      "[45/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.20544, val loss: 0.42633, in 1.299s\n",
      "[46/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.20012, val loss: 0.42322, in 0.641s\n",
      "[47/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.19534, val loss: 0.42083, in 0.601s\n",
      "[48/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.19094, val loss: 0.41841, in 0.881s\n",
      "[49/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.18623, val loss: 0.41764, in 1.303s\n",
      "[50/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.18207, val loss: 0.41601, in 0.682s\n",
      "[51/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.17810, val loss: 0.41420, in 0.764s\n",
      "[52/1000] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.17416, val loss: 0.41223, in 1.290s\n",
      "[53/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.17023, val loss: 0.41094, in 1.094s\n",
      "[54/1000] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.16627, val loss: 0.40929, in 0.632s\n",
      "[55/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.16256, val loss: 0.40816, in 0.583s\n",
      "[56/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.15907, val loss: 0.40738, in 0.878s\n",
      "[57/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.15552, val loss: 0.40576, in 1.068s\n",
      "[58/1000] 5 trees, 155 leaves (31 on avg), max depth = 17, train loss: 0.15233, val loss: 0.40512, in 0.589s\n",
      "[59/1000] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.14908, val loss: 0.40448, in 0.565s\n",
      "[60/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.14600, val loss: 0.40394, in 1.003s\n",
      "[61/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.14280, val loss: 0.40257, in 1.030s\n",
      "[62/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.13992, val loss: 0.40182, in 0.505s\n",
      "[63/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.13697, val loss: 0.40063, in 0.605s\n",
      "[64/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.13421, val loss: 0.39906, in 1.162s\n",
      "[65/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.13128, val loss: 0.39772, in 0.961s\n",
      "[66/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.12860, val loss: 0.39655, in 0.607s\n",
      "[67/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.12588, val loss: 0.39562, in 0.562s\n",
      "[68/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.12294, val loss: 0.39345, in 0.856s\n",
      "[69/1000] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.12033, val loss: 0.39184, in 1.125s\n",
      "[70/1000] 5 trees, 155 leaves (31 on avg), max depth = 19, train loss: 0.11781, val loss: 0.39077, in 0.501s\n",
      "[71/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.11536, val loss: 0.38985, in 0.558s\n",
      "[72/1000] 5 trees, 155 leaves (31 on avg), max depth = 10, train loss: 0.11300, val loss: 0.38919, in 0.911s\n",
      "[73/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.11076, val loss: 0.38868, in 1.056s\n",
      "[74/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.10852, val loss: 0.38843, in 0.617s\n",
      "[75/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.10632, val loss: 0.38733, in 0.781s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.10418, val loss: 0.38777, in 0.931s\n",
      "[77/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.10210, val loss: 0.38716, in 1.014s\n",
      "[78/1000] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.10014, val loss: 0.38724, in 0.498s\n",
      "[79/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.09809, val loss: 0.38690, in 0.518s\n",
      "[80/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.09616, val loss: 0.38665, in 1.044s\n",
      "[81/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.09415, val loss: 0.38590, in 1.022s\n",
      "[82/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.09228, val loss: 0.38499, in 0.555s\n",
      "[83/1000] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.09049, val loss: 0.38420, in 0.523s\n",
      "[84/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.08885, val loss: 0.38274, in 1.092s\n",
      "[85/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.08703, val loss: 0.38204, in 1.017s\n",
      "[86/1000] 5 trees, 155 leaves (31 on avg), max depth = 17, train loss: 0.08531, val loss: 0.38197, in 0.496s\n",
      "[87/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.08358, val loss: 0.38156, in 0.536s\n",
      "[88/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.08195, val loss: 0.38072, in 1.100s\n",
      "[89/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.08032, val loss: 0.38055, in 0.995s\n",
      "[90/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.07888, val loss: 0.38022, in 0.489s\n",
      "[91/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.07725, val loss: 0.37996, in 0.550s\n",
      "[92/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.07569, val loss: 0.37941, in 1.115s\n",
      "[93/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.07420, val loss: 0.37931, in 0.992s\n",
      "[94/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.07281, val loss: 0.37927, in 0.543s\n",
      "[95/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.07144, val loss: 0.37903, in 0.528s\n",
      "[96/1000] 5 trees, 155 leaves (31 on avg), max depth = 20, train loss: 0.07001, val loss: 0.37875, in 1.052s\n",
      "[97/1000] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.06860, val loss: 0.37853, in 1.040s\n",
      "[98/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.06723, val loss: 0.37856, in 0.517s\n",
      "[99/1000] 5 trees, 155 leaves (31 on avg), max depth = 15, train loss: 0.06603, val loss: 0.37799, in 0.510s\n",
      "[100/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.06482, val loss: 0.37793, in 1.185s\n",
      "[101/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.06354, val loss: 0.37805, in 1.091s\n",
      "[102/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.06232, val loss: 0.37769, in 0.534s\n",
      "[103/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.06103, val loss: 0.37768, in 0.676s\n",
      "[104/1000] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.05991, val loss: 0.37703, in 1.228s\n",
      "[105/1000] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.05881, val loss: 0.37683, in 1.038s\n",
      "[106/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.05773, val loss: 0.37704, in 0.512s\n",
      "[107/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.05662, val loss: 0.37733, in 0.561s\n",
      "[108/1000] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.05561, val loss: 0.37702, in 1.171s\n",
      "[109/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.05452, val loss: 0.37658, in 1.000s\n",
      "[110/1000] 5 trees, 155 leaves (31 on avg), max depth = 13, train loss: 0.05353, val loss: 0.37674, in 0.499s\n",
      "[111/1000] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.05249, val loss: 0.37677, in 0.659s\n",
      "[112/1000] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.05151, val loss: 0.37692, in 1.121s\n",
      "[113/1000] 5 trees, 155 leaves (31 on avg), max depth = 16, train loss: 0.05052, val loss: 0.37671, in 0.986s\n",
      "[114/1000] 5 trees, 155 leaves (31 on avg), max depth = 17, train loss: 0.04956, val loss: 0.37646, in 0.522s\n",
      "[115/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.04864, val loss: 0.37690, in 1.151s\n",
      "[116/1000] 5 trees, 155 leaves (31 on avg), max depth = 11, train loss: 0.04771, val loss: 0.37685, in 1.090s\n",
      "[117/1000] 5 trees, 155 leaves (31 on avg), max depth = 18, train loss: 0.04680, val loss: 0.37696, in 1.074s\n",
      "[118/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.04596, val loss: 0.37717, in 0.489s\n",
      "[119/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.04510, val loss: 0.37744, in 0.497s\n",
      "[120/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.04422, val loss: 0.37782, in 0.993s\n",
      "[121/1000] 5 trees, 155 leaves (31 on avg), max depth = 17, train loss: 0.04343, val loss: 0.37848, in 0.950s\n",
      "[122/1000] 5 trees, 155 leaves (31 on avg), max depth = 12, train loss: 0.04268, val loss: 0.37837, in 0.524s\n",
      "[123/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.04190, val loss: 0.37828, in 0.505s\n",
      "[124/1000] 5 trees, 155 leaves (31 on avg), max depth = 14, train loss: 0.04114, val loss: 0.37807, in 0.763s\n",
      "Fit 620 trees in 103.955 s, (19220 total leaves)\n",
      "Time spent computing histograms: 55.989s\n",
      "Time spent finding best splits:  21.795s\n",
      "Time spent applying splits:      13.066s\n",
      "Time spent predicting:           0.432s\n",
      "predict\n",
      "0.7754673732025359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "gg = HistGradientBoostingClassifier(verbose=1, random_state=1, max_iter=1000)\n",
    "print(\"fit\")\n",
    "gg.fit(X_train_train_rolled, y_train_train)\n",
    "print(\"predict\")\n",
    "print(custom_score(gg.predict(X_train_val_rolled), y_train_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-944467c8329e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_train_rolled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"random_state =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_val_rolled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \"\"\"\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0mtotal_n_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 verbose=self.verbose)\n\u001b[0;32m--> 380\u001b[0;31m             for i in range(n_jobs))\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "for i in range(0, 8):\n",
    "    bc = BaggingClassifier(\n",
    "        verbose=1,\n",
    "        random_state=i,\n",
    "        n_estimators=70,\n",
    "        n_jobs=-2\n",
    "    )\n",
    "    bc.fit(X_train_train_rolled, y_train_train)\n",
    "    print(\"random_state =\", i, \" \", custom_score(bc.predict(X_train_val_rolled), y_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-2)]: Done 444 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-2)]: Done 794 tasks      | elapsed:   34.0s\n",
      "[Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed:   43.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7152021912689559"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc = ExtraTreesClassifier(\n",
    "    verbose=1,\n",
    "    random_state=7,\n",
    "    n_estimators=1000,\n",
    "    n_jobs=-2\n",
    ")\n",
    "etc.fit(X_train_train_rolled, y_train_train)\n",
    "custom_score(etc.predict(X_train_val_rolled), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8425553319919518"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(etc.predict(X_train_val_rolled) == bc.predict(X_train_val_rolled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   19.0s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   18.4s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   19.7s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   18.4s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   18.1s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   17.4s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   17.5s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   17.4s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   18.0s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   18.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7460787543984956, 0.7578073111516713, 0.7498914084261744, 0.7568112500771721, 0.7521785044982809, 0.7532130867032345, 0.7585675942465293, 0.7495620166087473, 0.7447450983784023, 0.7418007638569297]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator_rfs = [RandomForestClassifier(\n",
    "    verbose=1,\n",
    "    random_state=i,\n",
    "    n_estimators=100, # default=100\n",
    "    n_jobs=-2,\n",
    ") for i in range(10)]\n",
    "\n",
    "print('Training')\n",
    "[estimator_rf.fit(X_train_train_rolled, y_train_train) for estimator_rf in estimator_rfs]\n",
    "\n",
    "# train_score_rf = custom_score(estimator_rf.predict(X_train_train_rolled), y_train_train)\n",
    "print(\"Validation\")\n",
    "val_score_rfs = [custom_score(estimator_rf.predict(X_train_val_rolled), y_train_val) for estimator_rf in estimator_rfs]\n",
    "print(val_score_rfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   23.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-8ad84bf4eb59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mens_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_train_rolled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Elementary validation score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                                   idx + 1, len(clfs))\n\u001b[1;32m     80\u001b[0m                 )\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             )\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[0;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \"\"\"\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0mtotal_n_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 verbose=self.verbose)\n\u001b[0;32m--> 380\u001b[0;31m             for i in range(n_jobs))\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "ens_model = VotingClassifier(\n",
    "    [('random_forest', RandomForestClassifier(n_estimators=100, random_state=1, verbose=1, n_jobs=-2)),\n",
    "     ('bag_clf', BaggingClassifier(n_estimators=70, random_state=1, verbose=1, n_jobs=-2)),\n",
    "     ('hgbc', HistGradientBoostingClassifier(random_state=1, verbose=1, max_iter=??)),\n",
    "     ('etc', ExtraTreesClassifier(n_estimators=1000, random_state=1, verbose=1, n_jobs=-2))\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[1, 1, 1, 1])\n",
    "\n",
    "print('Fitting')\n",
    "ens_model.fit(X_train_train_rolled, y_train_train)\n",
    "\n",
    "print('Elementary validation score')\n",
    "for est in ens_model.estimators_:\n",
    "    print(est, \"------> validation_score :\", custom_score(est.predict(X_train_val_rolled), y_train_val))\n",
    "\n",
    "print('Predicting')\n",
    "print(\"soft voting validation score :\", custom_score(ens_model.predict(X_train_val_rolled), y_train_val))\n",
    "\n",
    "ens_model.set_params(voting='hard') # Change voting policy\n",
    "print('hard voting validation score :', custom_score(ens_model.predict(X_train_val_rolled), y_train_val))\n",
    "ens_model.set_params(voting='soft') # Back to original voting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting\n",
      "Predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft voting validation score : 0.760879703732226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard voting validation score : 0.7429610390794625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('random_forest',\n",
       "                              RandomForestClassifier(n_jobs=-2, random_state=1,\n",
       "                                                     verbose=1)),\n",
       "                             ('bag_clf',\n",
       "                              BaggingClassifier(n_estimators=70, n_jobs=-2,\n",
       "                                                random_state=1, verbose=1)),\n",
       "                             ('etc',\n",
       "                              ExtraTreesClassifier(n_estimators=1000, n_jobs=-2,\n",
       "                                                   random_state=1,\n",
       "                                                   verbose=1))],\n",
       "                 voting='soft', weights=[1, 1, 0])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Fitting')\n",
    "# ens_model.fit(X_train_train_rolled, y_train_train)\n",
    "ens_model.set_params(weights=[1, 1, 1])\n",
    "print('Predicting')\n",
    "print(\"soft voting validation score :\", custom_score(ens_model.predict(X_train_val_rolled), y_train_val))\n",
    "\n",
    "ens_model.set_params(voting='hard') # Change voting policy\n",
    "print('hard voting validation score :', custom_score(ens_model.predict(X_train_val_rolled), y_train_val))\n",
    "ens_model.set_params(voting='soft') # Back to original voting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models_archives/ensemble_model_rf-bc-gbc_rs=1.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(ens_model, \"models_archives/ensemble_model_rf-bc-gbc_rs=1.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Random Forest Params | Time Features | LogMod Features | Sleep Features | Shifts | Comments | Training Score | Validation Score |\n",
    "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | \n",
    "| `random_state=1, n_estimators=100` | `quantiles=[0.0001, 0.01, ODD_DECILES, 0.99, 0.9999], interquantiles=[(0.1, 0.9), (0.3, 0.7)]` | `quantiles_inv=[ODD_DECILES], interquantiles_inv=[(0.1, 0.9), (0.3, 0.7)]`| Yes | (-1, 0, 1) | - | - | 0.758 |\n",
    "| `random_state=1, n_estimators=100, max_features='log2'` | `quantiles=[0.0001, 0.01, ODD_DECILES, 0.99, 0.9999], interquantiles=[(0.1, 0.9), (0.3, 0.7)]` | `quantiles_inv=[ODD_DECILES], interquantiles_inv=[(0.1, 0.9), (0.3, 0.7)]`| Yes | (-1, 0, 1) | - | - | 0.735 |\n",
    "| `random_state=1, n_estimators=100` | `quantiles=[0.0001, 0.01, ODD_DECILES, 0.99, 0.9999], interquantiles=[(0.1, 0.9), (0.45, 0.55)]` | `quantiles_inv=[ODD_DECILES], interquantiles_inv=[(0.1, 0.9), (0.45, 0.55)]`| Yes | (-1, 0, 1) | - | - | 0.759 |\n",
    "| `GradientBoostingClassifier` | `quantiles=[0.0001, 0.01, ODD_DECILES, 0.99, 0.9999], interquantiles=[(0.1, 0.9), (0.45, 0.55)]` | `quantiles_inv=[ODD_DECILES], interquantiles_inv=[(0.1, 0.9), (0.3, 0.7)]`| Yes | (-1, 0, 1) | - | - | 0.768 |\n",
    "| `VotingClassifier([RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier], weights=[0.3, 0.3, 0.4]); random_state=1` | `quantiles=[0.0001, 0.01, ODD_DECILES, 0.99, 0.9999], interquantiles=[(0.1, 0.9), (0.3, 0.7)]` | `quantiles_inv=[ODD_DECILES], interquantiles_inv=[(0.1, 0.9), (0.3, 0.7)]`| Yes | (-1, 0, 1) | - | - | hard=0.767; soft=0.771 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Random Forest Params | Time Features Quantiles | Time Features Moments | Sleep Features | Pulse Freq (f_max, A_max) | Shifts | Comments | Training Score | Validation Score |\n",
    "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "| - | 0.1, 0.5, 0.9 | - | No | - | 0 | - | 1| 0.67|\n",
    "| - | 0.1, 0.5, 0.9 | - | Yes | - | 0 | - | 1 | 0.69|\n",
    "| - | 0.1, 0.5, 0.9 | - | Yes | - | -1, 0, 1 | - | 1 | 0.7|\n",
    "| - | 0.01, 0.1, 0.5, 0.9, 0.99 | - | Yes | - | -1, 0, 1 | - | 1| 0.7  |\n",
    "| `min_samples_leaf=10` | 0.01, 0.1, 0.5, 0.9, 0.99 | - | Yes | - | -1, 0, 1 | - | 0.89 | 0.69  |\n",
    "| `min_samples_leaf=10` | 0.01, 0.1, 0.5, 0.9, 0.99 | 1, 2 | Yes | - | -1, 0, 1 |  - | 0.89 | 0.69  |\n",
    "| - | 0.01, 0.1, 0.5, 0.9, 0.99 | 1, 2 | Yes | - | -1, 0, 1 |  - | 1 | 0.697  |\n",
    "| - | 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99 | - | Yes | - | -1, 0, 1 |  - | 1 | 0.708  |\n",
    "| - | 0.01, DECILES, 0.99 | - | Yes | - | -1, 0, 1 |  - | 1 | 0.709  |\n",
    "| `min_samples_leaf=10` | 0.01, DECILES, 0.99 | - | Yes | - | -1, 0, 1 |  - | 0.89 | 0.697 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 |  - | 0.89 | 0.699 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 |  - | 1 | 0.713929 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Yes | -1, 0, 1 |  - | 1 | 0.7 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Yes | -1, 0, 1 | - | 0.89 | 0.697 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Pulse Only | -1, 0, 1 | - | 0.89 | 0.7 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Pulse Only | -1, 0, 1 |  - | 1 | 0.7055 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | - |  1 | 0.7055 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee 0.5 | - | Yes | - | -1, 0, 1 |  - | 1 | 0.708 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee 0.5 | - | Yes | - | -1, 0, 1 |  - | 0.89 | 0.700|\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee MIN, MAX | - | Yes | - | -1, 0, 1 | - | 0.89| 0.7 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee MIN, MAX | - | Yes | - | -1, 0, 1 | - | 1| 0.703|\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `bandlog rescaled`| 1| 0.665 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = 10%, 90% for logmod`| 1| 0.7165 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = 10%, 90% for logmod`| ? | 0.70 < x < 0.71 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES for logmod`| 1 | 0.737|\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES for logmod`| 0.898 | 0.721 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES for logmod; eeg_mean only`| 1 | 0.645 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES and quantiles = 0.1, 0.5, 0.9 for logmod`| 1 | 0.719 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `interquantiles_inv = (0.1, 0.9), (0.3, 0.7) and quantiles_inv = 0.5 for logmod`| 1 | 0.74.. |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES and interquantiles_inv = (0.1, 0.9), (0.3, 0.7) for logmod`| 1 | 0.753 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES and interquantiles_inv = (0.1, 0.9), (0.3, 0.7) for logmod, interquantiles = (0.1, 0.9), (0.3, 0.7) for time features`| 1 | 0.754 (Alex) - 0.742 (Mrml) |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES and interquantiles_inv = (0.1, 0.9), (0.3, 0.7) for logmod, interquantiles = (0.1, 0.9), (0.3, 0.7) for time features;  n_estimators = 300`| 1 | 0.756 (Alex)|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.700000    0.774816\n",
       "0.679310    0.776229\n",
       "0.658621    0.776255\n",
       "0.637931    0.776255\n",
       "0.617241    0.776266\n",
       "0.596552    0.776984\n",
       "0.575862    0.776993\n",
       "0.555172    0.779529\n",
       "0.534483    0.780217\n",
       "0.513793    0.781572\n",
       "0.472414    0.781634\n",
       "0.493103    0.781705\n",
       "0.451724    0.783088\n",
       "0.431034    0.784910\n",
       "0.410345    0.786205\n",
       "0.389655    0.786205\n",
       "0.348276    0.786207\n",
       "0.368966    0.786274\n",
       "0.306897    0.786526\n",
       "0.244828    0.786706\n",
       "0.265517    0.786708\n",
       "0.327586    0.787061\n",
       "0.286207    0.787190\n",
       "0.224138    0.788614\n",
       "0.203448    0.789349\n",
       "0.100000    0.792994\n",
       "0.182759    0.793041\n",
       "0.162069    0.793193\n",
       "0.141379    0.794047\n",
       "0.120690    0.794047\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_noob = np.linspace(0.1, 0.7, num=30)\n",
    "scores = list()\n",
    "for w in w_noob:\n",
    "    scores.append(custom_score(meta_estimator.predict_soft(X_train_val_rolled, weights=[w, w, w, 1]), y_train_val))\n",
    "    \n",
    "pd.Series(index=w_noob, data=scores).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7958563452403618"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_score(meta_estimator.predict_soft(X_train_val_rolled, weights=[0, 0, 0, 1]), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New submission file at submissions/ensemble_21-01-03.csv\n"
     ]
    }
   ],
   "source": [
    "#test_ids = get_subject_ids(h5_test)\n",
    "#X_test_raw = make_input_default(h5_test)\n",
    "#X_test_rolled = concat_windows(X_test_raw, test_ids, h5_test, shifts)\n",
    "#opt_w = 0.120690 \n",
    "y_pred = meta_estimator.predict_soft(X_test_rolled, weights=[opt_w, opt_w, opt_w, 1]).astype(int)\n",
    "#print(custom_score(meta_estimator.predict_soft(X_train_val_rolled, weights=[opt_w, opt_w, opt_w, 1]), y_train_val))\n",
    "#submit_to_kaggle(y_pred, h5_test, fname='ensemble_21-01-03.csv', msg=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 2, 0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_ = MinMaxScaler()\n",
    "\n",
    "X_train_train_rolled_svc = scaler_.fit_transform(X_train_train_rolled) \n",
    "X_train_val_rolled_svc = scaler_.transform(X_train_val_rolled)\n",
    "X_test_rolled_svc = scaler_.transform(X_test_rolled)\n",
    "\n",
    "\n",
    "estimator_svc = SVC(verbose=1, kernel='rbf', C=1, max_iter=1000, random_state=1)\n",
    "estimator_svc.fit(X_train_train_rolled_svc, y_train_train)\n",
    "\n",
    "train_score_svc = custom_score(estimator_svc.predict(X_train_train_rolled_svc), y_train_train)\n",
    "val_score_svc = custom_score(estimator_svc.predict(X_train_val_rolled_svc), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "estimator_rf = RandomForestClassifier(\n",
    "    random_state=1,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "clf.fit(X_train_train_rolled, y_train_train)\n",
    "estimator_rf.fit(clf.transform(X_train_train_rolled), y_train_train)\n",
    "\n",
    "# train_score_rf = custom_score(estimator_rf.predict(X_train_train_rolled), y_train_train)\n",
    "val_score_rf = custom_score(estimator_rf.predict(clf.transform(X_train_val_rolled)), y_train_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.13757384,  2.20432082, -0.70161822,  1.40496954],\n",
       "       [-4.363443  ,  3.82483367, -0.03437306,  3.44951271],\n",
       "       [-3.89253356,  4.15378957, -0.03478012,  1.36605611],\n",
       "       ...,\n",
       "       [-2.07190603,  1.44258113, -1.40363889,  0.08353262],\n",
       "       [-3.11006764,  3.8393016 , -1.80742658, -1.15457525],\n",
       "       [-4.44476345,  5.3927919 , -1.67107598, -0.19829525]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.transform(X_train_val_rolled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "estimator_ab = AdaBoostClassifier(\n",
    "    base_estimator = estimator_rf,\n",
    "    # verbose=1,\n",
    "    random_state=1,\n",
    "    n_estimators=50, # default=100\n",
    "    #learning_rate=0.5\n",
    ")\n",
    "\n",
    "estimator_ab.fit(X_train_train_rolled, y_train_train)\n",
    "\n",
    "train_score_ab = custom_score(estimator_ab.predict(X_train_train_rolled), y_train_train)\n",
    "val_score_ab = custom_score(estimator_ab.predict(X_train_val_rolled), y_train_val)\n",
    "\n",
    "print(train_score_ab)\n",
    "print(val_score_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  6, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.DataFrame(np.random.rand(100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     49.739217\n",
       "1     52.057421\n",
       "2     46.613845\n",
       "3     53.308325\n",
       "4     43.123208\n",
       "        ...    \n",
       "95    50.187987\n",
       "96    53.308523\n",
       "97    48.514607\n",
       "98    54.877092\n",
       "99    49.926575\n",
       "Length: 100, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDES\n",
    "- recherche de l'harmonique dans le spectre (donc ne garder que les quantiles inverses qui sont moindres)\n",
    "- (cod) essayer l'entropie\n",
    "- (cod) essayer les paramtres de Hjorth\n",
    "- essayer EMD (Empirical Mode Decomposition)\n",
    "- (cod) essayer MMD (Minimum-Maximum Distance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[399056.36732905],\n",
       "       [179230.45407763],\n",
       "       [ 75138.16948643]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mmd(h5_train[\"alpha_eeg_2\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theophile/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/_methods.py:192: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/_methods.py:202: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/function_base.py:1280: RuntimeWarning: invalid value encountered in subtract\n",
      "  a = op(a[slice1], a[slice2])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/function_base.py:1280: RuntimeWarning: overflow encountered in subtract\n",
      "  a = op(a[slice1], a[slice2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hjorth_parameters(np.exp(2*h5_train[\"eeg_1\"][:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ">>alpha_eeg_1_logE>>  moment_1                 0\n",
       ">>eeg_4>>             qt_0.01                  0\n",
       "                      qt_0.1                   0\n",
       "                      qt_0.3                   0\n",
       "                      qt_0.5                   0\n",
       "                                           ...  \n",
       "energy>>eeg_1>>       Hjorth_complexity    22694\n",
       "energy>>eeg_6>>       Hjorth_mobility      22697\n",
       "energy>>eeg_5>>       Hjorth_mobility      22698\n",
       "                      Hjorth_complexity    22698\n",
       "energy>>eeg_6>>       Hjorth_complexity    22698\n",
       "Length: 170, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_train.isna().sum(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python37164bitdc6ddf9b5234459bacda0ad4bcef452b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
