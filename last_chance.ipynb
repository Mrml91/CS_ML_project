{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "from kaggle_submit import submit_to_kaggle\n",
    "\n",
    "\n",
    "from objects import *\n",
    "from helpers import *\n",
    "# from utils.globals import *\n",
    "from utils.distribution_statistics import *\n",
    "\n",
    "train_file = \"kaggle_data/X_train.h5/X_train.h5\"\n",
    "test_file = \"kaggle_data/X_test.h5/X_test.h5\"\n",
    "\n",
    "def get_train_test_connections():\n",
    "    h5_train = h5py.File(train_file, mode='a')\n",
    "    h5_test = h5py.File(test_file, mode='a')\n",
    "    return h5_train, h5_test\n",
    "\n",
    "def close_train_test_connections(h5_train, h5_test):\n",
    "    h5_train.close()\n",
    "    h5_test.close()\n",
    "    \n",
    "h5_train, h5_test = get_train_test_connections()\n",
    "\n",
    "y_train = pd.read_csv(\"kaggle_data/y_train.csv\", index_col=0, squeeze=True)\n",
    "y_train_arr = y_train.to_numpy()\n",
    "\n",
    "# MAKE CUSTOM FEATURES\n",
    "#from additional_features.make_features import make_all_features\n",
    "#make_all_features(h5_train, h5_test, n_chunks=10, verbose=True, overwrite=False)\n",
    "from additional_features.features_to_frequential import _create_log_modulus, BAND_EEG\n",
    "\n",
    "for h5_file in (h5_train, h5_test):\n",
    "    _create_log_modulus(h5_file, n_chunks=10, features_to_convert=BAND_EEG, overwrite=False, verbose=True)\n",
    "\n",
    "# Close connections because utils.globals needs them\n",
    "close_train_test_connections(h5_train, h5_test)\n",
    "\n",
    "from utils import *\n",
    "\n",
    "h5_train, h5_test = get_train_test_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "eeg_nums = list(range(1, 8))\n",
    "greek_letters = [\"alpha\", \"beta\", \"delta\", \"theta\"]\n",
    "\n",
    "EEG_FEATURES = [f\"eeg_{i}\" for i in eeg_nums]\n",
    "EEG_BAND_FEATURES = [f\"{greek}_eeg_{i}\" for greek, i in itertools.product(greek_letters, eeg_nums)]\n",
    "OTHER_TIME_FEATURES = [\"speed_norm\", \"accel_norm\", \"pulse\"]\n",
    "\n",
    "EEG_LOGMOD_FEATURES = [f\"{eeg}_ft_logmod\" for eeg in EEG_FEATURES]\n",
    "EEG_BAND_LOGMOD_FEATURES = [f\"{eeg_band}_ft_logmod\" for eeg_band in EEG_BAND_FEATURES]\n",
    "OTHER_LOGMOD_FEATURES = [f\"{time_feat}_ft_logmod\" for time_feat in OTHER_TIME_FEATURES]\n",
    "\n",
    "SLEEP_FEATURES = [feat for feat in h5_train.keys() if \"sleep\" in feat]\n",
    "\n",
    "# OLD NAMES\n",
    "BAND_LOG_ENERGY_FEATURES_OLD = [f\"{greek}_{eeg}_logE\" for greek, eeg in itertools.product(greek_letters, EEG_FEATURES)]\n",
    "LOGMOD_FEATURES_OLD = EEG_LOGMOD_FEATURES + OTHER_LOGMOD_FEATURES\n",
    "TIME_FEATURES_OLD = EEG_FEATURES + OTHER_TIME_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha_eeg_1',\n",
       " 'alpha_eeg_1_ft_logmod',\n",
       " 'alpha_eeg_2',\n",
       " 'alpha_eeg_2_ft_logmod',\n",
       " 'alpha_eeg_3',\n",
       " 'alpha_eeg_3_ft_logmod',\n",
       " 'alpha_eeg_4',\n",
       " 'alpha_eeg_4_ft_logmod',\n",
       " 'alpha_eeg_5',\n",
       " 'alpha_eeg_5_ft_logmod',\n",
       " 'alpha_eeg_6',\n",
       " 'alpha_eeg_6_ft_logmod',\n",
       " 'alpha_eeg_7',\n",
       " 'alpha_eeg_7_ft_logmod',\n",
       " 'alpha_eeg_mean_frontal_logE',\n",
       " 'alpha_eeg_mean_frontal_occipital_logE',\n",
       " 'beta_eeg_1',\n",
       " 'beta_eeg_1_ft_logmod',\n",
       " 'beta_eeg_2',\n",
       " 'beta_eeg_2_ft_logmod',\n",
       " 'beta_eeg_3',\n",
       " 'beta_eeg_3_ft_logmod',\n",
       " 'beta_eeg_4',\n",
       " 'beta_eeg_4_ft_logmod',\n",
       " 'beta_eeg_5',\n",
       " 'beta_eeg_5_ft_logmod',\n",
       " 'beta_eeg_6',\n",
       " 'beta_eeg_6_ft_logmod',\n",
       " 'beta_eeg_7',\n",
       " 'beta_eeg_7_ft_logmod',\n",
       " 'beta_eeg_mean_frontal_logE',\n",
       " 'beta_eeg_mean_frontal_occipital_logE',\n",
       " 'delta_eeg_1',\n",
       " 'delta_eeg_1_ft_logmod',\n",
       " 'delta_eeg_2',\n",
       " 'delta_eeg_2_ft_logmod',\n",
       " 'delta_eeg_3',\n",
       " 'delta_eeg_3_ft_logmod',\n",
       " 'delta_eeg_4',\n",
       " 'delta_eeg_4_ft_logmod',\n",
       " 'delta_eeg_5',\n",
       " 'delta_eeg_5_ft_logmod',\n",
       " 'delta_eeg_6',\n",
       " 'delta_eeg_6_ft_logmod',\n",
       " 'delta_eeg_7',\n",
       " 'delta_eeg_7_ft_logmod',\n",
       " 'delta_eeg_mean_frontal_logE',\n",
       " 'delta_eeg_mean_frontal_occipital_logE',\n",
       " 'eeg_mean_frontal',\n",
       " 'eeg_mean_frontal_ft_logmod',\n",
       " 'eeg_mean_frontal_occipital',\n",
       " 'eeg_mean_frontal_occipital_ft_logmod',\n",
       " 'index',\n",
       " 'index_absolute',\n",
       " 'index_window',\n",
       " 'pulse_max_freq',\n",
       " 'pulse_max_logE',\n",
       " 'speed_x',\n",
       " 'speed_y',\n",
       " 'speed_z',\n",
       " 'theta_eeg_1',\n",
       " 'theta_eeg_1_ft_logmod',\n",
       " 'theta_eeg_2',\n",
       " 'theta_eeg_2_ft_logmod',\n",
       " 'theta_eeg_3',\n",
       " 'theta_eeg_3_ft_logmod',\n",
       " 'theta_eeg_4',\n",
       " 'theta_eeg_4_ft_logmod',\n",
       " 'theta_eeg_5',\n",
       " 'theta_eeg_5_ft_logmod',\n",
       " 'theta_eeg_6',\n",
       " 'theta_eeg_6_ft_logmod',\n",
       " 'theta_eeg_7',\n",
       " 'theta_eeg_7_ft_logmod',\n",
       " 'theta_eeg_mean_frontal_logE',\n",
       " 'theta_eeg_mean_frontal_occipital_logE',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = set(h5_train.keys())\n",
    "all_features - set(BAND_LOG_ENERGY_FEATURES_OLD) - set(LOGMOD_FEATURES_OLD) - set(TIME_FEATURES_OLD) - set(SLEEP_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_default(h5_file):\n",
    "    \n",
    "    dfs = list()\n",
    "    \n",
    "    dfs.append( # df_bandlog\n",
    "        make_input_new(\n",
    "            h5_file,\n",
    "            features=BAND_LOG_ENERGY_FEATURES_OLD,\n",
    "            rescale_by_subject=False,\n",
    "            moments=[1],\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #dfs.append( # df_spectral_entropy = \n",
    "    #    make_input_new(\n",
    "    #        h5_file,\n",
    "    #        features=EEG_LOGMOD_FEATURES,\n",
    "    #        rescale_by_subject=False,\n",
    "    #        entropy=True,\n",
    "    #        pre_op=lambda x: np.exp(2 * x),\n",
    "    #        pre_op_name='energy'\n",
    "    #    )\n",
    "    #)\n",
    "    \n",
    "    #dfs.append( # df_time_hjorth = \n",
    "    #    make_input_new(\n",
    "    #        h5_file,\n",
    "    #        features=EEG_FEATURES,\n",
    "    #        rescale_by_subject=False,\n",
    "    #        hjorth=True,\n",
    "            #pre_op=lambda x: np.exp(2 * x),\n",
    "            #pre_op_name='energy'\n",
    "    #    )\n",
    "    #)\n",
    "        \n",
    "    dfs.append( # df_sleep = \n",
    "        make_input_new(\n",
    "            h5_file,\n",
    "            features=SLEEP_FEATURES,\n",
    "            rescale_by_subject=False,\n",
    "            moments=[1]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ## LOGMOD RENAME COLUMNS\n",
    "    \n",
    "    dfs.append( # df_logmod = \n",
    "        make_input_new(\n",
    "            h5_file,\n",
    "            features=LOGMOD_FEATURES_OLD,\n",
    "            rescale_by_subject=False,\n",
    "            #interquantiles=[(0.2, 0.8)],\n",
    "            quantiles_inv=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "            diff_orders=[0],\n",
    "            interquantiles_inv=[(0.1, 0.9), (0.3, 0.7)],\n",
    "        )\n",
    "    )\n",
    "    \n",
    " \n",
    "    dfs.append( # df_time_diff_0 = \n",
    "        make_input_new(\n",
    "            h5_file,\n",
    "            features=TIME_FEATURES_OLD,\n",
    "            rescale_by_subject=False,\n",
    "            # moments=[1, 2],\n",
    "            quantiles=[1e-4, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 1-1e-4],\n",
    "            interquantiles=[(0.1, 0.9), (0.3, 0.7)],\n",
    "            diff_orders=[0]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #dfs.append( # df_eeg_bands =\n",
    "    #    make_input_new(\n",
    "    #        h5_file,\n",
    "    #        features=EEG_BAND_FEATURES,\n",
    "    #       hjorth=True,\n",
    "    #       diff_orders=[0]\n",
    "    #    )\n",
    "    #)\n",
    "    \n",
    "    # EEG INTEGRATED\n",
    "    #eeg_features = [feat for feat in FEATURES if re.search('^eeg_\\d$', feat)]\n",
    "    # print(eeg_features)\n",
    "    #df_eeg_integrated = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=eeg_features,\n",
    "    #    rescale_by_subject=False,\n",
    "    #    # moments=[1, 2],\n",
    "    #    quantiles=[0.1, 0.5, 0.9],\n",
    "    #    #interquantiles=[(0.1, 0.9), (0.3, 0.7)],\n",
    "    #    diff_orders=[-1]\n",
    "    #)\n",
    "    \n",
    "    \n",
    "    #df_time_diff_1 = make_input_new(\n",
    "     #   h5_file,\n",
    "     #   features=TIME_FEATURES_OLD,\n",
    "     #   rescale_by_subject=False,\n",
    "     #   moments=[1, 2],\n",
    "        #quantiles=[1e-4, 1-1e-4],\n",
    "     #   diff_orders=[1]\n",
    "    #)\n",
    "    \n",
    "    #df_pulse_max_freq = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=[\"pulse_max_freq\"],\n",
    "    #    rescale_by_subject=True,\n",
    "    #    moments=[1],\n",
    "    #)\n",
    "    \n",
    "    #df_pulse_max_logE = make_input_new(\n",
    "    #    h5_file,\n",
    "    #    features=[\"pulse_max_logE\"],\n",
    "    #    rescale_by_subject=False,\n",
    "    #    moments=[1],\n",
    "        #pre_op=np.exp,\n",
    "        #pre_op_name=\"energy\"\n",
    "    #)\n",
    "    \n",
    "    \n",
    "    res_df = pd.concat(dfs, axis=1, keys=[str(i) for i in range(len(dfs))])\n",
    "    \n",
    "    # Filling policy\n",
    "    missing_values = res_df.isna().sum(axis=0)\n",
    "    missing_values = missing_values.loc[missing_values > 0]\n",
    "    if len(missing_values) > 0:\n",
    "        print(\"Missing values :\")\n",
    "        print(missing_values)\n",
    "        print(\"Filling missing values with zero\")\n",
    "        res_df = res_df.fillna(0)\n",
    "        \n",
    "    return res_df\n",
    "\n",
    "def shift_and_fill(df, shift):\n",
    "    shifted_df = df.shift(shift)\n",
    "    if shift > 0:\n",
    "        shifted_df.bfill(inplace=True)\n",
    "    elif shift < 0:\n",
    "        shifted_df.ffill(inplace=True)\n",
    "    return shifted_df\n",
    "\n",
    "\n",
    "def roll_and_concat(df, shifts_range):\n",
    "    return pd.concat(map(lambda shift: shift_and_fill(df, shift), shifts_range), \n",
    "                     axis=1, keys=shifts_range)    \n",
    "    \n",
    "def concat_windows(h5_file, df, shifts):\n",
    "    df = df.groupby(h5_file[\"index\"][:], as_index=False).apply(roll_and_concat, shifts_range=shifts)\n",
    "    return df\n",
    "    \n",
    "def make_input_default_test(h5_file):\n",
    "    return make_input_new(h5_file, [\"eeg_1\", \"eeg_2\"], moments=[1])\n",
    "\n",
    "def make_input_default_rolling(h5_file, shifts):\n",
    "    \"\"\"\n",
    "    !!! not suited for pca because columns have 3 levels\n",
    "    \"\"\"\n",
    "    df = make_input_default(h5_file)\n",
    "    df_with_window = concat_windows(h5_file, df, shifts)\n",
    "    return df_with_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature #10/10\u001b[1K\r"
     ]
    }
   ],
   "source": [
    "X_train_raw = make_input_default(h5_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = get_subject_ids(h5_train)\n",
    "train_train_ids, train_val_ids = train_ids[:28], train_ids[28:]\n",
    "\n",
    "X_train_train = X_train_raw.loc[subjects_ids_to_indexers(h5_train, train_train_ids, as_indices=True), :]\n",
    "y_train_train = y_train_arr[subjects_ids_to_indexers(h5_train, train_train_ids, as_indices=True)]\n",
    "\n",
    "X_train_val = X_train_raw.loc[subjects_ids_to_indexers(h5_train, train_val_ids, as_indices=True), :]\n",
    "y_train_val = y_train_arr[subjects_ids_to_indexers(h5_train, train_val_ids, as_indices=True)]\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_ = MinMaxScaler()\n",
    "pca_ = PCA(0.99)\n",
    "\n",
    "X_train_train = pca_.fit_transform(scaler_.fit_transform(X_train_train))\n",
    "X_train_val = pca_.transform(scaler_.transform(X_train_val))\n",
    "X_test = pca_.transform(scaler_.transform(X_test_raw))\n",
    "\"\"\"\n",
    "def subjects_ids_col(h5_file):\n",
    "    return h5_file[\"index\"][:]\n",
    "\n",
    "def concat_windows(arr, subjects_ids, h5_file, shifts): # subjects_ids must be sorted\n",
    "    sid_col = subjects_ids_col(h5_file)\n",
    "    sid_col = sid_col[np.isin(sid_col, subjects_ids)]\n",
    "    df = pd.DataFrame(arr)\n",
    "    \n",
    "    return df.groupby(sid_col).apply(roll_and_concat, shifts_range=shifts)\n",
    "\n",
    "shifts = [-1, 0, 1]\n",
    "X_train_train_rolled = concat_windows(X_train_train.fillna(0), train_train_ids, h5_train, shifts)\n",
    "X_train_val_rolled = concat_windows(X_train_val.fillna(0), train_val_ids, h5_train, shifts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimator_rf = RandomForestClassifier(\n",
    "    random_state=1,\n",
    "    n_estimators=100, # default=100\n",
    ")\n",
    "\n",
    "#estimator_rf.fit(X_train_train_rolled, y_train_train)\n",
    "\n",
    "# train_score_rf = custom_score(estimator_rf.predict(X_train_train_rolled), y_train_train)\n",
    "#val_score_rf = custom_score(estimator_rf.predict(X_train_val_rolled), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   42.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3639           24.98m\n",
      "         2           1.2785           24.33m\n",
      "         3           1.2105           23.97m\n",
      "         4           1.1536           23.65m\n",
      "         5           1.1048           23.42m\n",
      "         6           1.0639           23.26m\n",
      "         7           1.0277           23.34m\n",
      "         8           0.9947           23.27m\n",
      "         9           0.9621           23.10m\n",
      "        10           0.9344           22.81m\n",
      "        20           0.7561           19.88m\n",
      "        30           0.6604           17.28m\n",
      "        40           0.5988           14.78m\n",
      "        50           0.5537           12.46m\n",
      "        60           0.5211           10.05m\n",
      "        70           0.4924            7.66m\n",
      "        80           0.4688            5.15m\n",
      "        90           0.4501            2.57m\n",
      "       100           0.4332            0.00s\n",
      "Predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft voting validation score : 0.7709691807713865\n",
      "hard voting validation score : 0.7673198539290149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('random_forest',\n",
       "                              RandomForestClassifier(random_state=1,\n",
       "                                                     verbose=1)),\n",
       "                             ('bag_clf',\n",
       "                              BaggingClassifier(random_state=1, verbose=1)),\n",
       "                             ('gbc',\n",
       "                              GradientBoostingClassifier(random_state=1,\n",
       "                                                         verbose=1))],\n",
       "                 voting='soft', weights=[0.3, 0.3, 0.4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "ens_model = VotingClassifier(\n",
    "    [('random_forest', RandomForestClassifier(random_state=1, verbose=1)),\n",
    "     ('bag_clf', BaggingClassifier(random_state=1, verbose=1)),\n",
    "     ('gbc', GradientBoostingClassifier(random_state=1, verbose=1)),\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[0.3, 0.3, 0.4])\n",
    "\n",
    "print('Fitting')\n",
    "ens_model.fit(X_train_train_rolled, y_train_train)\n",
    "\n",
    "print('Predicting')\n",
    "print(\"soft voting validation score :\", custom_score(ens_model.predict(X_train_val_rolled), y_train_val))\n",
    "\n",
    "ens_model.set_params(voting='hard') # Change voting policy\n",
    "print('hard voting validation score :', custom_score(ens_model.predict(X_train_val_rolled), y_train_val))\n",
    "ens_model.set_params(voting='soft') # Back to original voting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models_archives/ensemble_model_rf-bc-gbc_rs=1.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(ens_model, \"models_archives/ensemble_model_rf-bc-gbc_rs=1.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Random Forest Params | Time Features | LogMod Features | Sleep Features | Shifts | Comments | Training Score | Validation Score |\n",
    "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | \n",
    "| `random_state=1, n_estimators=100` | `quantiles=[0.0001, 0.01, ODD_DECILES, 0.99, 0.9999], interquantiles=[(0.1, 0.9), (0.3, 0.7)]` | `quantiles_inv=[ODD_DECILES], interquantiles_inv=[(0.1, 0.9), (0.3, 0.7)]`| Yes | (-1, 0, 1) | - | - | 0.758 |\n",
    "| `random_state=1, n_estimators=100, max_features='log2'` | `quantiles=[0.0001, 0.01, ODD_DECILES, 0.99, 0.9999], interquantiles=[(0.1, 0.9), (0.3, 0.7)]` | `quantiles_inv=[ODD_DECILES], interquantiles_inv=[(0.1, 0.9), (0.3, 0.7)]`| Yes | (-1, 0, 1) | - | - | 0.735 |\n",
    "| `random_state=1, n_estimators=100` | `quantiles=[0.0001, 0.01, ODD_DECILES, 0.99, 0.9999], interquantiles=[(0.1, 0.9), (0.45, 0.55)]` | `quantiles_inv=[ODD_DECILES], interquantiles_inv=[(0.1, 0.9), (0.45, 0.55)]`| Yes | (-1, 0, 1) | - | - | 0.759 |\n",
    "| `GradientBoostingClassifier` | `quantiles=[0.0001, 0.01, ODD_DECILES, 0.99, 0.9999], interquantiles=[(0.1, 0.9), (0.45, 0.55)]` | `quantiles_inv=[ODD_DECILES], interquantiles_inv=[(0.1, 0.9), (0.3, 0.7)]`| Yes | (-1, 0, 1) | - | - | 0.768 |\n",
    "| `VotingClassifier([RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier], weights=[0.3, 0.3, 0.4]); random_state=1` | `quantiles=[0.0001, 0.01, ODD_DECILES, 0.99, 0.9999], interquantiles=[(0.1, 0.9), (0.3, 0.7)]` | `quantiles_inv=[ODD_DECILES], interquantiles_inv=[(0.1, 0.9), (0.3, 0.7)]`| Yes | (-1, 0, 1) | - | - | hard=0.767; soft=0.771 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Random Forest Params | Time Features Quantiles | Time Features Moments | Sleep Features | Pulse Freq (f_max, A_max) | Shifts | Comments | Training Score | Validation Score |\n",
    "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "| - | 0.1, 0.5, 0.9 | - | No | - | 0 | - | 1| 0.67|\n",
    "| - | 0.1, 0.5, 0.9 | - | Yes | - | 0 | - | 1 | 0.69|\n",
    "| - | 0.1, 0.5, 0.9 | - | Yes | - | -1, 0, 1 | - | 1 | 0.7|\n",
    "| - | 0.01, 0.1, 0.5, 0.9, 0.99 | - | Yes | - | -1, 0, 1 | - | 1| 0.7  |\n",
    "| `min_samples_leaf=10` | 0.01, 0.1, 0.5, 0.9, 0.99 | - | Yes | - | -1, 0, 1 | - | 0.89 | 0.69  |\n",
    "| `min_samples_leaf=10` | 0.01, 0.1, 0.5, 0.9, 0.99 | 1, 2 | Yes | - | -1, 0, 1 |  - | 0.89 | 0.69  |\n",
    "| - | 0.01, 0.1, 0.5, 0.9, 0.99 | 1, 2 | Yes | - | -1, 0, 1 |  - | 1 | 0.697  |\n",
    "| - | 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99 | - | Yes | - | -1, 0, 1 |  - | 1 | 0.708  |\n",
    "| - | 0.01, DECILES, 0.99 | - | Yes | - | -1, 0, 1 |  - | 1 | 0.709  |\n",
    "| `min_samples_leaf=10` | 0.01, DECILES, 0.99 | - | Yes | - | -1, 0, 1 |  - | 0.89 | 0.697 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 |  - | 0.89 | 0.699 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 |  - | 1 | 0.713929 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Yes | -1, 0, 1 |  - | 1 | 0.7 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Yes | -1, 0, 1 | - | 0.89 | 0.697 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Pulse Only | -1, 0, 1 | - | 0.89 | 0.7 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | Pulse Only | -1, 0, 1 |  - | 1 | 0.7055 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | - |  1 | 0.7055 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee 0.5 | - | Yes | - | -1, 0, 1 |  - | 1 | 0.708 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee 0.5 | - | Yes | - | -1, 0, 1 |  - | 0.89 | 0.700|\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee MIN, MAX | - | Yes | - | -1, 0, 1 | - | 0.89| 0.7 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX + derivee MIN, MAX | - | Yes | - | -1, 0, 1 | - | 1| 0.703|\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `bandlog rescaled`| 1| 0.665 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = 10%, 90% for logmod`| 1| 0.7165 |\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = 10%, 90% for logmod`| ? | 0.70 < x < 0.71 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES for logmod`| 1 | 0.737|\n",
    "| `min_samples_leaf=10` | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES for logmod`| 0.898 | 0.721 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES for logmod; eeg_mean only`| 1 | 0.645 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES and quantiles = 0.1, 0.5, 0.9 for logmod`| 1 | 0.719 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `interquantiles_inv = (0.1, 0.9), (0.3, 0.7) and quantiles_inv = 0.5 for logmod`| 1 | 0.74.. |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES and interquantiles_inv = (0.1, 0.9), (0.3, 0.7) for logmod`| 1 | 0.753 |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES and interquantiles_inv = (0.1, 0.9), (0.3, 0.7) for logmod, interquantiles = (0.1, 0.9), (0.3, 0.7) for time features`| 1 | 0.754 (Alex) - 0.742 (Mrml) |\n",
    "| - | MIN, 0.01, ODD_DECILES, 0.99, MAX | - | Yes | - | -1, 0, 1 | `quantiles_inv = ODD_DECILES and interquantiles_inv = (0.1, 0.9), (0.3, 0.7) for logmod, interquantiles = (0.1, 0.9), (0.3, 0.7) for time features;  n_estimators = 300`| 1 | 0.756 (Alex)|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New submission file at submissions/rf_best_alternate_2021-01-01.csv\n"
     ]
    }
   ],
   "source": [
    "test_ids = get_subject_ids(h5_test)\n",
    "X_test_raw = make_input_default(h5_test)\n",
    "X_test_rolled = concat_windows(X_test_raw, test_ids, h5_test, shifts)\n",
    "y_pred = estimator_rf.predict(X_test_rolled)\n",
    "#submit_to_kaggle(y_pred, h5_test, fname='rf_best_alternate_2021-01-01.csv', msg=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_ = MinMaxScaler()\n",
    "\n",
    "X_train_train_rolled_svc = scaler_.fit_transform(X_train_train_rolled) \n",
    "X_train_val_rolled_svc = scaler_.transform(X_train_val_rolled)\n",
    "X_test_rolled_svc = scaler_.transform(X_test_rolled)\n",
    "\n",
    "\n",
    "estimator_svc = SVC(verbose=1, kernel='rbf', C=1, max_iter=1000, random_state=1)\n",
    "estimator_svc.fit(X_train_train_rolled_svc, y_train_train)\n",
    "\n",
    "train_score_svc = custom_score(estimator_svc.predict(X_train_train_rolled_svc), y_train_train)\n",
    "val_score_svc = custom_score(estimator_svc.predict(X_train_val_rolled_svc), y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "estimator_rf = RandomForestClassifier(\n",
    "    random_state=1,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "clf.fit(X_train_train_rolled, y_train_train)\n",
    "estimator_rf.fit(clf.transform(X_train_train_rolled), y_train_train)\n",
    "\n",
    "# train_score_rf = custom_score(estimator_rf.predict(X_train_train_rolled), y_train_train)\n",
    "val_score_rf = custom_score(estimator_rf.predict(clf.transform(X_train_val_rolled)), y_train_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.13757384,  2.20432082, -0.70161822,  1.40496954],\n",
       "       [-4.363443  ,  3.82483367, -0.03437306,  3.44951271],\n",
       "       [-3.89253356,  4.15378957, -0.03478012,  1.36605611],\n",
       "       ...,\n",
       "       [-2.07190603,  1.44258113, -1.40363889,  0.08353262],\n",
       "       [-3.11006764,  3.8393016 , -1.80742658, -1.15457525],\n",
       "       [-4.44476345,  5.3927919 , -1.67107598, -0.19829525]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.transform(X_train_val_rolled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "estimator_ab = AdaBoostClassifier(\n",
    "    base_estimator = estimator_rf,\n",
    "    # verbose=1,\n",
    "    random_state=1,\n",
    "    n_estimators=50, # default=100\n",
    "    #learning_rate=0.5\n",
    ")\n",
    "\n",
    "estimator_ab.fit(X_train_train_rolled, y_train_train)\n",
    "\n",
    "train_score_ab = custom_score(estimator_ab.predict(X_train_train_rolled), y_train_train)\n",
    "val_score_ab = custom_score(estimator_ab.predict(X_train_val_rolled), y_train_val)\n",
    "\n",
    "print(train_score_ab)\n",
    "print(val_score_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  6, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.DataFrame(np.random.rand(100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     49.739217\n",
       "1     52.057421\n",
       "2     46.613845\n",
       "3     53.308325\n",
       "4     43.123208\n",
       "        ...    \n",
       "95    50.187987\n",
       "96    53.308523\n",
       "97    48.514607\n",
       "98    54.877092\n",
       "99    49.926575\n",
       "Length: 100, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDÉES\n",
    "- recherche de l'harmonique dans le spectre (donc ne garder que les quantiles inverses qui sont moindres)\n",
    "- (codé) essayer l'entropie\n",
    "- (codé) essayer les paramètres de Hjorth\n",
    "- essayer EMD (Empirical Mode Decomposition)\n",
    "- (codé) essayer MMD (Minimum-Maximum Distance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[399056.36732905],\n",
       "       [179230.45407763],\n",
       "       [ 75138.16948643]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mmd(h5_train[\"alpha_eeg_2\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theophile/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/_methods.py:192: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/_methods.py:202: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/function_base.py:1280: RuntimeWarning: invalid value encountered in subtract\n",
      "  a = op(a[slice1], a[slice2])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/function_base.py:1280: RuntimeWarning: overflow encountered in subtract\n",
      "  a = op(a[slice1], a[slice2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hjorth_parameters(np.exp(2*h5_train[\"eeg_1\"][:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ">>alpha_eeg_1_logE>>  moment_1                 0\n",
       ">>eeg_4>>             qt_0.01                  0\n",
       "                      qt_0.1                   0\n",
       "                      qt_0.3                   0\n",
       "                      qt_0.5                   0\n",
       "                                           ...  \n",
       "energy>>eeg_1>>       Hjorth_complexity    22694\n",
       "energy>>eeg_6>>       Hjorth_mobility      22697\n",
       "energy>>eeg_5>>       Hjorth_mobility      22698\n",
       "                      Hjorth_complexity    22698\n",
       "energy>>eeg_6>>       Hjorth_complexity    22698\n",
       "Length: 170, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_train.isna().sum(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python37164bitdc6ddf9b5234459bacda0ad4bcef452b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
