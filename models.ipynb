{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "from kaggle_submit import submit_to_kaggle\n",
    "from helpers import *\n",
    "from utils.globals import *\n",
    "from utils.distribution_statistics import *\n",
    "from objects import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE CUSTOM FEATURES\n",
    "from additional_features.make_features import make_all_features\n",
    "make_all_features(n_chunks=10, verbose=True, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"kaggle_data/X_train.h5/X_train.h5\"\n",
    "test_file = \"kaggle_data/X_test.h5/X_test.h5\"\n",
    "\n",
    "h5_train = h5py.File(train_file, mode='a')\n",
    "h5_test = h5py.File(test_file, mode='a')\n",
    "\n",
    "y_train = pd.read_csv(\"kaggle_data/y_train.csv\", index_col=0, squeeze=True)\n",
    "y_train_arr = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_integrity(h5_file):\n",
    "    keys = list(h5_file.keys())\n",
    "    for i, key in enumerate(keys):\n",
    "        print_bis(f'{i+1}/{len(keys)}')\n",
    "        x = h5_file[key][:]\n",
    "        assert np.sum(np.isnan(x)) == 0\n",
    "        assert np.sum(np.isinf(x)) == 0\n",
    "        \n",
    "# ensure_integrity(h5_train)\n",
    "# ensure_integrity(h5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# SAVE MODEL\n",
    "\n",
    "from joblib import dump, load\n",
    "ARCHIVES_FOLDER = \"models_archives\"\n",
    "if not os.path.exists(ARCHIVES_FOLDER):\n",
    "    os.makedirs(ARCHIVES_FOLDER)\n",
    "\n",
    "def save_model(model, name):\n",
    "    fpath = os.path.join(ARCHIVES_FOLDER, f\"{name}.joblib\")\n",
    "    dump(model, fpath)\n",
    "    print(f\"New model saved at {fpath}\")\n",
    "    return fpath\n",
    "    \n",
    "    \n",
    "def load_model(name):\n",
    "    if not name.startswith(ARCHIVES_FOLDER):\n",
    "        name = os.path.join(ARCHIVES_FOLDER, name)\n",
    "    if not name.endswith(\".joblib\"):\n",
    "        name = f\"{name}.joblib\"\n",
    "    model = load(name)\n",
    "    return model\n",
    "\n",
    "LEADERBOARD_FILE = \"leaderboard.txt\"\n",
    "if not os.path.exists(LEADERBOARD_FILE):\n",
    "    with open(LEADERBOARD_FILE, 'a') as leaderboard:\n",
    "        leaderboard.write(\";;;\".join(['path', 'training_score', 'validation_score', 'comments']))\n",
    "    \n",
    "def write_model_to_leaderboard(model, model_name, train_score, val_score, comments=\"\"):\n",
    "    fpath = save_model(model, model_name)\n",
    "    with open(LEADERBOARD_FILE, \"a\") as leaderboard:\n",
    "        leaderboard.write(\"\\n\" + ';;;'.join([fpath, str(train_score), str(val_score), comments]))\n",
    "    \n",
    "\"\"\"\n",
    "_ = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input maker\n",
    "\n",
    "def make_input_for_svm_extreme(h5_file):\n",
    "    return make_input(h5_file, features=FEATURES, quantiles=TAIL_QUANTILES, dist_char=False, truncate_dist=False)\n",
    "\n",
    "svm_extreme_input_maker = InputMaker(make_input_for_svm_extreme)\n",
    "\n",
    "## input shaper\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca_cols_filters = [\n",
    "    lambda x: 'eeg' in x[0] and 'logmod' in x[0], \n",
    "    lambda x: bool(re.search('eeg_\\d', x[0])),\n",
    "]\n",
    "\n",
    "pca_list = [CustomPCA(columns_filter=filt, name=f\"PCA_{i}\", var_capture=0.95) for i, filt in enumerate(pca_cols_filters)]\n",
    "\n",
    "svm_extreme_input_shaper = InputShaper(*pca_list, StandardScaler())\n",
    "\n",
    "## parameters grid\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "svm_grid_1 = ParameterGrid({\"kernel\": [\"rbf\", \"sigmoid\"],\n",
    "                            \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "                            \"gamma\": [\"auto\", \"scale\"]}\n",
    "                          )\n",
    "svm_grid_2 = ParameterGrid({\"kernel\": [\"poly\"],\n",
    "                            \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "                            \"degree\": [1, 2, 3, 4]}\n",
    "                          )\n",
    "svm_hyperparameters = list(svm_grid_1) + list(svm_grid_2)\n",
    "\n",
    "\n",
    "# generic object\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_extreme = PoolModels(\n",
    "    input_maker=svm_extreme_input_maker,\n",
    "    n_splits=5,\n",
    "    train_size=27,\n",
    "    input_shaper=svm_extreme_input_shaper,\n",
    "    blueprint=SVC,\n",
    "    parameters_list=svm_hyperparameters,\n",
    "    iterations_to_warm=10,\n",
    "    seed=3,\n",
    "    h5_train=h5_train,\n",
    "    h5_test=h5_test,\n",
    "    y_train_arr=y_train_arr\n",
    ")\n",
    "\n",
    "# train\n",
    "#import matplotlib.pyplot as plt\n",
    "#svm_extreme.warm_up()\n",
    "#svm_extreme.plot_validation()\n",
    "#plt.show()\n",
    "#best_svm_extremes = svm_extreme.train_n_best_models_until_convergence(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature #21/53\u001b[1K\r"
     ]
    }
   ],
   "source": [
    "# glouton\n",
    "\n",
    "# input_maker\n",
    "\n",
    "def make_input_glouton(h5_file):\n",
    "    return make_input(h5_file, features=FEATURES, quantiles=QUANTILES, dist_char=True, truncate_dist=True)\n",
    "\n",
    "glouton_im = InputMaker(make_input_glouton)\n",
    "\n",
    "# input_shaper \n",
    "def make_filter(pat):\n",
    "    def f(col_name):\n",
    "        return bool(re.search(pat, col_name))\n",
    "    return f\n",
    "\n",
    "groups_pats = [\"alpha\", \"beta\", \"delta\", \"theta\", \"eeg_\\d.logmod\"]\n",
    "custom_pcas = [CustomPCA(make_filter(gp), name=gp, var_capture=0.95) for gp in groups_pats]\n",
    "glouton_is = InputShaper(*custom_pcas, StandardScaler())\n",
    "\n",
    "glouton_pool = PoolModels(\n",
    "    input_maker=glouton_im,\n",
    "    n_splits=5,\n",
    "    train_size=27,\n",
    "    input_shaper=glouton_is,\n",
    "    blueprint=SVC,\n",
    "    parameters_list=svm_hyperparameters,\n",
    "    iterations_to_warm=1000,\n",
    "    seed=3,\n",
    "    h5_train=h5_train,\n",
    "    h5_test=h5_test,\n",
    "    y_train_arr=y_train_arr\n",
    ")\n",
    "\n",
    "glouton_pool.warm_up()\n",
    "glouton_pool.plot_validation()\n",
    "plt.show()\n",
    "best_gloutons = glouton_pool.train_n_best_models_until_convergence(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python37164bitdc6ddf9b5234459bacda0ad4bcef452b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
