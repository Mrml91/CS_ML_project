{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "\n",
    "PICTURES_FOLDER = \"pictures\"\n",
    "os.makedirs(PICTURES_FOLDER, exist_ok=True)\n",
    "\n",
    "SLEEP_STAGES_COLORS = {\n",
    "    0: \"blue\",\n",
    "    1: \"green\",\n",
    "    2: \"red\",\n",
    "    3: \"black\",\n",
    "    4: \"orange\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"kaggle_data/X_train.h5/X_train.h5\"\n",
    "test_file = \"kaggle_data/X_test.h5/X_test.h5\"\n",
    "\n",
    "h5_train = h5py.File(train_file, mode='a')\n",
    "h5_test = h5py.File(test_file, mode='a')\n",
    "\n",
    "y_train = pd.read_csv(\"kaggle_data/y_train.csv\", index_col=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES = ['accel_norm', 'eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse', 'speed_norm']\n",
      "FREQUENCIES = {'accel_norm': 10, 'eeg_1': 50, 'eeg_2': 50, 'eeg_3': 50, 'eeg_4': 50, 'eeg_5': 50, 'eeg_6': 50, 'eeg_7': 50, 'pulse': 10, 'speed_norm': 10}\n"
     ]
    }
   ],
   "source": [
    "IRRELEVANT_FEATURES = ['index', 'index_absolute', 'index_window',\n",
    "                       'x', 'y', 'z',\n",
    "                       'speed_x', 'speed_y', 'speed_z']\n",
    "\n",
    "def update_globals():\n",
    "    features = [feat for feat in h5_train.keys() if feat not in IRRELEVANT_FEATURES]\n",
    "    frequencies = {feat: h5_train[feat][0].size // 30 for feat in features}\n",
    "    frequencies = {feat: freq if int(freq) in (10, 50) else 0 \n",
    "                   for feat, freq in frequencies.items()}\n",
    "    return features, frequencies\n",
    "    \n",
    "FEATURES, FREQUENCIES = update_globals()\n",
    "print(\"FEATURES =\", FEATURES)\n",
    "print(\"FREQUENCIES =\", FREQUENCIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accel_norm', 'eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse', 'speed_norm']\n"
     ]
    }
   ],
   "source": [
    "# HELPERS\n",
    "\n",
    "def print_bis(txt):\n",
    "    print(txt, end='\\x1b[1K\\r')\n",
    "    \n",
    "def print_ter(txt):\n",
    "    print(f\"\\n{txt}\")\n",
    "\n",
    "    \n",
    "def make_timeline(freq):\n",
    "    \"\"\"\n",
    "    ARGS:\n",
    "        freq (int): frequency in Hertz\n",
    "    \n",
    "    RETURNS:\n",
    "        (pd.timedelta_range) : timestamps for a signal sampled at <freq> Hz for 30 seconds\n",
    "    \"\"\"\n",
    "    return pd.timedelta_range(start='0s', end='30s', periods=freq*30)\n",
    "\n",
    "\n",
    "def make_full_timeline(windows, freq):\n",
    "    # test there is no missing data\n",
    "    deltas = np.unique(np.diff(windows))\n",
    "    assert (len(deltas) == 1) and (int(deltas[0]) == 1)\n",
    "    return pd.timedelta_range(start='0s',\n",
    "                              end=pd.to_timedelta('30s') * (windows[-1] + 1),\n",
    "                              periods=freq * 30 * (windows[-1] + 1))\n",
    "\n",
    "def get_subject_ids(h5_file):\n",
    "    return np.unique(h5_file[\"index\"][:])\n",
    "\n",
    "    \n",
    "def get_subject_boundaries(h5_file, subject_id, ready_to_use=True):\n",
    "    \"\"\"\n",
    "    Helper function to select data relating to a given subject (on numpy arrays)\n",
    "    \n",
    "    ARGS:\n",
    "        h5_file (h5py.File)\n",
    "        subject_id (int)\n",
    "        ready_to_use (bool, default=True): return a slice or a tuple\n",
    "        \n",
    "    RETURNS:\n",
    "        subject_boundaries : (slice) (index_start, index_end+1) if <ready_to_use>\n",
    "                             (tuple) (index_start, index_end) if not <ready_to_use>\n",
    "                        \n",
    "    \"\"\"\n",
    "    sids = h5_file['index'][:]\n",
    "    start = np.argmax(sids == subject_id)\n",
    "    end = len(sids) - 1 - np.argmax(sids[::-1] == subject_id)\n",
    "    \n",
    "    indexers = h5_file['index_absolute'][:]\n",
    "    start = indexers[start]\n",
    "    end = indexers[end]\n",
    "    if ready_to_use:\n",
    "        return slice(start, end + 1) # for numpy arrays\n",
    "    return (start, end)\n",
    "\n",
    "\n",
    "def get_subject_feature_signals(h5_file, subject_id, feature, as_timeseries=False):\n",
    "    \"\"\"\n",
    "    Get the full timeseries for a given (subject_id, feature) pair.\n",
    "    \n",
    "    ARGS:\n",
    "        h5_file (h5py.File)\n",
    "        subject_id (int)\n",
    "        feature (str)\n",
    "        \n",
    "    RETURNS:\n",
    "        timeseries : (pd.Series if <as_timeseries>) represents the <feature> timeseries of the subject \n",
    "                     (list[np.array[?]] if not <as_timeseries>) list of <feature> signals from the subject\n",
    "    \"\"\"\n",
    "    # Fetch subject boundaries\n",
    "    boundaries = get_subject_boundaries(h5_file, subject_id)\n",
    "    # Retrieve samples\n",
    "    feature_timeseries = h5_file[feature][boundaries]\n",
    "    if not as_timeseries:\n",
    "        return feature_timeseries\n",
    "    feature_timeseries = np.concatenate(feature_timeseries, axis=0)\n",
    "    # Build timeline\n",
    "    feature_frequency = FREQUENCIES[feature]\n",
    "    windows = h5_file['index_window'][boundaries]\n",
    "    timeline = make_full_timeline(windows, feature_frequency)\n",
    "    return pd.Series(data=feature_timeseries, index=timeline)\n",
    "\n",
    "\n",
    "def get_subject_sleep_stage(subject_id):\n",
    "    start, end = get_subject_boundaries(h5_train, subject_id, ready_to_use=False)\n",
    "    return y_train.loc[start:end] # because loc includes <end> (different behaviour than numpy arrays)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.0287616e+02, -2.4733213e+04, -2.8913711e+04, ...,\n",
       "         1.2473976e+01,  5.6019249e+00,  3.6001048e+00],\n",
       "       [ 2.1024307e+01,  2.4208887e+01,  1.0953083e+01, ...,\n",
       "        -3.6253862e+00,  2.5444579e+00,  1.5417756e+01],\n",
       "       [ 1.3676975e+01,  1.2750147e+01, -4.2638946e+00, ...,\n",
       "        -6.4895964e+00, -2.8554073e-01, -4.8893394e+01],\n",
       "       ...,\n",
       "       [-1.8695693e-01,  6.5529265e+00,  4.1397521e-01, ...,\n",
       "        -5.0123764e+01, -3.3739128e+01, -5.1105301e+01],\n",
       "       [-3.8710526e+01,  9.1726965e-01,  4.4646332e+01, ...,\n",
       "        -1.9432636e+01, -1.1449772e+01, -3.6908641e+00],\n",
       "       [ 2.2870004e+00, -6.5207219e+00,  4.5359030e+00, ...,\n",
       "         9.2413536e+01, -3.1127655e+01, -1.7876479e+02]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "get_subject_feature_signals(h5_train, 1, \"eeg_1\", as_timeseries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0 days 00:00:00               -902.876160\n",
       "0 days 00:00:00.020000015   -24733.212891\n",
       "0 days 00:00:00.040000031   -28913.710938\n",
       "0 days 00:00:00.060000047   -25399.294922\n",
       "0 days 00:00:00.080000062   -23260.080078\n",
       "                                 ...     \n",
       "0 days 07:04:59.919999937      208.143906\n",
       "0 days 07:04:59.939999952      142.460953\n",
       "0 days 07:04:59.959999968       92.413536\n",
       "0 days 07:04:59.979999984      -31.127655\n",
       "0 days 07:05:00               -178.764786\n",
       "Length: 1275000, dtype: float32"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "get_subject_feature_signals(h5_train, 1, \"eeg_1\", as_timeseries=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accel_norm', 'eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse', 'speed_norm']\n"
     ]
    }
   ],
   "source": [
    "def _create_speed_and_acceleration(h5_file, overwrite=False, verbose=True):\n",
    "    \"\"\"\n",
    "    a[t] = (v[t] - v[t-1]) / dt \n",
    "    ===> v[t] = sum_{s=0}^{t} a[s] (+ v[-1] = 0)\n",
    "    \"\"\"\n",
    "    freq = 10\n",
    "    dt = 1 / freq\n",
    "    \n",
    "    # Create datasets if required\n",
    "    if \"accel_norm\" in h5_file.keys() and not overwrite:\n",
    "        return None\n",
    "    shape, dtype = h5_file[\"x\"].shape, h5_file[\"x\"].dtype\n",
    "    for name in [\"accel_norm\", \"speed_x\", \"speed_y\", \"speed_z\", \"speed_norm\"]:\n",
    "        try:\n",
    "            h5_file.create_dataset(name, shape=shape, dtype=dtype)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Initiate subject id\n",
    "    sid = -1\n",
    "    for ix in range(shape[0]):\n",
    "        if sid != h5_file[\"index\"][ix]:\n",
    "            sid = h5_file[\"index\"][ix]\n",
    "            speed = np.array([[0, 0, 0]])\n",
    "            if verbose:\n",
    "                print_bis(f\"SUBJECT #{sid}\")\n",
    "        # acceleration\n",
    "        accel = np.stack([h5_file[feat][ix] for feat in (\"x\", \"y\", \"z\")], axis=-1)\n",
    "        h5_file[\"accel_norm\"][ix] = np.linalg.norm(accel, ord=2, axis=1)\n",
    "        # speed\n",
    "        speed = speed + np.cumsum(accel, axis=0) * dt\n",
    "        h5_file[\"speed_x\"][ix] = speed[:, 0]\n",
    "        h5_file[\"speed_y\"][ix] = speed[:, 1]\n",
    "        h5_file[\"speed_z\"][ix] = speed[:, 2]\n",
    "        h5_file[\"speed_norm\"][ix] = np.linalg.norm(speed, ord=2, axis=1)\n",
    "        # speed for next iteration\n",
    "        speed = speed[[-1], :]\n",
    "    return None\n",
    "        \n",
    "\n",
    "# Create speed and acceleration\n",
    "_create_speed_and_acceleration(h5_train, overwrite=False, verbose=True)\n",
    "_create_speed_and_acceleration(h5_test, overwrite=False, verbose=True)\n",
    "FEATURES, FREQUENCIES = update_globals()\n",
    "print(FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name     | Frequency (Hz)|\n",
    "| ---------| -----------|\n",
    "| $\\delta$ | 0-4 |\n",
    "| $\\theta$ | 4-8 |\n",
    "| $\\alpha$ | 8-13 |\n",
    "| $\\beta$  | 13-22 |\n",
    "| $\\gamma$ | 30-. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES = ['accel_norm', 'alpha_eeg_1_logE', 'alpha_eeg_2_logE', 'alpha_eeg_3_logE', 'alpha_eeg_4_logE', 'alpha_eeg_5_logE', 'alpha_eeg_6_logE', 'alpha_eeg_7_logE', 'beta_eeg_1_logE', 'beta_eeg_2_logE', 'beta_eeg_3_logE', 'beta_eeg_4_logE', 'beta_eeg_5_logE', 'beta_eeg_6_logE', 'beta_eeg_7_logE', 'delta_eeg_1_logE', 'delta_eeg_2_logE', 'delta_eeg_3_logE', 'delta_eeg_4_logE', 'delta_eeg_5_logE', 'delta_eeg_6_logE', 'delta_eeg_7_logE', 'eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse', 'speed_norm', 'theta_eeg_1_logE', 'theta_eeg_2_logE', 'theta_eeg_3_logE', 'theta_eeg_4_logE', 'theta_eeg_5_logE', 'theta_eeg_6_logE', 'theta_eeg_7_logE']\n",
      "FREQUENCIES = {'accel_norm': 10, 'alpha_eeg_1_logE': 0, 'alpha_eeg_2_logE': 0, 'alpha_eeg_3_logE': 0, 'alpha_eeg_4_logE': 0, 'alpha_eeg_5_logE': 0, 'alpha_eeg_6_logE': 0, 'alpha_eeg_7_logE': 0, 'beta_eeg_1_logE': 0, 'beta_eeg_2_logE': 0, 'beta_eeg_3_logE': 0, 'beta_eeg_4_logE': 0, 'beta_eeg_5_logE': 0, 'beta_eeg_6_logE': 0, 'beta_eeg_7_logE': 0, 'delta_eeg_1_logE': 0, 'delta_eeg_2_logE': 0, 'delta_eeg_3_logE': 0, 'delta_eeg_4_logE': 0, 'delta_eeg_5_logE': 0, 'delta_eeg_6_logE': 0, 'delta_eeg_7_logE': 0, 'eeg_1': 50, 'eeg_2': 50, 'eeg_3': 50, 'eeg_4': 50, 'eeg_5': 50, 'eeg_6': 50, 'eeg_7': 50, 'pulse': 10, 'speed_norm': 10, 'theta_eeg_1_logE': 0, 'theta_eeg_2_logE': 0, 'theta_eeg_3_logE': 0, 'theta_eeg_4_logE': 0, 'theta_eeg_5_logE': 0, 'theta_eeg_6_logE': 0, 'theta_eeg_7_logE': 0}\n"
     ]
    }
   ],
   "source": [
    "# SIGNAL PROCESSING\n",
    "\n",
    "# max frequency in our fourier transform: 25 Hz so no gamma\n",
    "BANDS_FRONTIERS = [-1, 4, 8, 13, 22]\n",
    "BANDS_LABELS = ['delta', 'theta', 'alpha', 'beta']\n",
    "\n",
    "from scipy.fft import fft\n",
    "\n",
    "\"\"\"\n",
    "def get_spectrum(seq, fs):\n",
    "    ft_modulus = np.abs(fft(seq))\n",
    "    # The signal is real so the spectrum is symmetric \n",
    "    if len(seq) % 2 == 0:\n",
    "        ft_modulus = ft_modulus[:len(seq) // 2]\n",
    "    else:\n",
    "        ft_modulus = ft_modulus[:len(seq) // 2 + 1]\n",
    "    freqs = np.arange(0, len(ft_modulus)) * fs / len(seq) # frequencies of the spectrum\n",
    "    return pd.Series(data=ft_modulus, index=freqs)\n",
    "\n",
    "\n",
    "def get_energy_by_band(seq, fs):\n",
    "    spectrum = get_spectrum(seq, fs)\n",
    "    bands = pd.cut(spectrum.index,\n",
    "                   bins=BANDS_FRONTIERS,\n",
    "                   labels=BANDS_LABELS\n",
    "                  )\n",
    "    energy = spectrum.pow(2).groupby(bands).sum() # energy proportional to this\n",
    "    # energy.clip(1e-10, None)\n",
    "    return energy\n",
    "\"\"\"\n",
    "\n",
    "def get_spectrum_energy_chunk(sequences, sampling_freq):\n",
    "    fourier_transform = fft(sequences, axis=1)\n",
    "    energy = np.power(fourier_transform.real, 2) + np.power(fourier_transform.imag, 2) # proportional to energy\n",
    "    energy = energy[:, :int(np.ceil(sequences.shape[1] / 2))] # Shannon\n",
    "    frequencies = np.arange(0, energy.shape[1]) * sampling_freq / sequences.shape[1]\n",
    "    bands = pd.cut(frequencies, bins=BANDS_FRONTIERS, labels=BANDS_LABELS)\n",
    "    energy_by_band = pd.DataFrame(data=energy, columns=bands)\n",
    "    energy_by_band = energy_by_band.groupby(energy_by_band.columns, axis=1).sum()\n",
    "    return energy_by_band\n",
    "    \n",
    "    \n",
    "def chunks_iterator(N, size): # with np.array convention \n",
    "    chunk_size = int(np.ceil(size / N))\n",
    "    if chunk_size == 0:\n",
    "        yield 0, size\n",
    "    else:\n",
    "        i = 0\n",
    "        while i <= size:\n",
    "            yield i, i + chunk_size\n",
    "            i += chunk_size\n",
    "    \n",
    "    \n",
    "\n",
    "def _create_log_energy(h5_file, n_chunks=10, overwrite=False, verbose=True):\n",
    "    if (not overwrite) and ('alpha_eeg_1_logE' in h5_file.keys()):\n",
    "        return None\n",
    "    \n",
    "    eegs = list(filter(lambda x: x.startswith('eeg'), FEATURES))\n",
    "    shape = (h5_file[\"eeg_1\"].shape[0], 1)\n",
    "    dtype = h5_file[\"eeg_1\"].dtype\n",
    "    \n",
    "    for band_name in BANDS_LABELS:\n",
    "        for eeg in eegs:\n",
    "            try:\n",
    "                h5_file.create_dataset(f\"{band_name}_{eeg}_logE\", shape=shape, dtype=dtype)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    for chunk_num, (chunk_start, chunk_end) in enumerate(chunks_iterator(n_chunks, shape[0])):\n",
    "        if verbose:\n",
    "            print_bis(f\"{chunk_num+1}/{n_chunks}\")\n",
    "        for eeg in eegs:\n",
    "            energy = get_spectrum_energy_chunk(h5_file[eeg][chunk_start:chunk_end], FREQUENCIES[eeg])\n",
    "            log_energy = np.log(energy)\n",
    "            for band_name in BANDS_LABELS:\n",
    "                h5_file[f\"{band_name}_{eeg}_logE\"][chunk_start:chunk_end] = log_energy[[band_name]].values\n",
    "    return None\n",
    "    \n",
    "        \n",
    "\n",
    "#def get_spectrum_maxima(seq, fs, thresh=0.1):\n",
    "#    spectrum = get_spectrum(seq, fs)\n",
    "#    delta_left = np.diff(spectrum, prepend=spectrum[0] - 1) > 0 # ascending\n",
    "#    delta_right = np.diff(spectrum[::-1], prepend=spectrum[-1] - 1)[::-1] > 0 # descending\n",
    "#    ix_keep = np.logical_and(delta_left, delta_right) # local maximum\n",
    "#    spectrum_util = spectrum.loc[ix_keep]\n",
    "#    spectrum_util = spectrum_util.loc[spectrum_util > spectrum_util.max() * thresh]\n",
    "#    return spectrum_util\n",
    "\n",
    "_create_log_energy(h5_train, n_chunks=100, overwrite=False, verbose=True)\n",
    "_create_log_energy(h5_test, n_chunks=100, overwrite=False, verbose=True)\n",
    "FEATURES, FREQUENCIES = update_globals()\n",
    "print(\"FEATURES =\", FEATURES)\n",
    "print(\"FREQUENCIES =\", FREQUENCIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def plot_subject_quantiles(subject_id, q_inf=0.025, q_sup=0.975, n_quantiles=20):\n",
    "    sleep_states = get_subject_sleep_state(subject_id)\n",
    "    qts = np.linspace(q_inf, q_sup, n_quantiles).round(3)\n",
    "    for feature in FEATURES:\n",
    "        signal = get_subject_feature_signals(h5_train, subject_id, feature, as_timeseries=False)\n",
    "        size = signal[0].size\n",
    "        signal_by_state = pd.Series(data=np.concatenate(signal, axis=0),\n",
    "                                    index=np.repeat(sleep_states.values, size))\n",
    "        qt_df = signal_by_state.groupby(signal_by_state.index).quantile(qts)\n",
    "        qt_df.unstack(0).plot()\n",
    "        # sns.heatmap(qt_df.unstack(0))\n",
    "        plt.title(feature)\n",
    "        plt.show()\n",
    "    return None\n",
    "\"\"\"\n",
    "_=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE #37 SUBJECT 31/31 (RESCALE = True)\u001b[1KK\r"
     ]
    }
   ],
   "source": [
    "def robust_rescale(df):\n",
    "    \"\"\"\n",
    "    X_rescaled = (X - MED(X)) / MED(|X - MED(X)|)\n",
    "    \"\"\"\n",
    "    med = df.median()\n",
    "    med_spread = (df - df.median()).abs().median()\n",
    "    # df_rescaled = (df - med) / med_spread\n",
    "    return (df - med) / med_spread\n",
    "\n",
    "def min_max_rescale(df):\n",
    "    min_ = df.min()\n",
    "    max_ = df.max()\n",
    "    return (df - min_) / (max_ - min_)\n",
    "    \n",
    "def z_rescale(df): \n",
    "    mean = df.mean()\n",
    "    std = df.std()\n",
    "    return (df - mean) / std\n",
    "\n",
    "def get_fig_subjects():\n",
    "    fig, axes = plt.subplots(10, 3, figsize=(10, 40))\n",
    "    return fig, np.ravel(axes)\n",
    "\n",
    "def title_with_subject_id(ax, subject_id):\n",
    "    ax.set_title(f'SUBJECT #{subject_id}')\n",
    "    return None\n",
    "\n",
    "def save_feature_quantiles(feature,\n",
    "                           inf_qt=0.025,\n",
    "                           sup_qt=0.975,\n",
    "                           n_quantiles=21,\n",
    "                           robust_rescaling=False,\n",
    "                           overwrite=False,\n",
    "                           verbose=True):\n",
    "    \"\"\"\n",
    "    See pictures/quantile_plots\n",
    "    \n",
    "    Can be improved (make robust and not robust qplots simultaneously)\n",
    "    \"\"\"\n",
    "    # Make directory if it does not exist\n",
    "    qplot_dir = os.path.join(PICTURES_FOLDER, f\"quantile_plots\")\n",
    "    os.makedirs(qplot_dir, exist_ok=True)\n",
    "    # Escape if not overwrite and already done\n",
    "    qplot_fname = os.path.join(qplot_dir, f'{feature}{\"--rescaled\" if robust_rescaling else \"\"}.png')\n",
    "    if (not overwrite) and os.path.exists(qplot_fname):\n",
    "        return None\n",
    "    # Otherwise,\n",
    "    subject_ids = get_subject_ids(h5_train)\n",
    "    quantiles = np.linspace(inf_qt, sup_qt, n_quantiles).round(3)\n",
    "    subjects_quantiles = dict()\n",
    "    for cnt, sid in enumerate(subject_ids):\n",
    "        if verbose:\n",
    "            print_bis(f\"FEATURE #{FEATURES.index(feature)} SUBJECT {cnt+1}/{len(subject_ids)} (RESCALE = {str(robust_rescaling)})\")\n",
    "        # Robust representation of the signal\n",
    "        signal = get_subject_feature_signals(h5_train, sid, feature, as_timeseries=False)\n",
    "        size = signal[0].size\n",
    "        signal = pd.Series(np.concatenate(signal))\n",
    "        if robust_rescaling:\n",
    "            signal = robust_rescale(signal)\n",
    "        # Behaviour by sleep stage\n",
    "        sleep_stages = get_subject_sleep_stage(sid).values\n",
    "        signal_by_stage = signal.groupby(np.repeat(sleep_stages, size))\n",
    "        subjects_quantiles[sid] = signal_by_stage.quantile(quantiles).unstack(0)\n",
    "        \n",
    "    fig, axes = get_fig_subjects()\n",
    "    for ax, sid in zip(axes, subject_ids):\n",
    "        subjects_quantiles[sid].plot(ax=ax, color=SLEEP_STAGES_COLORS)\n",
    "        title_with_subject_id(ax, sid)\n",
    "    plt.savefig(qplot_fname)\n",
    "    plt.close(fig)\n",
    "    return subjects_quantiles\n",
    "\n",
    "\n",
    "# TO WRITE QUANTILE PLOTS IN pictures/quantile_plots\n",
    "for i, feat in enumerate(FEATURES):\n",
    "    # print_ter(f\"========= FEATURE {i+1}/{len(FEATURES)} =========\")\n",
    "    save_feature_quantiles(feat, robust_rescaling=False, overwrite=False, verbose=True)\n",
    "    save_feature_quantiles(feat, robust_rescaling=True, overwrite=False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def do_nothing(x):\n",
    "    return x\n",
    "\n",
    "def aggregate_stat(stat_func, char, dataset, y_vals, chunksize=1000):\n",
    "    \"\"\"stat_func must have axis kwarg and take 2d arrays as arg\"\"\"\n",
    "    chunks_ix = np.array_split(y_vals.index, len(y_vals) / chunksize)\n",
    "    final = pd.Series([list() for _ in np.unique(y_vals)], index=np.unique(y_vals))\n",
    "    for cnt, ix in enumerate(chunks_ix): \n",
    "        # print(cnt, '/', len(chunks_ix))\n",
    "        tmp = pd.Series(stat_func(dataset[char][ix.tolist()], axis=1), index=y_vals.loc[ix])\n",
    "        tmp = tmp.groupby(tmp.index).agg(list)\n",
    "        final = final + tmp.reindex(final.index, fill_value=list())\n",
    "    return final\n",
    "\n",
    "def custom_group_mean(char, dataset, y_vals, chunksize=1000):\n",
    "    chunks_ix = np.array_split(y_vals.index, len(y_vals) / chunksize)\n",
    "    for cnt, ix in enumerate(chunks_ix): \n",
    "        # print(cnt, '/', len(chunks_ix))\n",
    "        mean_tmp = pd.Series(np.mean(dataset[char][ix.tolist()], axis=1), index=y_vals.loc[ix])\n",
    "        mean_tmp = mean_tmp.groupby(mean_tmp.index).agg(['mean', 'size'])\n",
    "        if cnt == 0:\n",
    "            mean_df = mean_tmp\n",
    "            continue\n",
    "        mean_df.loc[:, 'mean'] = (mean_df.prod(axis=1) + mean_tmp.prod(axis=1)) / (mean_df['size'] + mean_tmp['size'])\n",
    "        mean_df.loc[:, 'size'] = mean_df['size'] + mean_df['size']\n",
    "    return mean_df['mean']\n",
    "\n",
    "# custom_group_mean('eeg_1', h5_train, y_train)\n",
    "averages = pd.concat(map(lambda x: custom_group_mean(x, h5_train, y_train, 1000), FEATURES), axis=1, keys=FEATURES)\n",
    "'''\n",
    "_=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_proba_transition(subject_id=None):\n",
    "    if subject_id:\n",
    "        start, end = get_subject_boundaries(h5_train, subject_id, ready_to_use=False)\n",
    "        y = y_train.loc[start:end]\n",
    "    else: # all subjects\n",
    "        y = y_train.loc[:]\n",
    "    transition_df = pd.DataFrame(data={\"stage\": y, \"stage_after\": y.shift(-1)})\n",
    "    transition_df = transition_df.iloc[:-1] # NaN\n",
    "    transition_df = transition_df.astype(int)\n",
    "    counts = transition_df.groupby([\"stage\", \"stage_after\"]).size()\n",
    "    counts = counts.unstack(1, fill_value=0)\n",
    "    probas = counts.div(counts.sum(axis=1), axis=0)\n",
    "    probas = probas.reindex(range(0, 5), axis=0, fill_value=0)\n",
    "    probas = probas.reindex(range(0, 5), axis=1, fill_value=0)\n",
    "    return probas\n",
    "\n",
    "transition_plots_dir = os.path.join(PICTURES_FOLDER, \"transition_plots\")\n",
    "os.makedirs(transition_plots_dir, exist_ok=True)\n",
    "\n",
    "def save_transition_plots_by_subject(overwrite=False, verbose=True):\n",
    "    fpath = os.path.join(transition_plots_dir, \"transition_matrix_by_subject.png\")\n",
    "    if (not overwrite) and os.path.exists(fpath):\n",
    "        return None\n",
    "    subject_ids = get_subject_ids(h5_train)\n",
    "    fig, axes = get_fig_subjects()\n",
    "    for ax, sid in zip(axes, subject_ids):\n",
    "        if verbose:\n",
    "            print_bis(f\"SUBJECT #{sid}\")\n",
    "        probas = get_proba_transition(subject_id=sid)\n",
    "        sns.heatmap(probas, ax=ax, vmin=0, vmax=1, annot=True)\n",
    "        title_with_subject_id(ax, sid)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fpath)\n",
    "    plt.close(fig)    \n",
    "    return None\n",
    "\n",
    "def save_transition_plot_global(overwrite=False):\n",
    "    fpath = os.path.join(transition_plots_dir, \"transition_matrix_global.png\")\n",
    "    if (not overwrite) and os.path.exists(fpath):\n",
    "        return None\n",
    "    proba_global = get_proba_transition()\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(proba_global, ax=ax, vmin=0, vmax=1, annot=True)\n",
    "    fig.savefig(fpath)\n",
    "    plt.close(fig)\n",
    "    return None\n",
    "    \n",
    "save_transition_plots_by_subject(overwrite=False)\n",
    "save_transition_plot_global(overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature #37\u001b[1K\r"
     ]
    }
   ],
   "source": [
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n",
    "def make_input_feature(h5_file, feature, n_chunks=100):\n",
    "    print_bis(f\"Feature #{FEATURES.index(feature)}\")\n",
    "    if FREQUENCIES[feature] == 0:\n",
    "        return h5_file[feature][:]\n",
    "    feature_array = np.empty(shape=(h5_file[feature].shape[0], len(QUANTILES)))\n",
    "    for i, j in chunks_iterator(n_chunks, h5_file[feature].shape[0]):\n",
    "        feature_array[i:j, :] = np.quantile(h5_file[feature][i:j], QUANTILES, axis=1).T\n",
    "    return feature_array\n",
    "\n",
    "def make_input(h5_file):\n",
    "    return np.concatenate([make_input_feature(h5_file, feat) for feat in FEATURES], axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature #37\u001b[1K\r"
     ]
    }
   ],
   "source": [
    "X_train = make_input(h5_train)\n",
    "y_train_ = y_train.values\n",
    "\n",
    "X_test = make_input(h5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "basic_rf = RandomForestClassifier()\n",
    "basic_rf.fit(X_train, y_train_)\n",
    "y_test = basic_rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_4.csv\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "SUBMISSION_FOLDER = \"kaggle_data/submissions\"\n",
    "os.makedirs(SUBMISSION_FOLDER, exist_ok=True)\n",
    "\n",
    "def serialize_for_submission(y, save=True):\n",
    "    submission = pd.Series(data=y, index=h5_test[\"index_absolute\"][:], name=\"sleep_stage\")\n",
    "    if not save:\n",
    "        return submission\n",
    "    submissions = os.listdir(SUBMISSION_FOLDER)\n",
    "    if len(submissions) == 0:\n",
    "        fname = os.path.join(SUBMISSION_FOLDER, \"submission_1.csv\")\n",
    "    else:\n",
    "        last = sorted(submissions)[-1]\n",
    "        last_num = re.search(\"(\\d+)\\.csv\", last).groups()[0]\n",
    "        fname = os.path.join(SUBMISSION_FOLDER, f\"submission_{int(last_num)+1}.csv\")\n",
    "    submission.to_csv(fname, index_label='index')\n",
    "    \n",
    "z = serialize_for_submission(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.csv'"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"(\\d+)\\.csv\", \"submission_10.csv\").group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_train.close()\n",
    "h5_test.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python37164bitdc6ddf9b5234459bacda0ad4bcef452b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
