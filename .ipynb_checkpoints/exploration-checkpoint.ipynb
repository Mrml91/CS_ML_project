{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "\n",
    "PICTURES_FOLDER = \"pictures\"\n",
    "os.makedirs(PICTURES_FOLDER, exist_ok=True)\n",
    "\n",
    "SLEEP_STAGES_COLORS = {\n",
    "    0: \"blue\",\n",
    "    1: \"green\",\n",
    "    2: \"red\",\n",
    "    3: \"black\",\n",
    "    4: \"orange\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to lock file, errno = 35, error message = 'Resource temporarily unavailable')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-39019b487c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"kaggle_data/X_test.h5/X_test.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'w-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to lock file, errno = 35, error message = 'Resource temporarily unavailable')"
     ]
    }
   ],
   "source": [
    "train_file = \"kaggle_data/X_train.h5/X_train.h5\"\n",
    "test_file = \"kaggle_data/X_test.h5/X_test.h5\"\n",
    "\n",
    "h5_train = h5py.File(train_file, mode='a')\n",
    "h5_test = h5py.File(test_file, mode='a')\n",
    "\n",
    "y_train = pd.read_csv(\"kaggle_data/y_train.csv\", index_col=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_globals():\n",
    "    features = list(h5_train.keys())\n",
    "    frequencies = {feat: h5_train[feat][0].size / 30 \n",
    "                   for feat in features \n",
    "                   if feat not in ('index', 'index_absolute', 'index_window')}\n",
    "    return features, frequencies\n",
    "    \n",
    "FEATURES, FREQUENCIES = update_globals()\n",
    "print(\"FEATURES =\", FEATURES)\n",
    "print(\"FREQUENCIES =\", FREQUENCIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# HELPERS\n",
    "\n",
    "def make_timeline(freq):\n",
    "    \"\"\"\n",
    "    ARGS:\n",
    "        freq (int): frequency in Hertz\n",
    "    \n",
    "    RETURNS:\n",
    "        (pd.timedelta_range) : timestamps for a signal sampled at <freq> Hz for 30 seconds\n",
    "    \"\"\"\n",
    "    return pd.timedelta_range(start='0s', end='30s', periods=freq*30)\n",
    "\n",
    "\n",
    "def make_full_timeline(windows, freq):\n",
    "    # test there is no missing data\n",
    "    deltas = np.unique(np.diff(windows))\n",
    "    assert (len(deltas) == 1) and (int(deltas[0]) == 1)\n",
    "    return pd.timedelta_range(start='0s',\n",
    "                              end=pd.to_timedelta('30s') * (windows[-1] + 1),\n",
    "                              periods=freq * 30 * (windows[-1] + 1))\n",
    "\n",
    "def get_subject_ids(h5_file):\n",
    "    return np.unique(h5_file[\"index\"][:])\n",
    "\n",
    "    \n",
    "def get_subject_boundaries(h5_file, subject_id, ready_to_use=True):\n",
    "    \"\"\"\n",
    "    Helper function to select data relating to a given subject (on numpy arrays)\n",
    "    \n",
    "    ARGS:\n",
    "        h5_file (h5py.File)\n",
    "        subject_id (int)\n",
    "        ready_to_use (bool, default=True): return a slice or a tuple\n",
    "        \n",
    "    RETURNS:\n",
    "        subject_boundaries : (slice) (index_start, index_end+1) if <ready_to_use>\n",
    "                             (tuple) (index_start, index_end) if not <ready_to_use>\n",
    "                        \n",
    "    \"\"\"\n",
    "    sids = h5_file['index'][:]\n",
    "    start = np.argmax(sids == subject_id)\n",
    "    end = len(sids) - 1 - np.argmax(sids[::-1] == subject_id)\n",
    "    \n",
    "    indexers = h5_file['index_absolute'][:]\n",
    "    start = indexers[start]\n",
    "    end = indexers[end]\n",
    "    if ready_to_use:\n",
    "        return slice(start, end + 1) # for numpy arrays\n",
    "    return (start, end)\n",
    "\n",
    "\n",
    "def get_subject_feature_signals(h5_file, subject_id, feature, as_timeseries=False):\n",
    "    \"\"\"\n",
    "    Get the full timeseries for a given (subject_id, feature) pair.\n",
    "    \n",
    "    ARGS:\n",
    "        h5_file (h5py.File)\n",
    "        subject_id (int)\n",
    "        feature (str)\n",
    "        \n",
    "    RETURNS:\n",
    "        timeseries : (pd.Series if <as_timeseries>) represents the <feature> timeseries of the subject \n",
    "                     (list[np.array[?]] if not <as_timeseries>) list of <feature> signals from the subject\n",
    "    \"\"\"\n",
    "    # Fetch subject boundaries\n",
    "    boundaries = get_subject_boundaries(h5_file, subject_id)\n",
    "    # Retrieve samples\n",
    "    feature_timeseries = h5_file[feature][boundaries]\n",
    "    if not as_timeseries:\n",
    "        return feature_timeseries\n",
    "    feature_timeseries = np.concatenate(feature_timeseries, axis=0)\n",
    "    # Build timeline\n",
    "    feature_frequency = FREQUENCIES[feature]\n",
    "    windows = h5_file['index_window'][boundaries]\n",
    "    timeline = make_full_timeline(windows, feature_frequency)\n",
    "    return pd.Series(data=feature_timeseries, index=timeline)\n",
    "\n",
    "\n",
    "def get_subject_sleep_stage(subject_id):\n",
    "    start, end = get_subject_boundaries(h5_train, subject_id, ready_to_use=False)\n",
    "    return y_train.loc[start:end] # because loc includes <end> (different behaviour than numpy arrays)\n",
    "    \n",
    "\n",
    "def _create_speed_and_acceleration(h5_file, overwrite=False, verbose=True):\n",
    "    freq = 10\n",
    "    dt = 1 / freq\n",
    "    \n",
    "    # Create datasets if required\n",
    "    if \"accel_norm\" in h5_file.keys() and not overwrite:\n",
    "        return None\n",
    "    shape, dtype = h5_file[\"x\"].shape, h5_file[\"x\"].dtype\n",
    "    for name in [\"accel_norm\", \"speed_x\", \"speed_y\", \"speed_z\", \"speed_norm\"]:\n",
    "        try:\n",
    "            h5_file.create_dataset(name, shape=shape, dtype=dtype)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Initiate subject id\n",
    "    sid = -1\n",
    "    for ix in range(shape[0]):\n",
    "        if sid != h5_file[\"index\"][ix]:\n",
    "            sid = h5_file[\"index\"][ix]\n",
    "            speed = np.array([[0, 0, 0]])\n",
    "            if verbose:\n",
    "                print(f\"SUBJECT #{sid}\")\n",
    "        # acceleration\n",
    "        accel = np.stack([h5_file[feat][ix] for feat in (\"x\", \"y\", \"z\")], axis=-1)\n",
    "        h5_file[\"accel_norm\"][ix] = np.linalg.norm(accel, ord=2, axis=1)\n",
    "        # speed\n",
    "        speed = speed + np.cumsum(accel, axis=0) * dt\n",
    "        h5_file[\"speed_x\"][ix] = speed[:, 0]\n",
    "        h5_file[\"speed_y\"][ix] = speed[:, 1]\n",
    "        h5_file[\"speed_z\"][ix] = speed[:, 2]\n",
    "        h5_file[\"speed_norm\"][ix] = np.linalg.norm(speed, ord=2, axis=1)\n",
    "        # speed for next iteration\n",
    "        speed = speed[[-1], :]\n",
    "    return None\n",
    "        \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "# FOURIER\n",
    "\n",
    "from scipy.fft import fft\n",
    "\n",
    "def get_spectrum(seq, fs):\n",
    "    ft_modulus = np.abs(fft(seq))\n",
    "    # The signal is real so the spectrum is symmetric \n",
    "    if len(seq) % 2 == 0:\n",
    "        ft_modulus = ft_modulus[:len(seq) // 2]\n",
    "    else:\n",
    "        ft_modulus = ft_modulus[:len(seq) // 2 + 1]\n",
    "    freqs = np.arange(0, len(ft_modulus)) * fs / len(seq) # frequencies of the spectrum\n",
    "    return pd.Series(data=ft_modulus, index=freqs)\n",
    "    \n",
    "\n",
    "def get_spectrum_maxima(seq, fs, thresh=0.1):\n",
    "    spectrum = get_spectrum(seq, fs)\n",
    "    delta_left = np.diff(spectrum, prepend=spectrum[0] - 1) > 0 # ascending\n",
    "    delta_right = np.diff(spectrum[::-1], prepend=spectrum[-1] - 1)[::-1] > 0 # descending\n",
    "    ix_keep = np.logical_and(delta_left, delta_right) # local maximum\n",
    "    spectrum_util = spectrum.loc[ix_keep]\n",
    "    spectrum_util = spectrum_util.loc[spectrum_util > spectrum_util.max() * thresh]\n",
    "    return spectrum_util\n",
    "\n",
    "# ACCELEROMETER\n",
    "\n",
    "def acceleration_to_speed(accel_arr):\n",
    "    if accel_arr.ndim == 2:\n",
    "        return np.cumsum(accel_arr, axis=1)\n",
    "    return np.cumsum(accel_arr)\n",
    "\n",
    "def vec_to_norm(arr):\n",
    "    return np.linalg.norm(arr, axis=1)\n",
    "\"\"\"\n",
    "\n",
    "# Create speed and acceleration\n",
    "_create_speed_and_acceleration(h5_train, overwrite=False, verbose=True)\n",
    "_create_speed_and_acceleration(h5_test, overwrite=False, verbose=True)\n",
    "FEATURES, FREQUENCIES = update_globals()\n",
    "print(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# get_subject_feature_signals(h5_train, 1, \"eeg_1\", as_timeseries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# get_subject_feature_signals(h5_train, 1, \"eeg_1\", as_timeseries=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def plot_subject_quantiles(subject_id, q_inf=0.025, q_sup=0.975, n_quantiles=20):\n",
    "    sleep_states = get_subject_sleep_state(subject_id)\n",
    "    qts = np.linspace(q_inf, q_sup, n_quantiles).round(3)\n",
    "    for feature in FEATURES:\n",
    "        signal = get_subject_feature_signals(h5_train, subject_id, feature, as_timeseries=False)\n",
    "        size = signal[0].size\n",
    "        signal_by_state = pd.Series(data=np.concatenate(signal, axis=0),\n",
    "                                    index=np.repeat(sleep_states.values, size))\n",
    "        qt_df = signal_by_state.groupby(signal_by_state.index).quantile(qts)\n",
    "        qt_df.unstack(0).plot()\n",
    "        # sns.heatmap(qt_df.unstack(0))\n",
    "        plt.title(feature)\n",
    "        plt.show()\n",
    "    return None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def robust_rescale(df):\n",
    "    \"\"\"\n",
    "    X_rescaled = (X - MED(X)) / MED(|X - MED(X)|)\n",
    "    \"\"\"\n",
    "    med = df.median()\n",
    "    med_spread = (df - df.median()).abs().median()\n",
    "    # df_rescaled = (df - med) / med_spread\n",
    "    return (df - med) / med_spread\n",
    "\n",
    "def min_max_rescale(df):\n",
    "    min_ = df.min()\n",
    "    max_ = df.max()\n",
    "    return (df - min_) / (max_ - min_)\n",
    "    \n",
    "def z_rescale(df): \n",
    "    mean = df.mean()\n",
    "    std = df.std()\n",
    "    return (df - mean) / std\n",
    "\n",
    "def save_feature_quantiles(feature,\n",
    "                           inf_qt=0.025,\n",
    "                           sup_qt=0.975,\n",
    "                           n_quantiles=21,\n",
    "                           robust_rescaling=False,\n",
    "                           overwrite=False,\n",
    "                           verbose=True):\n",
    "    \"\"\"\n",
    "    See pictures/quantile_plots\n",
    "    \n",
    "    Can be improved (make robust and not robust qplots simultaneously)\n",
    "    \"\"\"\n",
    "    # Make directory if it does not exist\n",
    "    qplot_dir = os.path.join(PICTURES_FOLDER, f\"quantile_plots\")\n",
    "    os.makedirs(qplot_dir, exist_ok=True)\n",
    "    # Escape if not overwrite and already done\n",
    "    qplot_fname = os.path.join(qplot_dir, f'{feature}{\"--rescaled\" if robust_rescaling else \"\"}.png')\n",
    "    if (not overwrite) and os.path.exists(qplot_fname):\n",
    "        return None\n",
    "    # Otherwise,\n",
    "    subject_ids = get_subject_ids(h5_train)\n",
    "    quantiles = np.linspace(inf_qt, sup_qt, n_quantiles).round(3)\n",
    "    subjects_quantiles = dict()\n",
    "    for cnt, sid in enumerate(subject_ids):\n",
    "        if verbose:\n",
    "            print(f\"--> SUBJECT {cnt+1}/{len(subject_ids)}\")\n",
    "        # Robust representation of the signal\n",
    "        signal = get_subject_feature_signals(train_ds, sid, feature, as_timeseries=False)\n",
    "        size = signal[0].size\n",
    "        signal = pd.Series(np.concatenate(signal))\n",
    "        if robust_rescaling:\n",
    "            signal = robust_rescale(signal)\n",
    "        # Behaviour by sleep stage\n",
    "        sleep_stages = get_subject_sleep_stage(sid).values\n",
    "        signal_by_stage = signal.groupby(np.repeat(sleep_stages, size))\n",
    "        subjects_quantiles[sid] = signal_by_stage.quantile(quantiles).unstack(0)\n",
    "        \n",
    "    fig, axes = plt.subplots(10, 3, figsize=(10, 40))\n",
    "    for ax, sid in zip(np.ravel(axes), subject_ids):\n",
    "        subjects_quantiles[sid].plot(ax=ax, title=f\"Subject {sid}\", color=SLEEP_STAGES_COLORS)\n",
    "    plt.savefig(os.path.join(qplot_dir,\n",
    "                             f'{feature}{\"--rescaled\" if robust_rescaling else \"\"}.png')\n",
    "               )\n",
    "    plt.clf()\n",
    "    return subjects_quantiles\n",
    "\n",
    "\n",
    "# TO WRITE QUANTILE PLOTS IN pictures/quantile_plots\n",
    "for i, feat in enumerate(FEATURES):\n",
    "    print(f\"========= FEATURE {i+1}/{len(FEATURES)} =========\")\n",
    "    save_feature_quantiles(feat, robust_rescaling=False, overwrite=False, verbose=True)\n",
    "    save_feature_quantiles(feat, robust_rescaling=True, overwrite=False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Former code to change\n",
    "```python\n",
    "def do_nothing(x):\n",
    "    return x\n",
    "\n",
    "def aggregate_stat(stat_func, char, dataset, y_vals, chunksize=1000):\n",
    "    \"\"\"stat_func must have axis kwarg and take 2d arrays as arg\"\"\"\n",
    "    chunks_ix = np.array_split(y_vals.index, len(y_vals) / chunksize)\n",
    "    final = pd.Series([list() for _ in np.unique(y_vals)], index=np.unique(y_vals))\n",
    "    for cnt, ix in enumerate(chunks_ix): \n",
    "        # print(cnt, '/', len(chunks_ix))\n",
    "        tmp = pd.Series(stat_func(dataset[char][ix.tolist()], axis=1), index=y_vals.loc[ix])\n",
    "        tmp = tmp.groupby(tmp.index).agg(list)\n",
    "        final = final + tmp.reindex(final.index, fill_value=list())\n",
    "    return final\n",
    "\n",
    "def custom_group_mean(char, dataset, y_vals, chunksize=1000):\n",
    "    chunks_ix = np.array_split(y_vals.index, len(y_vals) / chunksize)\n",
    "    for cnt, ix in enumerate(chunks_ix): \n",
    "        # print(cnt, '/', len(chunks_ix))\n",
    "        mean_tmp = pd.Series(np.mean(dataset[char][ix.tolist()], axis=1), index=y_vals.loc[ix])\n",
    "        mean_tmp = mean_tmp.groupby(mean_tmp.index).agg(['mean', 'size'])\n",
    "        if cnt == 0:\n",
    "            mean_df = mean_tmp\n",
    "            continue\n",
    "        mean_df.loc[:, 'mean'] = (mean_df.prod(axis=1) + mean_tmp.prod(axis=1)) / (mean_df['size'] + mean_tmp['size'])\n",
    "        mean_df.loc[:, 'size'] = mean_df['size'] + mean_df['size']\n",
    "    return mean_df['mean']\n",
    "\n",
    "# custom_group_mean('eeg_1', train_ds, train_y)\n",
    "averages = pd.concat(map(lambda x: custom_group_mean(x, train_ds, train_y, 1000), FEATURES), axis=1, keys=FEATURES)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.close()\n",
    "test_ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python37164bitdc6ddf9b5234459bacda0ad4bcef452b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
